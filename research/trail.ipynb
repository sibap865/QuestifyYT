{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello everyone welcome to AI anytime  channel so I'm starting a new playlist  called finetuning of large language  models in this playlist I will cover you  know the different aspect of fine-tuning  a large language model how to you know  finetune on your data set you will have  your custom data sets you want to fine  tune llms in this entire playlist we'll  have multiple videos we'll start with  the fundamentals and we starting in this  video as well the fundamentals you know  the different steps of pre-training  fine-tuning and the Noel techniques like  Laura and Kora how to select the right  hyper parameters for fine tuning task  what other tools that you can use to  fine tune like Excel toll for example  that we'll cover in this video which is  a low code fine-tuning tool and you know  how to use Laura and Kora to fine tune  uh with reduced memory uh consumption  now in this playlist we'll have videos  to F tune Lama 2 F tune Mistral fine  tune open Lama and fine tune other open  source llms and also the Clos Source One  like for example you want to find tune  the GPT models for example 3.5 you know  on your data or the Vex AI models like  you know that that basically fuels the  Palm uh Google B the Palm 2 models we  also fine tune with those so this is  completely focused on fine tuning this  entire playlist will have at least 10 to  12 videos but the first video today we  have going to talk about how to you know  F tune an llm and how to select the tool  the techniques and the data set and how  to configure and set it up uh in your  infrastructure for example how to choose  a compute what kind of cloud providers  you can use so all those nuances will  cover in this video this is a beginner  friendly video where we when I'm uh  assuming that you don't have fine-tuning  uh experience prior to this video okay  so let's start our experimentation with  uh this first video where we look at the  fundamentals of pre-training fine-tuning  and the techniques like Laura and  Kora all right uh guys so when we talk  about you know training llms there are  essentially three approaches that we see  and that's what we're going to look at  here so what I'm going to write first is  training uh  llms and we'll understand uh in very  brief about these three approaches so  the three approach that I'm talking so  let me first write the three  approaches so the three approaches that  we're going to have is  pre-training and after pre-training we  have fine  tuning and then then we have on a very  high level there are more but I'm just  writing here which are the commonly used  that we have used to fine tune in the  community so these are three approaches  pre-training fine-tuning Laura and Q  Laura now fine tuning and luras and the  you know even if you go more Downstream  uh I'm keeping this separate because I  will explain why I'm keeping fine tuning  separate uh than Lowa of course you can  use use Laura qora to F tune we'll see  that let's start a bit with definition  we'll go into code we'll also find tune  uh  shortly but let's first try to  understand because when I talk to you  know people who are trying to fine tune  basically they lack the fundamentals or  basic basics of these you know natural  language or the terminologies that we  see so I was talking to many of my you  know um colleagues or Community friends  and I was ask them like what are Q  projections what are projections that  you see in the lowas that you target  modules or whatever they had no idea  about it and that's what I want to cover  here that your fundamentals and Basics  should be covered because rest of the  thing I will show that how easily you  can f tune with tools like Exel all and  we'll see that uh in a bit now what do  you mean by  pre-training now it's pre-training  involves several steps now imagine if  you have a massive data set of Text data  so let me just write it over here you  have  uh massive Text data now the source can  be you know Wikipedia already Twitter  now X and you have you have your own you  know kind of an um your own proprietary  data kind of a thing and these are  mainly in terabytes okay we talking  about if you go to Lama 2 they say two  or three trillion I think it's two  trilon tokens we'll come to the tokens  later but just imagine extracting or  creating two trillion tokens would need  at least even more than terabytes of  data for you so in the pre-training step  is first is that you need a massive text  Data okay in in terabytes once you have  that you then identify uh model  architecture so let me just write here  identify the model  architecture and here I'm covering  because these topics are really vast uh  but I'm just covering on the very high  level but even once you have text Data  the lot of pre-processing steps like you  know removing pii you know looking at  the D duplications kind of a thing a lot  of other things that we look at but  we'll explain that once we are  fine-tuning okay but here just on very  high level to give you a definition of  pre-training fine-tuning and lowas and  qas now  uh once you have a model architecture  chosen or created specifically for the  task at hand then you need a tokenizer  okay so you need a tokenizer Let me just  write it over  here you need a  tokenizer okay that is trained to  appropriately handle your data handle  the data ensuring that it can  efficiently encode and decode text so  here I'm going to write  the tokenizer will play two role one is  of  encoding and decoding not necessarily  always encoding and decoding depends  what kind of task you are executing  we'll see  that now your tokenizer is done then  once you have the tokenizer ready the  data set that you have the massive data  set that you created is then  pre-processed using the tokenizer  vocabulary okay so let me just write it  over here  data set  is  pre-processed data set is  pre-processed using the tokenizers vocab  okay so you create the vocab vocab or  vocabulary of your tokenizers using  libraries like sentence piece that goes  you know that that becomes one of the  additions in the Transformers I will see  that uh shortly now you have data set is  pre-processed using you know tokenizers  vocabulary so let me just write it over  here tokenizers  vocab okay now this is important because  this will convert the raw text that you  have you have a raw text into  a uh into a format that is suitable for  training the model now here you will  have once you have this after this St  you will have a  suitable  you know uh uh  data in a  format for  training right now and this steps  involve you know mapping uh tokens to  the corresponding IDs incorporating  necessarily special token so let me just  write it because once I'm explaining the  config file the yaml file once we find  tune where you have lot of you know  special tokens like EOS BOS blah blah  blah this is how we'll cover that so  suitable data in a format for training  now here what we do now this steps  involves so I will keep it uh like this  so This steps involve mapping tokens we  we you would have seen something called  map using when we are using tokenizer to  map it now mapping tokens let me just  write mapping tokens to the IDS to the  respective or I think it's let's  corresponding might be the right  word corresponding IDs and then you know  uh incorporation of or rather let's  right  incorporating  any special  tokens special tokens okay or attention  mask can be other thing as well I just  recall no attention  mask okay now once the data is  pre-processed now it is ready for uh  training by the way these are the these  are the part of the same so I will now  uh these are the part that depends what  kind of architecture you have chosen but  now your data is ready you can pre-train  your your you ready to go into the  pre-training phase now what happens in  the pre-training phase okay let me let  me write it over here  what  happens in the pre-training  phase now in pre-training Phase let me  just write it the model learns to  predict the next word in a  sentence learns to  predict  the next word in a  sentence that's what happens or not only  this because it can also fill in so  we'll talk about what is filling and  what is Generation fill in missing words  as well so you can do fill in missing  words as well so let me just write it  over  here correct this is what it does now  this process of course involves  optimizing the models parameters you  know like it's trial and error kind of  an experimentation right this model you  have to optimize through different  parameters and that's an iterative  training procedure basically that  maximizes the likelihood of generating  the correct word or sequence of word  given the context now to accomplish this  pre-training  phase we typically employs a variant of  the self-supervised the learning  technique the model is presented with  partially masked input sequences where  certain tokens are of course we intent  intentionally hide that and it must  predict those missing tokens based on  the surrounding  context and because we have a massive  amounts of data right so we can train it  on that now model gradually develops a  reach understanding of language patterns  grammar and the semantic  relationship now once we talk about  filling so you have this fill in missing  words that we are talking that's  something that has been called as masked  language  model masked language model which is  called  MLM and when you talk about  Generation  generation now that has been called as  as are not limited to by the way but  this has been called calar learning  model that's called Cal language  modeling excuse me not learning Cal  language model or  modeling and that's why we use Auto Cal  from know pre-trained or whatever when  you use a Transformers pipeline now  these are the most these are the used  one we will be focusing on this this is  what we're going to focus C language  modeling now unlike masked language  modeling where you know certain tokens  are masked and the model predicts those  missing tokens Cal language modeling  focus on predicting the next word in a  sentence given the preceding  context now this step whatever you just  uh you just uh you just have seen here  on the screen this step is a  pre-training step here your model is now  able to understand the language patterns  Now understand uh the IC relationship  and all of those things basically it  gets the general language knowledge  making the model A proficient language  encoder right but it lacks a specific  knowledge so  after after  pre-training right you have a model now  it capture General  language so let can just write capture  General  language but you know it lacks and this  is what we're going to look at the next  fine-tuning step it lacks specific  knowledge lack specific knowledge about  a particular task or  domain  or domain and that's why we fine tune  because we want to fine tune it for a  specific purpose now to bridge this Gap  a subsequent fine-tuning phase follows  pre-training right that's where fine  tuning comes in guys in the play now  we'll look we'll talk about fine tuning  right now we only covering theoretical  aspect here we'll go into code a bit uh  so uh in a bit now let's talk about fine  tuning if you want to skip this part you  can also skip this you know but I will  recommend that you should have the  knowledge otherwise you will not be able  to build or find you know whatever  you'll be only having very high level  knowledge if you want to achieve that  you don't have to even learn those  things those are so many low code no  code fine tuning tool just upload your  data and get a finetune model but if you  want to make your carer in this field  you should understand everything from  scratch which is required of course when  I say everything don't go and read now  Marco Chain by the way that will not  help  uh now once we talk about finetuning now  after the pre-training phase where where  the model learns General language  knowledge fine tuning  allows let me write with  this now fine tuning  allows us to  specialize specialize the models  capability specialize the llms I'll  write rather llms  capabilities and optimize its  performance and  optimize its  performance  okay or let me make it more descriptive  on  a  narrower or a downstream task or a task  is specific data set or something like  that right let me just write task is  specific data  set okay uh this is what we do now fine  tuning of course if you look at the  pre-training we have covered all of this  high level steps text Data identifying  the model architecture tokenizer blah  blah blah now fing also uh similarly  also it also has many  steps so you have you have a task  specific data set that has been gathered  that that will probably have labeled  examples relevant to the desired task  now for example let's talk about some a  very famous word nowadays in Industry  that has been used called instruct  tuning or instruction tuned model we  call about instruction tuned model now  for instruction tuned  model a data set of instru instruction  and response pair is gathered so first  thing that you need a data set here  again a task specific data set so for  this if you are talking about  instruction uh tuned model you need a  data set of your instruction and  response instruction and  response data set is gathered now these  are basically pairs it can be a question  answer pair now the finetuning data set  size is significantly smaller than than  the you know your pre-training so you  don't need that huge data set now but of  course even even for this case you need  a sufficient uh uh like data set having  you know at least 10,000 question answer  pairs or something like that  now uh when the pre-training model is  initialized with its previously learned  parameters the model is then trained on  task specific data set optimizing its  parameter to minimize a task is specific  loss function loss function means how of  the model was from the desired result  that basically very lemon terms now I'm  writing  here uh task  specific loss  function task specific loss function now  during the fine tuning process the  parameters that we will have from the  pre-train models are then adjusted okay  we we will'll focus on this word a lot  that's called adjusted but let me write  it over here only  now uh the params let me just write like  this the  params of the pre-trained  model of the pre-trained model are  adjusted  are adjusted  using you know and focus on this word  adjusted we always talk about adjusting  weights let's adjust the weight and all  of those adjustment that we do with the  weights it all Bo down to weights guys  when you talk about fine tuning Now  using something called gradient based  optimization algorithms not limited to  but this is what has been used in the  recent fine-tuned model that's called  gradient based optimization  algorithms and if you have worked with  deep learning you will probably know  about this gradient based optimization  alos now one of these alos can be uh  SGD okay let me just write SGD or  Adam uh whatever there are there are a  lot more okay now SGD for example Le I  hope I spelled it right uh sastic and I  hope this is right sastic gradient  descent now the gradients are computed  by back propagating the law so again the  back propagation comes in okay so it the  gradients are then first computed by  back propagation the LW through the  model's layers you will have n numbers  of layers allowing the model to learn  from its mistakes and update its  parameters accordingly now you will have  a back propagation working there for you  in this models layers that helps you the  model will learn from the mistakes every  time and then update the parameter so it  improves itself now to enhance the fine  tuning you will have you know different  additional techniques here so let me  just write it in the one point here more  about fine tuning then we move to Lura  and Q luras and then we go up in a bit  of fine tuning kind of a thing now  uh okay  so you can also improve as I was saying  you can improve through you know uh  learning rate  scheduling okay there are there lot of  other techniques that we can follow uh  learning rate  scheduling and regular regularization  method like dropouts or weight decays or  early stopping to prevent overfitting  and whatnot right now these techniques  help optimize the models generalization  always remember that your fin tuned  model should not memorize but should  generalize okay uh on your unseen data  or the validation of new data now model  generalization and prevent it from  memorizing as I said the train data set  too closely now this is on the fine  tuning step now we are moving towards  some of  the newly  developed  techniques uh in the ecosystem that's  called a  Lowa low rank  adaptation they have a very good  research paper on Laura everybody should  read that paper low rank  adaptation now Laura is interesting  without Laura we would not have seen  this you know uh I'll say the rise of  the open source llms like mral or Jer  whatever you know the way we find unit  because we do not have the resources to  do that right so for people like you  know the US the researchers the  developers the people working in  startups they need these kind of you  know Frameworks to work with now because  see fine tuning is expensive as you know  don't be live in a Dreamland if you are  a a researcher or that you will be able  to create something like llama to with a  limited budget it's not possible when  you see how Mistral has been trained  they had invested Millions into that so  you can of course fine tune a few models  on your small data set but the  performance then again will not be  similar so it's fine tuning age so let  me write ft is  expensive it's simple it's really  expensive that requires you know 100 of  GBS of virtual Rams the vram that's  where GPU comes in guys you know to  train you know multi-billion parameters  models to solve this specific problem  Laura was introduced okay it was  proposed uh you know it's compared to  compared to if you compare to fine  tuning opt opt 175 B with Adam that that  has been the benchmarked in the paper  also Lura can reduce the number of  trainable parameters by 10,000  times and the GPU memory requirement by  over three times just as in right now a  3X memory requirement reduction is still  in the realm of unfeasible for us people  like us because 3x is still not that  right then that's we need Kora we'll  talk about that okay so let's talk about  so if Laura is giving you for example  and we will cover Laura in detail that's  why I'm not writing a lot of things over  here we will because we have to work  with Laura and Kora when we are fine  tuning so we'll focus on that now  Lura uh 3x okay I was saying 3x X now 3x  memory requirement now  imagine but this is also not something  that we can work with it even we need  more and that's where Kora came in  picture Okay so with  Kora and if you don't understand right  now about Lura and K don't worry I will  cover that each and everything in detail  each and every line that we write in  luras and Q the code the confix whatever  the argument that we work with I will  explain each and every line now Kora  just word just add the word quantized so  I'm not writing everything but  quantization quantized okay a quantized  low rank adaptation it uses a library  that's called bits and bytes okay so it  uses a  library called bits and bytes B  andb this is a library that you know it  uses on the Fly By the way and it  achieves a near lossless  quantization so when I talk about  lossless your performance degradation is  not much of course there can be a bit of  performance degradation or there will be  reduction in the performance but that  might be myth nobody has you know has  done a really a research on big sample  that that really happens once you  quantize the llm but of course there can  be a bit of performance degradation but  that's why we calling it bits and  bites and that provides you a lossless  quantization of language models and  applies it to the Lowa procedure because  QA is based on luras by the way now this  results in a massive reductions in  memory  requirement enabling the training of  models as large as like Llama  270b Or you can do that on you know for  example  2x 2x RTX  3090 you can do that you can also uh and  and by the way if you compare let me  just uh write it okay let me just do it  if you compare without Kora this will  take around a00 which is one of the most  used GPU to train llms or create llms  you need which is by the way 800 is 80GB  80GB  GPU now you you need 16 x you need 16  different a00 gpus not one GPU node that  you need you need 16 okay so just  imagine how much it reduce if you use  Kora you know it boils down to 2 RTX 390  and of course if you have 1 a00 much  better okay to do that uh but just  imagine uh and the cost goes down once  you use loraa and  Q okay now let's see the question comes  in that okay we have been talking about  all of this thing how can we achieve our  task how can we you know F tune what  kind of machine we have to use and all  of the question that you will have in  your mind so for that I will tell you  that let's go into fine tuning so let me  just write over  here fine  tuning we are now going into uh fine  tuning  step uh in this tutorial that I'm  creating you know we'll be focusing of  you know 6B or 7B llms not more than  that because I want to keep it as a  tutorial you can explore yourself if you  have compute and all I do have compute  but you should learn and you should of  course do it yourself right now the  first thing that you think will come in  your mind is training  compute that tell me about training  compute and that's what I also you know  get worried okay training comput now for  training of course you need a GPU to do  that we'll talk about model and data set  separately that's something that we'll  have  something uh now if you want to F tune  uh model like Lama 27b or Mistral 7B let  me just write it over here now these are  the model I'm writing because this  models are commercially available to use  with an Enterprise you can make money  you know using these models guys because  you'll be building a product so don't  see that just you'll be making money by  using this model but once you build a  product on top of this now Lama to 7B  and mral 7B if you want to F tune on a  very uh on a very high level I'll say  these are the requirement that you need  the alha talk about  memory and this is based on Uther Uther  AI Transformers math 101 blog post I  will give the link in description if you  want to understand how calcul  calculations work you know to decide a  compute you should look at Transformer  math 101 blog post find the link in  description now now the memory  requirement is around let's keep it like  150 to  195  GB this would be the memory requirement  you know to F  tune okay  now so renting gpus now the next thing  is how to rent  gpus or how to use a GPU there are many  options that you can  consider uh one is uh the most I will  it's not in any order but I will write  in my Preferred Choice okay uh in an  order I rely on  runpod runp pod. I rely on runp pod then  there is V  AI then there is Lambda  labs and then then you have hyperscalers  like big players you know I will just  only write AWS say maker because I have  worked with it a lot so these are the  top and you have your Google collab also  how can I forget collab our life saer  right now run pod is my Preferred Choice  hands down run pod is my preferred  choice I will recommend you to choose  runp or between Lambda Labs but Lambda  Labs service support and all that are  very bad okay so runp pod is something  that I recommend it's extremely easy to  work with and also affordable if you're  looking for cheapest option that you can  afford which is V so let me call it  here cheapest I'm writing cheapest I'm  not writing affordable affordable is  like runp now here security can be a  concern so look at if you looking at  secure cloud or Community Cloud kind of  a thing I recommend runp that's I said  okay now if you have a super computer  you can also do with that but I do not  have  now the next thing is uh once you have  training compute decided once you have a  training compute then you have to gather  data  set I show you that how you can get a  data set if you do not have one the main  source for learning purpose is hugging  face data sets once you have the data  set and I'll cover a few things in data  set because you know once if you have  you are creating your own data set you  have to look at something called Uh  there's few things you have to look at  one is diversity in  data you do not want your models to only  do one very specific task you know you  should have a bit of diversity now  assumed a use case like you know you're  training a chat  model that does not mean that data would  only be about one specific type of chat  right you you will want to diversify the  chat examples you know the samples uh  like different scenarios so model can  learn how to generate outputs of various  type of input now imagine one of your  Ender comes in and start putting about  some question that your model have not  even seen you know so at Le those kind  of things so it it understands the  semantics better now the size of data  set on a very uh like a thumb Ru your  file size should be 10 MB that's how I  decide if you are using a CSV with  question answer payers your file size  should be 10 MB that's how I go with  okay 10 MB of your data okay now or at  least I'll recommend 10,000 question  answer  pair and the quality of data the this is  important because if you look at the  latest model and this is the most  important by the way this is the most  important thing F example F llm by  Microsoft or Orca for example these are  the examples where organizations have  shown that how a better quality data can  help you you know train an llm on a very  smaller size like 3B or 1.5b kind of a  parameter of parms this is important to  understand now we'll not talk about data  at this moment let's let's see that so  to use runp pod what we have to do is to  go to runp  pod uh dashboard here and you can see I  already have a template has been  deployed but it's very easy to do that  you have to come to secure Cloud you can  also look for Community Cloud but I  prefer this it's Community cloud is bit  cheaper because your GPU can be also  availability is an issue on community  Cloud because the same GPU can be rented  by somebody else's as well if you are  not using that now that's on the  community Cloud uh on secure Cloud they  have two things latest generation and  previous generation on latest generation  you will see the latest of gpus like h00  which gives you 80GB vram with 250 GB  Ram you know which which is the most  expensive one over here then you also  have previous generation with a00 then  that's what we're going to use okay so  a00 you can see almost $2 per hour so  one hour training you cost $2 so on for  example if you have decent enough data  set like 2,000 3,000 rows $5 you'll be  able to uh do it now uh for that let me  first show you a couple of things we  have eight eight maximum that you can uh  you know spin up over here the eight  different node of that particular 800  GPU but this is not what we want to do  okay uh so let's see how we can deploy  it uh uh how we can find unit so I  already have deployed what you have to  do you have to click on deploy and it  will deploy it will take a few minutes  to deploy it and then there will be  option to connect once you click on  connect it will show open Jupiter lab  and that's what I did over here I have a  Jupiter lab now the first thing that we  have to do is we have to get clone the  Excel toll so let's get clone so get  clone uh and then we'll take that from  here so come over here let me just click  on  this this this and then you clone you'll  see a folder here now let's let's go  inside the folder in the examples you  will see different models which are  supported you can also see the mial are  also supported very soon in this okay if  you click on mial you will see mial yaml  so mial is also there 8X 7B model by  Mistral the mixture of experts now if  you go back to examples you will see Cod  llama Lama 2 Mamba Mistral MPT pythia  red Pama and some other LMS are also  there  let's open for example open Lama so if  you open open Lama you will see config  yaml now in config yaml you will see all  these details I will explain each and  everything you know in a bit what what  do we mean by loading 8bit loading 4bit  what is Lura R which is not there  because that will be in your QA or Lura  if you click on Lura yal this is a  configuration file which helps you you  know Excel toll basically takes this  single file as an input and F tune your  model here you have to Define your data  set if you look at the data set it says  gp4 llm clean there is nothing but this  is available on your hugging face  repository so basically T the data from  there but you can also do it from  locally as well so if you have local uh  uh machine where you are running this  and you have your data locally you can  also uh uh assign the path from that  local machine as well it has your  validation adapter blah blah blah and  we'll explain that everything uh in a  bit what do we mean by qware bits and  bite things will come in picture you can  see load in 4bit will be true for Q  because we want to load this model in  4bit and something like that right let's  do that so what I'm going to do is uh  first thing let me go back to my folder  here and you will have a requirements  txt let's expand that now you have your  requirements txt now in requirement txt  we'll find out everything okay what are  the requirements thing that you know you  have extra and Cuda you are getting from  here and lot of other things that you  are getting now uh we'll make some  hashes we don't need this or rather what  we can do we can just remove this from  here we do not need this anymore because  we already have Cuda with us over here  now Cuda is not required any other cudas  that you see I do not see any other  cudas over here I'm just looking for  torch Auto packaging pip Transformers  tokenizers bits and bytes accelerate  deep speed you know uh add  fire  pyl data  sets flash attention you can see 2.3.3  sentence piece wend B ws and biases eops  X forers Optimum HF Transformers Kama  Numba Bird score for evaluation evaluate  R rou score for Val evaluation as well  you know we have psychic learn art FS  chat graduate and fine let's save this  here here okay now once you save it I'll  just close this now let's go CD inside  it have to CD so let's do CD so CD XEL  to and you can see ax AO I am always you  know typing it wrong but I'll just CD  inside it and let's do a PWD to find out  what is my present working directory you  can see it Exel tall uh now the next  thing that we have to do is okay we have  to look at the inst installing the  packages let's do what they suggest I'm  going to do pip install  packaging the PIP install packaging will  is a requirement already satisfied the  next thing is PIP  install and then hyphen e which  basically says okay within this  directory or we can you can also take it  from you know their uh I'm saying okay  flash attention and deep speed so let me  just do not need that okay deep speed  okay flash attention and deep  speed uh to let's run  this what what are the thing that I am  making a mistake  uh deep speed flash attention H it  should not be dot here my bad it should  be dot should be the outside of  here okay you have your flash attention  and deep speed thing going on okay over  there it will install everything which  is in your requirements txt you can see  Bird score Numba sentence piece which  the dependency of Transformer helps with  you know vocabulary addition tokenizer  blah blah blah know pep has been  installed bits and bytes accelerate  that's looking good uh let's see it out  over there come  down flash attention make sure you have  flash attention greater than 2.0 flat  flash attention one point will give you  error okay uh let's come  down it's installing it once you install  that you have you can also go to the  data set path if you don't want to use  this data you can also do that with  other data as well but I will keep the  same  data but we can make few changes if you  want let's know because we are relying  on Laura here we'll use Lura so if you  come to Lura okay let me come to Laura  low rank adaptation now once you come to  Laura you can do a bit of changes like  you know let me see what is the  EPO okay uh where is that uh number of  epoch this is too much I don't want four  Epoch okay let's do it for one Epoch  okay so for we'll do it for one  Epoch okay the number of epo become your  number epox become one micro batch size  I will also make it one and gradient  accumulation steps I'll make this eight  okay I will explain that don't worry if  you don't understand these terms we'll  explain each and every this  terminologist which has been written  over here now I made some  changes uh and then I'll go back back  and then I will execute my learnings  okay that's what I'm going to do okay  now let me just do a  save okay uh and if you come you can  also see it over here this how you can  train it out you can see it says uses  this how you can train this is how you  can  inference all right so execution of  learning will happen like that so let's  come to fine tuning here okay now we are  done we'll add few  sales and once you have done all of  these changes let's go back and maybe  you can copy this thing uh quickly let's  copy  this let me come over  here and I have to add  that you can see excelerate launch  hyphen M Exel to CLI train examples it  has open  Llama now for example if currently you  see it says open Lama now you have to go  to open Lama 3B I don't know which one I  changed up okay uh if you go to  Laura yeah uh yeah this is what I also  changed okay fine now let me just uh  close this one low yl and let's run  it now once you run it you will see a  few warnings you can ignore those  warnings don't not have to worry about  those  warnings  you can see it's currently downloading  the data from hugging face it will you  know uh create a train and validation  splits you can see it's currently doing  it will also map the to with the  tokenizers will also do that you can see  it's happening happening over  here you have to wait for all these  process basically what they do guys  right it's imagine this as a low code  fine tuning tool if you don't want to  write a lot of code you just want to  make some changes because they already  have created the yaml for you okay this  is what they have done you can see total  number of steps total number of tokens  over  here find out the total number of tokens  currently getting the model file which  is PCH model. bin the file that it's  getting you can see it's around 6.85 GB  you know runport is basically will have  some cash of course definitely they will  have some cash and contain  cast in the container because you know  you are just fetching it from hugging  fish directly that's why it's bit faster  if you do it of course locally it will  probably be not that FAS okay uh it will  take a bit of time let's see how much  time it takes I don't want to pause the  video because I want to show you each  and everything that goes if you want to  you can skip the video of course you can  see you know extra special tokens EOS  BOS paddings total number of tokens  supervised tokens all those details over  here which have been listed okay that  you can find it out  once the model has been downloaded which  is your pytorch model. bin the next  thing that we have to do it will also  load the model okay so basically once it  load the model in the in that uh once it  loads the model you Cuda kernel should  be there because once it's loading it  needs GPU to load that okay on on a  device that's when we do dot two device  or something to you know pipe that with  CA  okay you can see it says starting  trainer so it has a trainer argument I  will explain that don't worry if you do  not know about trainers and all we'll  explain each and everything in detail  now guys what will happen you see can it  has started training okay you can see  efficiency estimate total number of  tokens per device it's training your on  your data now what I'm going to do is  I'm going to pause the video till it  train or F Tunes it and then we'll look  after that all right uh so you can see  uh it has been fine tuned the model that  we wanted to fine tune we have fine  tuned the llm and you can find out uh  train samples per second train runtime  train steps per second train loss EPO it  has been completed 100% it took more  than 1 hour it took around 2 hours you  know it took 1 hour 55 minutes you can  see it over here uh let's go back here  in the  workspace Exel  all and you can find out here you have a  folder called Laura out now in Lura out  you have your last checkpoint and you  can keep on saving checkpoints we'll  cover that maybe in the upcoming videos  where we will have a lot of videos on  fine tuning now but what I'm trying to  say is that you can also save it on  multiple checkpoints for example if you  want to use a th checkpoints with you  can also do that but you can see here we  have our adapter model. bin and adapter  config right now you can also merge that  with Pip but I'm not covering that part  right now here because you would have no  idea about pip if you a beginner so now  you just you just saw that how we find  tuna model and we got our checkpoint  weights over here you can find out this  is the last checkpoint if you want to  use this which has a config Json which  has if you click on the config Json it  will open a Json file for you then you  have your you know safe tensors okay the  model weights then you have Optimizer  dopy PT then you have your scheder you  have your Trend State you can file out  all the related information over here  you can also close this target module  seven items you can find out all the  different projections I will cover each  and everything in detail what do we mean  by this now it shows that uh sometimes  it gives you error uh if you look at  this gradi if you run this gradio  sometimes in runp it it gives you an  error also because let's try it out try  it out our luck here I just want to show  you that you can also do  that yeah gradio installation has issues  with in within runp pod sometimes when  you are using  it but anyway uh we'll move if it works  it will open a gradio uh for you a  gradio app gradio is a python library  that helps you build you know uh web  apps okay you can see it says running on  local over here okay now it runs on a  local URL okay you have to probably do a  true uh s equals to True something to  you know get it on a URL okay now you  can also click on let's click on this  now once you click on  this it says this share link expires in  72 hours for free permanent hosting and  GPU upgrades run gradate deploy okay now  you can see that we have an axel toall  gradio interface okay now here you can  try it out now if you let's copy this  and see what kind of response or even if  it's generating any response or not so  if you click on  this it will either give you an error or  it says okay you can see give three tips  for staying healthy and you can see it  out over here eat a healthy diet consume  plenty of fruits vegetables whole grains  lean proteins blah blah blah and you can  see the speed as well because we are on  800 right now that you know and it's  generating your responses pretty fast  right look at the tokens per second over  here now you have an interface that that  is working on your data your data that  you have from hugging face for example  we have used Alpa here Alpa 2K  test you can see it's how it says how  can we reduce air pollution let me ask a  similar question but not exactly the  same so what I'm going to ask here  is tell  me the ways I can  reduce pollution let me ask this  question and I'm asking tell me the ways  I can reduce pollution it says make a  conso and when you are working with  python you have to use a prom to look at  the uh basically to par the output a bit  but this is fantastic right now you can  see it  says make a conscious effort to walk  bike or car Pole to work is school or  irand rather than relying on a single  occupancy vehicle huge public  transportation car pooling blah blah  blah Reduce Reuse and recycle talking  about a bit of sustainability over here  now this is fantastic you saw in two  hours at least for one EPO of course  that is fully customizable as we saw in  the uh over there in the yaml files but  this is how easy it is nowadays to F  tune guys we'll now talk about we'll go  back to  our uh cast here and we'll talk about  each and everything in detail okay now  how did we find tune what are the sh  config that we have considered and all  of those things so let's start our  journey with all these parameters now  let's understand how Laura works and  what is Laura and how these parameters  are being decided that we have seen in  the yaml file as well so let me just  write about Laura here so we're going to  talk about Laura  now now it's a Training Method let's see  this as a Training Method let's keep it  lemon so you can understand better it's  a Training  Method so I'm writing it's a Training  Method to designed to ex designed to  expedite the training process of  llms training process of  llms this helps you with of course  memory consumption it reduces the memory  conj by introducing something called  pairs of rank decomposition weight  matrices this is important so what I'm  going to write here is look at this word  here okay something called rank  decomposition and basically that's a  pairs so I'm going to write pairs of  rank  decomposition  matrices  and this basically also known as right  we have a as we also talked about this  earlier called update  matrices  okay and you will have some existing  weights already of the pre-trained llms  that we're going to use so basically you  know it it helps you you know add these  weights okay training this basically the  new added weights  okay now  there are three things that you should  consider when we talking about loraa the  first is the or I'll say three or fours  let's let's understand that okay  preservation of pre-trained Weights so  let me just write it over  here so preservation  of pre-trained  Weights that's the first thing now what  we understand by this like Laura  basically maintains the Frozen state of  previously trained weights that  basically minimize the risk of  catastrophic  forgetting now this particular step  ensures that the model retains it  existing knowledge so let me just write  it over here okay  model  retains the its existing knowledge while  adopting to existing knowledge  while adapting to the new data that's  why we call it right that LM still has  the base knowledge and that's why  sometimes it hallucinates gives it from  the base knowledge right while adapting  to new  data now the second thing which is  important for you to understand the  portability of trained weights can you  port that okay so let me just write it  over here  portability  of trained  weights the rank decomposition matrices  that that gets used in loraa have  significantly fewer parameters of course  right that's why it helps you with the  memory reduction right when you compare  this with original model now this  particular characteristic allows the  trained loraa weights to be easily  transferred and utilized in other  context making them highly portable so  it's highly  portable now the third thing is the  third advantage that we're going to talk  about is integration with attention  layers integration with attention  layer now Lowa matrices are you know  Incorporated basically into the  attention layers of the original model  the adaptation scale parameters we we'll  see that parameter once we are looking  at the code allows control over the  extent to which model adjust to the new  training data and that's where we'll see  about you know alpha or the scales that  has been used now the next one is memory  efficiency that's the advantage that we  also talk about is memory  efficiency now it's improved memory  efficiency as we know opens up you know  the possibility of running fine tune  task unless then as we discussed earlier  3x the required compute for a native  fine tuning process say without Laura  let's look at the Lura hyper parameter  let's look at the code now to understand  uh the Lura hyper parameter so go to  examples of the same exal GitHub  repository let's open any we trained  basically open Lama right find now you  can also open Mistral Also let's open oh  let me open Mistral now to show  you so you can see I'm opening Mistral  and the mistal let's go to  Kora now on the cura I will first talk  about the Laura config let me make it  bigger so you can see it and let's talk  about this these are your Lura hyper  parameters that we're going to talk  about and I will explain that what we  mean by this you find tun any llms you  go to any videos any blog you will see  this code now you should have you should  have the understanding you should know  that what are these means okay so let me  just show you over s it over here that  what are these things mean now on the  memory efficiency  the next thing that we're going to talk  about is Lowa hyper  parameters so I'm just writing Lura  hyper  parameters now the first thing if you go  back you have something called Lura R  which is nothing but the Lowa rank okay  so let me just write first thing you  should know what is Lowa rank okay so  let me just write it like that okay  that's called basically your Lowa rank  low rank adaptation rank what do we mean  by lower rank gu this basically  determines the number of rank  decomposition  matrices rank decomposition is applied  to weight matrices in order to reduce  memory consumption and computational  requirements if you look at the original  Lura paper recommends a rank of R equals  to weight eight okay so by standard let  me just write here you know by the paper  they recommend standard r equal to 8 but  here you can see for mistal you know in  Axel to GitHub repository for mral is  it's has been written  32 and eight is the minimum amount keep  in mind that higher rank if you have  higher  rank leads to better  results better  results and there's one tradeoff now but  for that you need High compute  so now you should know that if you are  getting any errors like okay you have  you know taking a lot of computational  power you can play around this hyper  parameter which lower rank make it eight  from 32 make it 16 or or something like  that you know even know it's a trial and  error  experimentation now the more complex  your data set your rank should be higher  if you have a very complex data set  where you have lot of relationship a lot  of derived relations and so on and so  forth you need your rank to be on the  higher side okay okay now to match a you  know a a full fine tune you can set the  rank to equal to the models hidden size  this is however of course not  recommended because it's a massive waste  of resources now how can you find out  the models hidden size but of course you  have to go through the config Json by  the way okay or so there are two ways of  reading or basically finding out so  let's say is  reading hidden  size  now there are two way one is config  Json and the other is that you can do it  through  Transformers Auto  model Transformers Auto  model I I think I can quickly you know  write the code here so let me just write  the code for you now this is how you  have your auto model so from Transformer  let's quickly get  it so for example from Transformers  import you have Auto model as we as we  do it we do auto model for  Cal so you have your Cal LM that's how  you import you define a model name then  you'll have an  identifier so let's define that model  name whatever then you have your that  you basically get your model now through  Auto model something like that now how  do you find the hidden size this is the  code to get the hidden size  you do it something called Model do  config do hidden State that's how you do  it hidden  size and then just print the hidden  size This Is How We Do It print the  hidden size right let me open uh  here Lama  2 let's go to you know any of this okay  it's fine  you can see it over here hidden size is  here  8192  right this is your hidden size you can  find it like here also and you can also  find it you know through Transformers  Library as on but this is not  recommended because it's a waste of  resource otherwise why would you use  Lura then but this is what it is right  if you want to really achieve those  performances of pre-training uh like how  meta has created Lama 2 or M A has cre  Mistral the next is let's go back next  is Lura Alpha okay so let's write it  Alpha here this is a scaling Factor now  I'm writing Lowa  Alpha now it's a scaling factor for the  Lura  determines the extent to which the model  is adapted towards new TR the alpha  value adjust the contribution of the  update matrices during the training  process or during the train proc  process lower value gives more weight to  the original data so if you have lower  value it gives more weight to the  original and it maintains the models  existing knowledge to a greater extent  so it will it will be more inclined  towards the you know your base knowledge  the base data that you have so let me  just write it so for example you you  will able to uh make a note of it I will  write lower  value Gibs  more  weight to the original  data to the original data and  maintain maintain the  models existing  knowledge knowledge to a greater  extent let's try it like  this okay this is what now if you can  see here it says Lowa  Alpha and that Lowa Alpha is 16 you can  make eight also you can make 16 also  that depends now the next is let's talk  about Lowa Target modules which is again  very important you know talking about  the Lowa Target modules that you see  over there now Lowa Target modules is  one of the important thing so let me  just write  Lowa Target  module oh you cannot see it I just  noticed Lowa Target module now in the  Lowa Target  module here you can determine which  specific weights and matrices are to be  trained the most basic ones to train are  the query vectors that's basically  called Q projection okay you would have  seen that word q projection so the most  basic ones are the query ve query  vectors and value vectors so let me just  write it over here you know query  vectors and then you have value  vectors that's called Q projection Q Pro  and this called V  Pro these are all project projection  matrices the names of these matrices  will differ from model to model of of  course depends on which llm you are  using you can find out the exact names  again there's a what you have to do keep  the same code AS written above just make  one changes which is your layer  names so layer names we you know  basically it's a dictionary that's how  you'll get it let me so it write it over  here model. State  dict model do state dict and then do  keys this is how you will get the and  basically you need a for Loop now  because there will be n number of layers  so you can get the for name in layer  names and then you can print that so  you'll see Q projection you know you  will see K projection you will see V  projection you will see o projection you  will see gate projection you will see  down projection you will see up  projection layer Norm weight blah blah  blah right so there can be n number of  Weights now the naming convention is  very easy model name do layer layer  number component and then it goes like  that so this is what you should note the  Q projection the projection matri Matrix  apply to the query vectors in your  attention mechanism of the Transformer  blocks transforms the input hidden  states to the desired dimensions for  Effective query representation and V  projection is bit different it's called  a value vectors in the tension mechanism  transforms the input hidden states to  the desired dimension for Effective  value representation so these two are  very important Q Pro and V Pro there are  others like that like K Pro which is key  vectors then you have o Pro different so  you can keep on looking at you know  different basically oo are nothing but  the output to the attention  mechanism so you know so however three  or four you know there are outliers as  well we have to look at outliers they  don't follow the naming convention is  specified here that I'm writing but they  have embedding token weights embed  tokens normalization weights normalize  then you have LM heads these are also if  you go back by the way excuse me sorry  then you have your you know which is not  of course here here because this might  not be using it but you have LM head let  me just write it out over here you have  LM  head so you have LM head then you have  embed  tokens then you have embed tokens then  you have normalization Norm these are  also that we consider when we are  training it now the LM head is nothing  but the output layer of a language model  language modeling I will say rather it's  responsible for generating predictions  or scores for the next  token based on the Learned  representation from the preceding layers  the previous one you know basically they  are placed in the bottom so important to  Target if your data data set has custom  syntax LM head is very important let me  just write it over here if your  data if your data has custom syntax  which is you know really  complex embed token they represent the  parameters associated with the embedding  layers of the model is like very  self-explanatory usually placed at the  beginning of the model as it just has to  map input tokens to their Vector  representation you have your input  tokens you have to first convert it to a  vector then the model will be able to  understand right it it needs a numerical  representation it cannot understand your  text Data it's important to Target again  if you have your custom syntax so this  goes same now normalization is very like  know very I say  common it's a normalization layer within  your model used to improve the stability  and convergence so basically helps you  with the convergence of you know deep  neural  networks this is what we look at the  Lura guys then we look at the Q Laura  now in QA we talk  about few things we'll talk about if you  go back here let me just  explain QA bits and bytes will be on  somewhere here so basically just explain  so quantized Lura is an efficient fine  tuning approach you know even makes it  more uh memory reduction so it include  few things the number one is back  propagation so let me just write over  here back  propagation of  gradients through a through a 4bit  quantized you know through a frozen  let's write uh  through a frozen 4bit quantized 4bit  quantized which is very very important  quantized into  Laura that's the first thing the second  thing is it's basically uses of a new  data type called nf4 you would have seen  that  nf4 that's word now what do we mean by  nf4 that's called 4bit normal flat  excuse me not flat flat float 4bit  normal  float now 4bit normal float basically  you know optimally handles normally  distributed weights you will have your  distributed weights it basically helps  you optimize those it's a new data type  you see nf4 so if you make nf4 true you  have to use bits and bytes and all for  of course for that as well now you have  your  nf4 then you also have few other things  like paged optimizers double  quantization you know to reduce use the  average memory footprint even further by  quanti quantizing the quantization  constant now these are the things that  are associated with Laura and qora then  you have your hyper parameters like  batch size and aox now I'll explain that  you don't need the tab or this to  explain that because these are very  common so now you understand this part  this part is very much self-explanatory  a base model code of you know you use  any fine tuning code take it from medium  take it from other YouTube videos  anywhere you will find this code on  very similar 99% of the code will be  same only thing that you have to change  is your data and the way you tokenize  and map those that's it and few of the  times if you are using some other LMS  now your data set is this you have your  data set where you are you know storing  it data set  validations output  directory qora out then you have your  adapter which is qora then the sequence  length you can see it says  8192 P to sequence true Laura I I have  already explained this part here Wenda B  which is weights and biases okay weights  and biases for you know machine learning  experimentation uh tracking and  monitoring now we'll talk about uh a few  things which is important let me first  come down okay this is special tokens we  have covered it now let me just go up  and explain that to  you okay now what is number of epo guys  let's talk about about number of number  of epoch the number of  epoch excuse  me the number of AO is an hyper  parameter in gradient descent you would  have seen the maxima and Minima you  would have seen this that that hyper  parabolic or parabolic graph once we see  the way you know back propagation works  and the model learns and the when we  talk about neural networks hyper  parameters in gradient descent  which controls the number of complete  passes through the training data set  each Epoch involves processing the  entire data set once and the models  parameters are updated after every Epoch  now batch size is a hyper parameter in  again gradient descent that determines  the number of training samples processed  before it's in batches it stands in the  batches how many sample are used in each  iteration to calculate the error and  then adjust the model in your back  propagation steps we'll talk about  stochastic gradient descent it's an  optimiz algorithm as we discussed  earlier to find the best internal  parameters for a  model aiming to minimize performance  measures like logarithmic loss or a mean  square error msse and you can find it  out more on the on internet as well  about that yeah  so now batch gradient descent sastic  gradient descent and mini batch gradient  descent there are three different ways  commonly used you know for training the  models  effectively now  you would have sometime this question  the difference between the batch versus  Epoch so the batch size is the number of  samples processed before the model is  updated the number of epo is the number  of complete passes through the training  data set so these are two different NS  understand with a quick example I'll  just show it this is the last maybe that  I will  show but let me uh show that okay over  here now assume you have a data set so  you have a data set of 200  samples a rows or it can be question  answer pair of rows in a csb and you  choose a batch size of five so for  example batch size of five and AO is  th000 this will not we will not do it in  a fine tuning llm because the computer  to Too Much five now this means the data  set will divided into how many the data  set will be divided into 4 40 batches so  dat will be divided into 40  batches because you have 200 samples it  has to be in 40  batches 40 batches each with five  samples how many each with five samples  right now the model weights will be  updated after each batch of five samples  after each batch of five samples model  weights will update model weights will  get  updated now this will also mean that one  Epoch will involve 40 batches or 40  updates to the model how many one  aoch will involve 40  batches or 40 update this is very  important guys you should know the  fundamentals  to fine tune the best way right now with  1,000 aox the model will will be exposed  to the entire data set 1,000 times so  that is total of how many 40,000 batches  just imagine how much compute power do  you need to do that so the larger B size  results in higher GPU memory right  higher GPU  memory and that's where will be using  gradient accumulation steps you see this  here gradient accumulation steps that's  why it is  important right gradient uh that that  has been used to overcome this problem  okay over  there now you have learning rates you  can see  0.02 learning rate is not that like you  know so SGD stochastic gradient descent  know estimates the errors and the the  way it learns right so think of learning  rate as a no that control the size of  steps taken to improve the model if the  learning rate is too small the model may  take a long time to learn or get stuck  in a suboptimal solution it might might  get a local Minima right so on the other  hand if the learning rate is too large  the model May learn too quickly and end  up with unstable so you need to find the  optimal or the right learning rate right  you know it is important as well now the  learning rate what else do we have so  gradient accumulation is very important  so higher batch sizes results in higher  memory consumption  now that's where we bring gradient  accumulation Ms to fix this it's a  mechanism to split the batch of samples  so you will have your for example you  have a global badge let me just draw it  if I  can let me show it you cannot see if  here Global badge then you will have  your uh mini badge so let me call it MB  so you have  mb0 then you have mb1 mini batch one and  you can you can see it over here that's  called micro batch size two right it's  all associated with it mb1 then you have  mb2 and then you have mb3 now imagine if  you have this then you have grad zero  again this will be the inside only so  you have grad zero you have grad one you  have grad two gradient two basically and  then you have grad three and then you  have Global batch  gradient very very interesting right  Global batch gradients  now into basically it splits into  several mini batches of samples that  will be run sequentially right now I'm  not covering back propagation because I  hope that you know what is back  propagation if you don't I will  recommend to watch you know some of the  videos which are already available on uh  YouTube there will be n number of videos  for that but yeah I think that concludes  for I think this part guys so what we  did in this video you know now you'll be  able to to understand each and  everything over here now you know what  is gradient checkpointing and these are  like warm- up state after how many steps  you want it to warm up how after how  many EPO you want to evaluate the model  on your validation data whatever you  know your saves after how many EPO you  want to save the weights and all of  those things right so these are bit uh  again you can go and there can be others  uh parameters as well but I wanted to  cover as much as possible now what we  did in this video guys so far it's a big  video it's a lengthy video I know but I  will recommend you to watch this video  completely as I said in the beginning as  well we started with the understanding  of pre-training training llms we looked  at the three different approach let me  summarize it we looked at the three  different approaches pre-training fine  tuning and Laura and  Kora we covered all of this  theoretically till high level on here  and then we moved after training compute  to run pod and on the runp we set up  something called Excel toll you know  Excel toll is a low  code uh fine-tuning tool for you so when  I say low code you should understand how  it works but you know it's very very  very powerful it's really important to  use this kind of tool to help on you on  your uh productivity and it helps you  become more efficient once you are fine  tuning it so we looked at Excel toll to  fine tune a large language model on this  particular data which is Alpa data I  also shown that how where you can change  your data you can look at a jonl file  and then you can f tune right we F tune  it for 2 hours almost and we got a  gradio application that we saw you could  see that how easy it is to spin up a  gradio that's already there for you and  we tested out with couple of prompts and  then we are ending with all of the hyper  parameters that are important as are  required you should know it we explain  that I hope you understood you got some  clarity now and you will have enough  understanding to now fine tune an llm  this ends our experiment guys in this  video that's all for this video guys I  hope you now have the enough  understanding of how to fine-tune a  large language model and how to select  the optimal hyperparameters for the  fine-tuning task and how can you use  tool like Excel toall that's the low  code uh tool for fine-tuning llms this  was the agenda of the video as well to  give you enough understanding  and where I wanted to cover the  fundamentals of pre-training fine-tuning  and the novel techniques like Laura and  Cur if you have any thoughts or  feedbacks please let me know in the  comment box you can also reach out to me  through my social media channels please  find those details on the channel about  us on the channel  Banner that's all uh for this video  please like the video hit the like icon  and if you haven't subscribed the  Channel please do subscribe the channel  share the video and Channel with your  friends and appear thank you so much for  watching see you in the  next\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=mrKuDK9dGlg&list=PLrLEqwuz-mRIEtuUEN8sse2XyksKNN4Om\", add_video_info=False,\n",
    "    language=[\"en\", \"id\"],\n",
    "    translation=\"en\",\n",
    ")\n",
    "d = loader.load()\n",
    "d[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"hello everyone welcome to AI anytime  channel so I'm starting a new playlist  called finetuning of large language  models in this playlist I will cover you  know the different aspect of fine-tuning  a large language model how to you know  finetune on your data set you will have  your custom data sets you want to fine  tune llms in this entire playlist we'll  have multiple videos we'll start with  the fundamentals and we starting in this  video as well the fundamentals you know  the different steps of pre-training  fine-tuning and the Noel techniques like  Laura and Kora how to select the right  hyper parameters for fine tuning task  what other tools that you can use to  fine tune like Excel toll for example  that we'll cover in this video which is  a low code fine-tuning tool and you know  how to use Laura and Kora to fine tune  uh with reduced memory uh consumption  now in this playlist we'll have videos  to F tune Lama 2 F tune Mistral fine  tune open Lama and fine tune other open  source llms and also the Clos Source One  like for example you want to find tune  the GPT models for example 3.5 you know  on your data or the Vex AI models like  you know that that basically fuels the  Palm uh Google B the Palm 2 models we  also fine tune with those so this is  completely focused on fine tuning this  entire playlist will have at least 10 to  12 videos but the first video today we  have going to talk about how to you know  F tune an llm and how to select the tool  the techniques and the data set and how  to configure and set it up uh in your  infrastructure for example how to choose  a compute what kind of cloud providers  you can use so all those nuances will  cover in this video this is a beginner  friendly video where we when I'm uh  assuming that you don't have fine-tuning  uh experience prior to this video okay  so let's start our experimentation with  uh this first video where we look at the  fundamentals of pre-training fine-tuning  and the techniques like Laura and  Kora all right uh guys so when we talk  about you know training llms there are  essentially three approaches that we see  and that's what we're going to look at  here so what I'm going to write first is  training uh  llms and we'll understand uh in very  brief about these three approaches so  the three approach that I'm talking so  let me first write the three  approaches so the three approaches that  we're going to have is  pre-training and after pre-training we  have fine  tuning and then then we have on a very  high level there are more but I'm just  writing here which are the commonly used  that we have used to fine tune in the  community so these are three approaches  pre-training fine-tuning Laura and Q  Laura now fine tuning and luras and the  you know even if you go more Downstream  uh I'm keeping this separate because I  will explain why I'm keeping fine tuning  separate uh than Lowa of course you can  use use Laura qora to F tune we'll see  that let's start a bit with definition  we'll go into code we'll also find tune  uh  shortly but let's first try to  understand because when I talk to you  know people who are trying to fine tune  basically they lack the fundamentals or  basic basics of these you know natural  language or the terminologies that we  see so I was talking to many of my you  know um colleagues or Community friends  and I was ask them like what are Q  projections what are projections that  you see in the lowas that you target  modules or whatever they had no idea  about it and that's what I want to cover  here that your fundamentals and Basics  should be covered because rest of the  thing I will show that how easily you  can f tune with tools like Exel all and  we'll see that uh in a bit now what do  you mean by  pre-training now it's pre-training  involves several steps now imagine if  you have a massive data set of Text data  so let me just write it over here you  have  uh massive Text data now the source can  be you know Wikipedia already Twitter  now X and you have you have your own you  know kind of an um your own proprietary  data kind of a thing and these are  mainly in terabytes okay we talking  about if you go to Lama 2 they say two  or three trillion I think it's two  trilon tokens we'll come to the tokens  later but just imagine extracting or  creating two trillion tokens would need  at least even more than terabytes of  data for you so in the pre-training step  is first is that you need a massive text  Data okay in in terabytes once you have  that you then identify uh model  architecture so let me just write here  identify the model  architecture and here I'm covering  because these topics are really vast uh  but I'm just covering on the very high  level but even once you have text Data  the lot of pre-processing steps like you  know removing pii you know looking at  the D duplications kind of a thing a lot  of other things that we look at but  we'll explain that once we are  fine-tuning okay but here just on very  high level to give you a definition of  pre-training fine-tuning and lowas and  qas now  uh once you have a model architecture  chosen or created specifically for the  task at hand then you need a tokenizer  okay so you need a tokenizer Let me just  write it over  here you need a  tokenizer okay that is trained to  appropriately handle your data handle  the data ensuring that it can  efficiently encode and decode text so  here I'm going to write  the tokenizer will play two role one is  of  encoding and decoding not necessarily  always encoding and decoding depends  what kind of task you are executing  we'll see  that now your tokenizer is done then  once you have the tokenizer ready the  data set that you have the massive data  set that you created is then  pre-processed using the tokenizer  vocabulary okay so let me just write it  over here  data set  is  pre-processed data set is  pre-processed using the tokenizers vocab  okay so you create the vocab vocab or  vocabulary of your tokenizers using  libraries like sentence piece that goes  you know that that becomes one of the  additions in the Transformers I will see  that uh shortly now you have data set is  pre-processed using you know tokenizers  vocabulary so let me just write it over  here tokenizers  vocab okay now this is important because  this will convert the raw text that you  have you have a raw text into  a uh into a format that is suitable for  training the model now here you will  have once you have this after this St  you will have a  suitable  you know uh uh  data in a  format for  training right now and this steps  involve you know mapping uh tokens to  the corresponding IDs incorporating  necessarily special token so let me just  write it because once I'm explaining the  config file the yaml file once we find  tune where you have lot of you know  special tokens like EOS BOS blah blah  blah this is how we'll cover that so  suitable data in a format for training  now here what we do now this steps  involves so I will keep it uh like this  so This steps involve mapping tokens we  we you would have seen something called  map using when we are using tokenizer to  map it now mapping tokens let me just  write mapping tokens to the IDS to the  respective or I think it's let's  corresponding might be the right  word corresponding IDs and then you know  uh incorporation of or rather let's  right  incorporating  any special  tokens special tokens okay or attention  mask can be other thing as well I just  recall no attention  mask okay now once the data is  pre-processed now it is ready for uh  training by the way these are the these  are the part of the same so I will now  uh these are the part that depends what  kind of architecture you have chosen but  now your data is ready you can pre-train  your your you ready to go into the  pre-training phase now what happens in  the pre-training phase okay let me let  me write it over here  what  happens in the pre-training  phase now in pre-training Phase let me  just write it the model learns to  predict the next word in a  sentence learns to  predict  the next word in a  sentence that's what happens or not only  this because it can also fill in so  we'll talk about what is filling and  what is Generation fill in missing words  as well so you can do fill in missing  words as well so let me just write it  over  here correct this is what it does now  this process of course involves  optimizing the models parameters you  know like it's trial and error kind of  an experimentation right this model you  have to optimize through different  parameters and that's an iterative  training procedure basically that  maximizes the likelihood of generating  the correct word or sequence of word  given the context now to accomplish this  pre-training  phase we typically employs a variant of  the self-supervised the learning  technique the model is presented with  partially masked input sequences where  certain tokens are of course we intent  intentionally hide that and it must  predict those missing tokens based on  the surrounding  context and because we have a massive  amounts of data right so we can train it  on that now model gradually develops a  reach understanding of language patterns  grammar and the semantic  relationship now once we talk about  filling so you have this fill in missing  words that we are talking that's  something that has been called as masked  language  model masked language model which is  called  MLM and when you talk about  Generation  generation now that has been called as  as are not limited to by the way but  this has been called calar learning  model that's called Cal language  modeling excuse me not learning Cal  language model or  modeling and that's why we use Auto Cal  from know pre-trained or whatever when  you use a Transformers pipeline now  these are the most these are the used  one we will be focusing on this this is  what we're going to focus C language  modeling now unlike masked language  modeling where you know certain tokens  are masked and the model predicts those  missing tokens Cal language modeling  focus on predicting the next word in a  sentence given the preceding  context now this step whatever you just  uh you just uh you just have seen here  on the screen this step is a  pre-training step here your model is now  able to understand the language patterns  Now understand uh the IC relationship  and all of those things basically it  gets the general language knowledge  making the model A proficient language  encoder right but it lacks a specific  knowledge so  after after  pre-training right you have a model now  it capture General  language so let can just write capture  General  language but you know it lacks and this  is what we're going to look at the next  fine-tuning step it lacks specific  knowledge lack specific knowledge about  a particular task or  domain  or domain and that's why we fine tune  because we want to fine tune it for a  specific purpose now to bridge this Gap  a subsequent fine-tuning phase follows  pre-training right that's where fine  tuning comes in guys in the play now  we'll look we'll talk about fine tuning  right now we only covering theoretical  aspect here we'll go into code a bit uh  so uh in a bit now let's talk about fine  tuning if you want to skip this part you  can also skip this you know but I will  recommend that you should have the  knowledge otherwise you will not be able  to build or find you know whatever  you'll be only having very high level  knowledge if you want to achieve that  you don't have to even learn those  things those are so many low code no  code fine tuning tool just upload your  data and get a finetune model but if you  want to make your carer in this field  you should understand everything from  scratch which is required of course when  I say everything don't go and read now  Marco Chain by the way that will not  help  uh now once we talk about finetuning now  after the pre-training phase where where  the model learns General language  knowledge fine tuning  allows let me write with  this now fine tuning  allows us to  specialize specialize the models  capability specialize the llms I'll  write rather llms  capabilities and optimize its  performance and  optimize its  performance  okay or let me make it more descriptive  on  a  narrower or a downstream task or a task  is specific data set or something like  that right let me just write task is  specific data  set okay uh this is what we do now fine  tuning of course if you look at the  pre-training we have covered all of this  high level steps text Data identifying  the model architecture tokenizer blah  blah blah now fing also uh similarly  also it also has many  steps so you have you have a task  specific data set that has been gathered  that that will probably have labeled  examples relevant to the desired task  now for example let's talk about some a  very famous word nowadays in Industry  that has been used called instruct  tuning or instruction tuned model we  call about instruction tuned model now  for instruction tuned  model a data set of instru instruction  and response pair is gathered so first  thing that you need a data set here  again a task specific data set so for  this if you are talking about  instruction uh tuned model you need a  data set of your instruction and  response instruction and  response data set is gathered now these  are basically pairs it can be a question  answer pair now the finetuning data set  size is significantly smaller than than  the you know your pre-training so you  don't need that huge data set now but of  course even even for this case you need  a sufficient uh uh like data set having  you know at least 10,000 question answer  pairs or something like that  now uh when the pre-training model is  initialized with its previously learned  parameters the model is then trained on  task specific data set optimizing its  parameter to minimize a task is specific  loss function loss function means how of  the model was from the desired result  that basically very lemon terms now I'm  writing  here uh task  specific loss  function task specific loss function now  during the fine tuning process the  parameters that we will have from the  pre-train models are then adjusted okay  we we will'll focus on this word a lot  that's called adjusted but let me write  it over here only  now uh the params let me just write like  this the  params of the pre-trained  model of the pre-trained model are  adjusted  are adjusted  using you know and focus on this word  adjusted we always talk about adjusting  weights let's adjust the weight and all  of those adjustment that we do with the  weights it all Bo down to weights guys  when you talk about fine tuning Now  using something called gradient based  optimization algorithms not limited to  but this is what has been used in the  recent fine-tuned model that's called  gradient based optimization  algorithms and if you have worked with  deep learning you will probably know  about this gradient based optimization  alos now one of these alos can be uh  SGD okay let me just write SGD or  Adam uh whatever there are there are a  lot more okay now SGD for example Le I  hope I spelled it right uh sastic and I  hope this is right sastic gradient  descent now the gradients are computed  by back propagating the law so again the  back propagation comes in okay so it the  gradients are then first computed by  back propagation the LW through the  model's layers you will have n numbers  of layers allowing the model to learn  from its mistakes and update its  parameters accordingly now you will have  a back propagation working there for you  in this models layers that helps you the  model will learn from the mistakes every  time and then update the parameter so it  improves itself now to enhance the fine  tuning you will have you know different  additional techniques here so let me  just write it in the one point here more  about fine tuning then we move to Lura  and Q luras and then we go up in a bit  of fine tuning kind of a thing now  uh okay  so you can also improve as I was saying  you can improve through you know uh  learning rate  scheduling okay there are there lot of  other techniques that we can follow uh  learning rate  scheduling and regular regularization  method like dropouts or weight decays or  early stopping to prevent overfitting  and whatnot right now these techniques  help optimize the models generalization  always remember that your fin tuned  model should not memorize but should  generalize okay uh on your unseen data  or the validation of new data now model  generalization and prevent it from  memorizing as I said the train data set  too closely now this is on the fine  tuning step now we are moving towards  some of  the newly  developed  techniques uh in the ecosystem that's  called a  Lowa low rank  adaptation they have a very good  research paper on Laura everybody should  read that paper low rank  adaptation now Laura is interesting  without Laura we would not have seen  this you know uh I'll say the rise of  the open source llms like mral or Jer  whatever you know the way we find unit  because we do not have the resources to  do that right so for people like you  know the US the researchers the  developers the people working in  startups they need these kind of you  know Frameworks to work with now because  see fine tuning is expensive as you know  don't be live in a Dreamland if you are  a a researcher or that you will be able  to create something like llama to with a  limited budget it's not possible when  you see how Mistral has been trained  they had invested Millions into that so  you can of course fine tune a few models  on your small data set but the  performance then again will not be  similar so it's fine tuning age so let  me write ft is  expensive it's simple it's really  expensive that requires you know 100 of  GBS of virtual Rams the vram that's  where GPU comes in guys you know to  train you know multi-billion parameters  models to solve this specific problem  Laura was introduced okay it was  proposed uh you know it's compared to  compared to if you compare to fine  tuning opt opt 175 B with Adam that that  has been the benchmarked in the paper  also Lura can reduce the number of  trainable parameters by 10,000  times and the GPU memory requirement by  over three times just as in right now a  3X memory requirement reduction is still  in the realm of unfeasible for us people  like us because 3x is still not that  right then that's we need Kora we'll  talk about that okay so let's talk about  so if Laura is giving you for example  and we will cover Laura in detail that's  why I'm not writing a lot of things over  here we will because we have to work  with Laura and Kora when we are fine  tuning so we'll focus on that now  Lura uh 3x okay I was saying 3x X now 3x  memory requirement now  imagine but this is also not something  that we can work with it even we need  more and that's where Kora came in  picture Okay so with  Kora and if you don't understand right  now about Lura and K don't worry I will  cover that each and everything in detail  each and every line that we write in  luras and Q the code the confix whatever  the argument that we work with I will  explain each and every line now Kora  just word just add the word quantized so  I'm not writing everything but  quantization quantized okay a quantized  low rank adaptation it uses a library  that's called bits and bytes okay so it  uses a  library called bits and bytes B  andb this is a library that you know it  uses on the Fly By the way and it  achieves a near lossless  quantization so when I talk about  lossless your performance degradation is  not much of course there can be a bit of  performance degradation or there will be  reduction in the performance but that  might be myth nobody has you know has  done a really a research on big sample  that that really happens once you  quantize the llm but of course there can  be a bit of performance degradation but  that's why we calling it bits and  bites and that provides you a lossless  quantization of language models and  applies it to the Lowa procedure because  QA is based on luras by the way now this  results in a massive reductions in  memory  requirement enabling the training of  models as large as like Llama  270b Or you can do that on you know for  example  2x 2x RTX  3090 you can do that you can also uh and  and by the way if you compare let me  just uh write it okay let me just do it  if you compare without Kora this will  take around a00 which is one of the most  used GPU to train llms or create llms  you need which is by the way 800 is 80GB  80GB  GPU now you you need 16 x you need 16  different a00 gpus not one GPU node that  you need you need 16 okay so just  imagine how much it reduce if you use  Kora you know it boils down to 2 RTX 390  and of course if you have 1 a00 much  better okay to do that uh but just  imagine uh and the cost goes down once  you use loraa and  Q okay now let's see the question comes  in that okay we have been talking about  all of this thing how can we achieve our  task how can we you know F tune what  kind of machine we have to use and all  of the question that you will have in  your mind so for that I will tell you  that let's go into fine tuning so let me  just write over  here fine  tuning we are now going into uh fine  tuning  step uh in this tutorial that I'm  creating you know we'll be focusing of  you know 6B or 7B llms not more than  that because I want to keep it as a  tutorial you can explore yourself if you  have compute and all I do have compute  but you should learn and you should of  course do it yourself right now the  first thing that you think will come in  your mind is training  compute that tell me about training  compute and that's what I also you know  get worried okay training comput now for  training of course you need a GPU to do  that we'll talk about model and data set  separately that's something that we'll  have  something uh now if you want to F tune  uh model like Lama 27b or Mistral 7B let  me just write it over here now these are  the model I'm writing because this  models are commercially available to use  with an Enterprise you can make money  you know using these models guys because  you'll be building a product so don't  see that just you'll be making money by  using this model but once you build a  product on top of this now Lama to 7B  and mral 7B if you want to F tune on a  very uh on a very high level I'll say  these are the requirement that you need  the alha talk about  memory and this is based on Uther Uther  AI Transformers math 101 blog post I  will give the link in description if you  want to understand how calcul  calculations work you know to decide a  compute you should look at Transformer  math 101 blog post find the link in  description now now the memory  requirement is around let's keep it like  150 to  195  GB this would be the memory requirement  you know to F  tune okay  now so renting gpus now the next thing  is how to rent  gpus or how to use a GPU there are many  options that you can  consider uh one is uh the most I will  it's not in any order but I will write  in my Preferred Choice okay uh in an  order I rely on  runpod runp pod. I rely on runp pod then  there is V  AI then there is Lambda  labs and then then you have hyperscalers  like big players you know I will just  only write AWS say maker because I have  worked with it a lot so these are the  top and you have your Google collab also  how can I forget collab our life saer  right now run pod is my Preferred Choice  hands down run pod is my preferred  choice I will recommend you to choose  runp or between Lambda Labs but Lambda  Labs service support and all that are  very bad okay so runp pod is something  that I recommend it's extremely easy to  work with and also affordable if you're  looking for cheapest option that you can  afford which is V so let me call it  here cheapest I'm writing cheapest I'm  not writing affordable affordable is  like runp now here security can be a  concern so look at if you looking at  secure cloud or Community Cloud kind of  a thing I recommend runp that's I said  okay now if you have a super computer  you can also do with that but I do not  have  now the next thing is uh once you have  training compute decided once you have a  training compute then you have to gather  data  set I show you that how you can get a  data set if you do not have one the main  source for learning purpose is hugging  face data sets once you have the data  set and I'll cover a few things in data  set because you know once if you have  you are creating your own data set you  have to look at something called Uh  there's few things you have to look at  one is diversity in  data you do not want your models to only  do one very specific task you know you  should have a bit of diversity now  assumed a use case like you know you're  training a chat  model that does not mean that data would  only be about one specific type of chat  right you you will want to diversify the  chat examples you know the samples uh  like different scenarios so model can  learn how to generate outputs of various  type of input now imagine one of your  Ender comes in and start putting about  some question that your model have not  even seen you know so at Le those kind  of things so it it understands the  semantics better now the size of data  set on a very uh like a thumb Ru your  file size should be 10 MB that's how I  decide if you are using a CSV with  question answer payers your file size  should be 10 MB that's how I go with  okay 10 MB of your data okay now or at  least I'll recommend 10,000 question  answer  pair and the quality of data the this is  important because if you look at the  latest model and this is the most  important by the way this is the most  important thing F example F llm by  Microsoft or Orca for example these are  the examples where organizations have  shown that how a better quality data can  help you you know train an llm on a very  smaller size like 3B or 1.5b kind of a  parameter of parms this is important to  understand now we'll not talk about data  at this moment let's let's see that so  to use runp pod what we have to do is to  go to runp  pod uh dashboard here and you can see I  already have a template has been  deployed but it's very easy to do that  you have to come to secure Cloud you can  also look for Community Cloud but I  prefer this it's Community cloud is bit  cheaper because your GPU can be also  availability is an issue on community  Cloud because the same GPU can be rented  by somebody else's as well if you are  not using that now that's on the  community Cloud uh on secure Cloud they  have two things latest generation and  previous generation on latest generation  you will see the latest of gpus like h00  which gives you 80GB vram with 250 GB  Ram you know which which is the most  expensive one over here then you also  have previous generation with a00 then  that's what we're going to use okay so  a00 you can see almost $2 per hour so  one hour training you cost $2 so on for  example if you have decent enough data  set like 2,000 3,000 rows $5 you'll be  able to uh do it now uh for that let me  first show you a couple of things we  have eight eight maximum that you can uh  you know spin up over here the eight  different node of that particular 800  GPU but this is not what we want to do  okay uh so let's see how we can deploy  it uh uh how we can find unit so I  already have deployed what you have to  do you have to click on deploy and it  will deploy it will take a few minutes  to deploy it and then there will be  option to connect once you click on  connect it will show open Jupiter lab  and that's what I did over here I have a  Jupiter lab now the first thing that we  have to do is we have to get clone the  Excel toll so let's get clone so get  clone uh and then we'll take that from  here so come over here let me just click  on  this this this and then you clone you'll  see a folder here now let's let's go  inside the folder in the examples you  will see different models which are  supported you can also see the mial are  also supported very soon in this okay if  you click on mial you will see mial yaml  so mial is also there 8X 7B model by  Mistral the mixture of experts now if  you go back to examples you will see Cod  llama Lama 2 Mamba Mistral MPT pythia  red Pama and some other LMS are also  there  let's open for example open Lama so if  you open open Lama you will see config  yaml now in config yaml you will see all  these details I will explain each and  everything you know in a bit what what  do we mean by loading 8bit loading 4bit  what is Lura R which is not there  because that will be in your QA or Lura  if you click on Lura yal this is a  configuration file which helps you you  know Excel toll basically takes this  single file as an input and F tune your  model here you have to Define your data  set if you look at the data set it says  gp4 llm clean there is nothing but this  is available on your hugging face  repository so basically T the data from  there but you can also do it from  locally as well so if you have local uh  uh machine where you are running this  and you have your data locally you can  also uh uh assign the path from that  local machine as well it has your  validation adapter blah blah blah and  we'll explain that everything uh in a  bit what do we mean by qware bits and  bite things will come in picture you can  see load in 4bit will be true for Q  because we want to load this model in  4bit and something like that right let's  do that so what I'm going to do is uh  first thing let me go back to my folder  here and you will have a requirements  txt let's expand that now you have your  requirements txt now in requirement txt  we'll find out everything okay what are  the requirements thing that you know you  have extra and Cuda you are getting from  here and lot of other things that you  are getting now uh we'll make some  hashes we don't need this or rather what  we can do we can just remove this from  here we do not need this anymore because  we already have Cuda with us over here  now Cuda is not required any other cudas  that you see I do not see any other  cudas over here I'm just looking for  torch Auto packaging pip Transformers  tokenizers bits and bytes accelerate  deep speed you know uh add  fire  pyl data  sets flash attention you can see 2.3.3  sentence piece wend B ws and biases eops  X forers Optimum HF Transformers Kama  Numba Bird score for evaluation evaluate  R rou score for Val evaluation as well  you know we have psychic learn art FS  chat graduate and fine let's save this  here here okay now once you save it I'll  just close this now let's go CD inside  it have to CD so let's do CD so CD XEL  to and you can see ax AO I am always you  know typing it wrong but I'll just CD  inside it and let's do a PWD to find out  what is my present working directory you  can see it Exel tall uh now the next  thing that we have to do is okay we have  to look at the inst installing the  packages let's do what they suggest I'm  going to do pip install  packaging the PIP install packaging will  is a requirement already satisfied the  next thing is PIP  install and then hyphen e which  basically says okay within this  directory or we can you can also take it  from you know their uh I'm saying okay  flash attention and deep speed so let me  just do not need that okay deep speed  okay flash attention and deep  speed uh to let's run  this what what are the thing that I am  making a mistake  uh deep speed flash attention H it  should not be dot here my bad it should  be dot should be the outside of  here okay you have your flash attention  and deep speed thing going on okay over  there it will install everything which  is in your requirements txt you can see  Bird score Numba sentence piece which  the dependency of Transformer helps with  you know vocabulary addition tokenizer  blah blah blah know pep has been  installed bits and bytes accelerate  that's looking good uh let's see it out  over there come  down flash attention make sure you have  flash attention greater than 2.0 flat  flash attention one point will give you  error okay uh let's come  down it's installing it once you install  that you have you can also go to the  data set path if you don't want to use  this data you can also do that with  other data as well but I will keep the  same  data but we can make few changes if you  want let's know because we are relying  on Laura here we'll use Lura so if you  come to Lura okay let me come to Laura  low rank adaptation now once you come to  Laura you can do a bit of changes like  you know let me see what is the  EPO okay uh where is that uh number of  epoch this is too much I don't want four  Epoch okay let's do it for one Epoch  okay so for we'll do it for one  Epoch okay the number of epo become your  number epox become one micro batch size  I will also make it one and gradient  accumulation steps I'll make this eight  okay I will explain that don't worry if  you don't understand these terms we'll  explain each and every this  terminologist which has been written  over here now I made some  changes uh and then I'll go back back  and then I will execute my learnings  okay that's what I'm going to do okay  now let me just do a  save okay uh and if you come you can  also see it over here this how you can  train it out you can see it says uses  this how you can train this is how you  can  inference all right so execution of  learning will happen like that so let's  come to fine tuning here okay now we are  done we'll add few  sales and once you have done all of  these changes let's go back and maybe  you can copy this thing uh quickly let's  copy  this let me come over  here and I have to add  that you can see excelerate launch  hyphen M Exel to CLI train examples it  has open  Llama now for example if currently you  see it says open Lama now you have to go  to open Lama 3B I don't know which one I  changed up okay uh if you go to  Laura yeah uh yeah this is what I also  changed okay fine now let me just uh  close this one low yl and let's run  it now once you run it you will see a  few warnings you can ignore those  warnings don't not have to worry about  those  warnings  you can see it's currently downloading  the data from hugging face it will you  know uh create a train and validation  splits you can see it's currently doing  it will also map the to with the  tokenizers will also do that you can see  it's happening happening over  here you have to wait for all these  process basically what they do guys  right it's imagine this as a low code  fine tuning tool if you don't want to  write a lot of code you just want to  make some changes because they already  have created the yaml for you okay this  is what they have done you can see total  number of steps total number of tokens  over  here find out the total number of tokens  currently getting the model file which  is PCH model. bin the file that it's  getting you can see it's around 6.85 GB  you know runport is basically will have  some cash of course definitely they will  have some cash and contain  cast in the container because you know  you are just fetching it from hugging  fish directly that's why it's bit faster  if you do it of course locally it will  probably be not that FAS okay uh it will  take a bit of time let's see how much  time it takes I don't want to pause the  video because I want to show you each  and everything that goes if you want to  you can skip the video of course you can  see you know extra special tokens EOS  BOS paddings total number of tokens  supervised tokens all those details over  here which have been listed okay that  you can find it out  once the model has been downloaded which  is your pytorch model. bin the next  thing that we have to do it will also  load the model okay so basically once it  load the model in the in that uh once it  loads the model you Cuda kernel should  be there because once it's loading it  needs GPU to load that okay on on a  device that's when we do dot two device  or something to you know pipe that with  CA  okay you can see it says starting  trainer so it has a trainer argument I  will explain that don't worry if you do  not know about trainers and all we'll  explain each and everything in detail  now guys what will happen you see can it  has started training okay you can see  efficiency estimate total number of  tokens per device it's training your on  your data now what I'm going to do is  I'm going to pause the video till it  train or F Tunes it and then we'll look  after that all right uh so you can see  uh it has been fine tuned the model that  we wanted to fine tune we have fine  tuned the llm and you can find out uh  train samples per second train runtime  train steps per second train loss EPO it  has been completed 100% it took more  than 1 hour it took around 2 hours you  know it took 1 hour 55 minutes you can  see it over here uh let's go back here  in the  workspace Exel  all and you can find out here you have a  folder called Laura out now in Lura out  you have your last checkpoint and you  can keep on saving checkpoints we'll  cover that maybe in the upcoming videos  where we will have a lot of videos on  fine tuning now but what I'm trying to  say is that you can also save it on  multiple checkpoints for example if you  want to use a th checkpoints with you  can also do that but you can see here we  have our adapter model. bin and adapter  config right now you can also merge that  with Pip but I'm not covering that part  right now here because you would have no  idea about pip if you a beginner so now  you just you just saw that how we find  tuna model and we got our checkpoint  weights over here you can find out this  is the last checkpoint if you want to  use this which has a config Json which  has if you click on the config Json it  will open a Json file for you then you  have your you know safe tensors okay the  model weights then you have Optimizer  dopy PT then you have your scheder you  have your Trend State you can file out  all the related information over here  you can also close this target module  seven items you can find out all the  different projections I will cover each  and everything in detail what do we mean  by this now it shows that uh sometimes  it gives you error uh if you look at  this gradi if you run this gradio  sometimes in runp it it gives you an  error also because let's try it out try  it out our luck here I just want to show  you that you can also do  that yeah gradio installation has issues  with in within runp pod sometimes when  you are using  it but anyway uh we'll move if it works  it will open a gradio uh for you a  gradio app gradio is a python library  that helps you build you know uh web  apps okay you can see it says running on  local over here okay now it runs on a  local URL okay you have to probably do a  true uh s equals to True something to  you know get it on a URL okay now you  can also click on let's click on this  now once you click on  this it says this share link expires in  72 hours for free permanent hosting and  GPU upgrades run gradate deploy okay now  you can see that we have an axel toall  gradio interface okay now here you can  try it out now if you let's copy this  and see what kind of response or even if  it's generating any response or not so  if you click on  this it will either give you an error or  it says okay you can see give three tips  for staying healthy and you can see it  out over here eat a healthy diet consume  plenty of fruits vegetables whole grains  lean proteins blah blah blah and you can  see the speed as well because we are on  800 right now that you know and it's  generating your responses pretty fast  right look at the tokens per second over  here now you have an interface that that  is working on your data your data that  you have from hugging face for example  we have used Alpa here Alpa 2K  test you can see it's how it says how  can we reduce air pollution let me ask a  similar question but not exactly the  same so what I'm going to ask here  is tell  me the ways I can  reduce pollution let me ask this  question and I'm asking tell me the ways  I can reduce pollution it says make a  conso and when you are working with  python you have to use a prom to look at  the uh basically to par the output a bit  but this is fantastic right now you can  see it  says make a conscious effort to walk  bike or car Pole to work is school or  irand rather than relying on a single  occupancy vehicle huge public  transportation car pooling blah blah  blah Reduce Reuse and recycle talking  about a bit of sustainability over here  now this is fantastic you saw in two  hours at least for one EPO of course  that is fully customizable as we saw in  the uh over there in the yaml files but  this is how easy it is nowadays to F  tune guys we'll now talk about we'll go  back to  our uh cast here and we'll talk about  each and everything in detail okay now  how did we find tune what are the sh  config that we have considered and all  of those things so let's start our  journey with all these parameters now  let's understand how Laura works and  what is Laura and how these parameters  are being decided that we have seen in  the yaml file as well so let me just  write about Laura here so we're going to  talk about Laura  now now it's a Training Method let's see  this as a Training Method let's keep it  lemon so you can understand better it's  a Training  Method so I'm writing it's a Training  Method to designed to ex designed to  expedite the training process of  llms training process of  llms this helps you with of course  memory consumption it reduces the memory  conj by introducing something called  pairs of rank decomposition weight  matrices this is important so what I'm  going to write here is look at this word  here okay something called rank  decomposition and basically that's a  pairs so I'm going to write pairs of  rank  decomposition  matrices  and this basically also known as right  we have a as we also talked about this  earlier called update  matrices  okay and you will have some existing  weights already of the pre-trained llms  that we're going to use so basically you  know it it helps you you know add these  weights okay training this basically the  new added weights  okay now  there are three things that you should  consider when we talking about loraa the  first is the or I'll say three or fours  let's let's understand that okay  preservation of pre-trained Weights so  let me just write it over  here so preservation  of pre-trained  Weights that's the first thing now what  we understand by this like Laura  basically maintains the Frozen state of  previously trained weights that  basically minimize the risk of  catastrophic  forgetting now this particular step  ensures that the model retains it  existing knowledge so let me just write  it over here okay  model  retains the its existing knowledge while  adopting to existing knowledge  while adapting to the new data that's  why we call it right that LM still has  the base knowledge and that's why  sometimes it hallucinates gives it from  the base knowledge right while adapting  to new  data now the second thing which is  important for you to understand the  portability of trained weights can you  port that okay so let me just write it  over here  portability  of trained  weights the rank decomposition matrices  that that gets used in loraa have  significantly fewer parameters of course  right that's why it helps you with the  memory reduction right when you compare  this with original model now this  particular characteristic allows the  trained loraa weights to be easily  transferred and utilized in other  context making them highly portable so  it's highly  portable now the third thing is the  third advantage that we're going to talk  about is integration with attention  layers integration with attention  layer now Lowa matrices are you know  Incorporated basically into the  attention layers of the original model  the adaptation scale parameters we we'll  see that parameter once we are looking  at the code allows control over the  extent to which model adjust to the new  training data and that's where we'll see  about you know alpha or the scales that  has been used now the next one is memory  efficiency that's the advantage that we  also talk about is memory  efficiency now it's improved memory  efficiency as we know opens up you know  the possibility of running fine tune  task unless then as we discussed earlier  3x the required compute for a native  fine tuning process say without Laura  let's look at the Lura hyper parameter  let's look at the code now to understand  uh the Lura hyper parameter so go to  examples of the same exal GitHub  repository let's open any we trained  basically open Lama right find now you  can also open Mistral Also let's open oh  let me open Mistral now to show  you so you can see I'm opening Mistral  and the mistal let's go to  Kora now on the cura I will first talk  about the Laura config let me make it  bigger so you can see it and let's talk  about this these are your Lura hyper  parameters that we're going to talk  about and I will explain that what we  mean by this you find tun any llms you  go to any videos any blog you will see  this code now you should have you should  have the understanding you should know  that what are these means okay so let me  just show you over s it over here that  what are these things mean now on the  memory efficiency  the next thing that we're going to talk  about is Lowa hyper  parameters so I'm just writing Lura  hyper  parameters now the first thing if you go  back you have something called Lura R  which is nothing but the Lowa rank okay  so let me just write first thing you  should know what is Lowa rank okay so  let me just write it like that okay  that's called basically your Lowa rank  low rank adaptation rank what do we mean  by lower rank gu this basically  determines the number of rank  decomposition  matrices rank decomposition is applied  to weight matrices in order to reduce  memory consumption and computational  requirements if you look at the original  Lura paper recommends a rank of R equals  to weight eight okay so by standard let  me just write here you know by the paper  they recommend standard r equal to 8 but  here you can see for mistal you know in  Axel to GitHub repository for mral is  it's has been written  32 and eight is the minimum amount keep  in mind that higher rank if you have  higher  rank leads to better  results better  results and there's one tradeoff now but  for that you need High compute  so now you should know that if you are  getting any errors like okay you have  you know taking a lot of computational  power you can play around this hyper  parameter which lower rank make it eight  from 32 make it 16 or or something like  that you know even know it's a trial and  error  experimentation now the more complex  your data set your rank should be higher  if you have a very complex data set  where you have lot of relationship a lot  of derived relations and so on and so  forth you need your rank to be on the  higher side okay okay now to match a you  know a a full fine tune you can set the  rank to equal to the models hidden size  this is however of course not  recommended because it's a massive waste  of resources now how can you find out  the models hidden size but of course you  have to go through the config Json by  the way okay or so there are two ways of  reading or basically finding out so  let's say is  reading hidden  size  now there are two way one is config  Json and the other is that you can do it  through  Transformers Auto  model Transformers Auto  model I I think I can quickly you know  write the code here so let me just write  the code for you now this is how you  have your auto model so from Transformer  let's quickly get  it so for example from Transformers  import you have Auto model as we as we  do it we do auto model for  Cal so you have your Cal LM that's how  you import you define a model name then  you'll have an  identifier so let's define that model  name whatever then you have your that  you basically get your model now through  Auto model something like that now how  do you find the hidden size this is the  code to get the hidden size  you do it something called Model do  config do hidden State that's how you do  it hidden  size and then just print the hidden  size This Is How We Do It print the  hidden size right let me open uh  here Lama  2 let's go to you know any of this okay  it's fine  you can see it over here hidden size is  here  8192  right this is your hidden size you can  find it like here also and you can also  find it you know through Transformers  Library as on but this is not  recommended because it's a waste of  resource otherwise why would you use  Lura then but this is what it is right  if you want to really achieve those  performances of pre-training uh like how  meta has created Lama 2 or M A has cre  Mistral the next is let's go back next  is Lura Alpha okay so let's write it  Alpha here this is a scaling Factor now  I'm writing Lowa  Alpha now it's a scaling factor for the  Lura  determines the extent to which the model  is adapted towards new TR the alpha  value adjust the contribution of the  update matrices during the training  process or during the train proc  process lower value gives more weight to  the original data so if you have lower  value it gives more weight to the  original and it maintains the models  existing knowledge to a greater extent  so it will it will be more inclined  towards the you know your base knowledge  the base data that you have so let me  just write it so for example you you  will able to uh make a note of it I will  write lower  value Gibs  more  weight to the original  data to the original data and  maintain maintain the  models existing  knowledge knowledge to a greater  extent let's try it like  this okay this is what now if you can  see here it says Lowa  Alpha and that Lowa Alpha is 16 you can  make eight also you can make 16 also  that depends now the next is let's talk  about Lowa Target modules which is again  very important you know talking about  the Lowa Target modules that you see  over there now Lowa Target modules is  one of the important thing so let me  just write  Lowa Target  module oh you cannot see it I just  noticed Lowa Target module now in the  Lowa Target  module here you can determine which  specific weights and matrices are to be  trained the most basic ones to train are  the query vectors that's basically  called Q projection okay you would have  seen that word q projection so the most  basic ones are the query ve query  vectors and value vectors so let me just  write it over here you know query  vectors and then you have value  vectors that's called Q projection Q Pro  and this called V  Pro these are all project projection  matrices the names of these matrices  will differ from model to model of of  course depends on which llm you are  using you can find out the exact names  again there's a what you have to do keep  the same code AS written above just make  one changes which is your layer  names so layer names we you know  basically it's a dictionary that's how  you'll get it let me so it write it over  here model. State  dict model do state dict and then do  keys this is how you will get the and  basically you need a for Loop now  because there will be n number of layers  so you can get the for name in layer  names and then you can print that so  you'll see Q projection you know you  will see K projection you will see V  projection you will see o projection you  will see gate projection you will see  down projection you will see up  projection layer Norm weight blah blah  blah right so there can be n number of  Weights now the naming convention is  very easy model name do layer layer  number component and then it goes like  that so this is what you should note the  Q projection the projection matri Matrix  apply to the query vectors in your  attention mechanism of the Transformer  blocks transforms the input hidden  states to the desired dimensions for  Effective query representation and V  projection is bit different it's called  a value vectors in the tension mechanism  transforms the input hidden states to  the desired dimension for Effective  value representation so these two are  very important Q Pro and V Pro there are  others like that like K Pro which is key  vectors then you have o Pro different so  you can keep on looking at you know  different basically oo are nothing but  the output to the attention  mechanism so you know so however three  or four you know there are outliers as  well we have to look at outliers they  don't follow the naming convention is  specified here that I'm writing but they  have embedding token weights embed  tokens normalization weights normalize  then you have LM heads these are also if  you go back by the way excuse me sorry  then you have your you know which is not  of course here here because this might  not be using it but you have LM head let  me just write it out over here you have  LM  head so you have LM head then you have  embed  tokens then you have embed tokens then  you have normalization Norm these are  also that we consider when we are  training it now the LM head is nothing  but the output layer of a language model  language modeling I will say rather it's  responsible for generating predictions  or scores for the next  token based on the Learned  representation from the preceding layers  the previous one you know basically they  are placed in the bottom so important to  Target if your data data set has custom  syntax LM head is very important let me  just write it over here if your  data if your data has custom syntax  which is you know really  complex embed token they represent the  parameters associated with the embedding  layers of the model is like very  self-explanatory usually placed at the  beginning of the model as it just has to  map input tokens to their Vector  representation you have your input  tokens you have to first convert it to a  vector then the model will be able to  understand right it it needs a numerical  representation it cannot understand your  text Data it's important to Target again  if you have your custom syntax so this  goes same now normalization is very like  know very I say  common it's a normalization layer within  your model used to improve the stability  and convergence so basically helps you  with the convergence of you know deep  neural  networks this is what we look at the  Lura guys then we look at the Q Laura  now in QA we talk  about few things we'll talk about if you  go back here let me just  explain QA bits and bytes will be on  somewhere here so basically just explain  so quantized Lura is an efficient fine  tuning approach you know even makes it  more uh memory reduction so it include  few things the number one is back  propagation so let me just write over  here back  propagation of  gradients through a through a 4bit  quantized you know through a frozen  let's write uh  through a frozen 4bit quantized 4bit  quantized which is very very important  quantized into  Laura that's the first thing the second  thing is it's basically uses of a new  data type called nf4 you would have seen  that  nf4 that's word now what do we mean by  nf4 that's called 4bit normal flat  excuse me not flat flat float 4bit  normal  float now 4bit normal float basically  you know optimally handles normally  distributed weights you will have your  distributed weights it basically helps  you optimize those it's a new data type  you see nf4 so if you make nf4 true you  have to use bits and bytes and all for  of course for that as well now you have  your  nf4 then you also have few other things  like paged optimizers double  quantization you know to reduce use the  average memory footprint even further by  quanti quantizing the quantization  constant now these are the things that  are associated with Laura and qora then  you have your hyper parameters like  batch size and aox now I'll explain that  you don't need the tab or this to  explain that because these are very  common so now you understand this part  this part is very much self-explanatory  a base model code of you know you use  any fine tuning code take it from medium  take it from other YouTube videos  anywhere you will find this code on  very similar 99% of the code will be  same only thing that you have to change  is your data and the way you tokenize  and map those that's it and few of the  times if you are using some other LMS  now your data set is this you have your  data set where you are you know storing  it data set  validations output  directory qora out then you have your  adapter which is qora then the sequence  length you can see it says  8192 P to sequence true Laura I I have  already explained this part here Wenda B  which is weights and biases okay weights  and biases for you know machine learning  experimentation uh tracking and  monitoring now we'll talk about uh a few  things which is important let me first  come down okay this is special tokens we  have covered it now let me just go up  and explain that to  you okay now what is number of epo guys  let's talk about about number of number  of epoch the number of  epoch excuse  me the number of AO is an hyper  parameter in gradient descent you would  have seen the maxima and Minima you  would have seen this that that hyper  parabolic or parabolic graph once we see  the way you know back propagation works  and the model learns and the when we  talk about neural networks hyper  parameters in gradient descent  which controls the number of complete  passes through the training data set  each Epoch involves processing the  entire data set once and the models  parameters are updated after every Epoch  now batch size is a hyper parameter in  again gradient descent that determines  the number of training samples processed  before it's in batches it stands in the  batches how many sample are used in each  iteration to calculate the error and  then adjust the model in your back  propagation steps we'll talk about  stochastic gradient descent it's an  optimiz algorithm as we discussed  earlier to find the best internal  parameters for a  model aiming to minimize performance  measures like logarithmic loss or a mean  square error msse and you can find it  out more on the on internet as well  about that yeah  so now batch gradient descent sastic  gradient descent and mini batch gradient  descent there are three different ways  commonly used you know for training the  models  effectively now  you would have sometime this question  the difference between the batch versus  Epoch so the batch size is the number of  samples processed before the model is  updated the number of epo is the number  of complete passes through the training  data set so these are two different NS  understand with a quick example I'll  just show it this is the last maybe that  I will  show but let me uh show that okay over  here now assume you have a data set so  you have a data set of 200  samples a rows or it can be question  answer pair of rows in a csb and you  choose a batch size of five so for  example batch size of five and AO is  th000 this will not we will not do it in  a fine tuning llm because the computer  to Too Much five now this means the data  set will divided into how many the data  set will be divided into 4 40 batches so  dat will be divided into 40  batches because you have 200 samples it  has to be in 40  batches 40 batches each with five  samples how many each with five samples  right now the model weights will be  updated after each batch of five samples  after each batch of five samples model  weights will update model weights will  get  updated now this will also mean that one  Epoch will involve 40 batches or 40  updates to the model how many one  aoch will involve 40  batches or 40 update this is very  important guys you should know the  fundamentals  to fine tune the best way right now with  1,000 aox the model will will be exposed  to the entire data set 1,000 times so  that is total of how many 40,000 batches  just imagine how much compute power do  you need to do that so the larger B size  results in higher GPU memory right  higher GPU  memory and that's where will be using  gradient accumulation steps you see this  here gradient accumulation steps that's  why it is  important right gradient uh that that  has been used to overcome this problem  okay over  there now you have learning rates you  can see  0.02 learning rate is not that like you  know so SGD stochastic gradient descent  know estimates the errors and the the  way it learns right so think of learning  rate as a no that control the size of  steps taken to improve the model if the  learning rate is too small the model may  take a long time to learn or get stuck  in a suboptimal solution it might might  get a local Minima right so on the other  hand if the learning rate is too large  the model May learn too quickly and end  up with unstable so you need to find the  optimal or the right learning rate right  you know it is important as well now the  learning rate what else do we have so  gradient accumulation is very important  so higher batch sizes results in higher  memory consumption  now that's where we bring gradient  accumulation Ms to fix this it's a  mechanism to split the batch of samples  so you will have your for example you  have a global badge let me just draw it  if I  can let me show it you cannot see if  here Global badge then you will have  your uh mini badge so let me call it MB  so you have  mb0 then you have mb1 mini batch one and  you can you can see it over here that's  called micro batch size two right it's  all associated with it mb1 then you have  mb2 and then you have mb3 now imagine if  you have this then you have grad zero  again this will be the inside only so  you have grad zero you have grad one you  have grad two gradient two basically and  then you have grad three and then you  have Global batch  gradient very very interesting right  Global batch gradients  now into basically it splits into  several mini batches of samples that  will be run sequentially right now I'm  not covering back propagation because I  hope that you know what is back  propagation if you don't I will  recommend to watch you know some of the  videos which are already available on uh  YouTube there will be n number of videos  for that but yeah I think that concludes  for I think this part guys so what we  did in this video you know now you'll be  able to to understand each and  everything over here now you know what  is gradient checkpointing and these are  like warm- up state after how many steps  you want it to warm up how after how  many EPO you want to evaluate the model  on your validation data whatever you  know your saves after how many EPO you  want to save the weights and all of  those things right so these are bit uh  again you can go and there can be others  uh parameters as well but I wanted to  cover as much as possible now what we  did in this video guys so far it's a big  video it's a lengthy video I know but I  will recommend you to watch this video  completely as I said in the beginning as  well we started with the understanding  of pre-training training llms we looked  at the three different approach let me  summarize it we looked at the three  different approaches pre-training fine  tuning and Laura and  Kora we covered all of this  theoretically till high level on here  and then we moved after training compute  to run pod and on the runp we set up  something called Excel toll you know  Excel toll is a low  code uh fine-tuning tool for you so when  I say low code you should understand how  it works but you know it's very very  very powerful it's really important to  use this kind of tool to help on you on  your uh productivity and it helps you  become more efficient once you are fine  tuning it so we looked at Excel toll to  fine tune a large language model on this  particular data which is Alpa data I  also shown that how where you can change  your data you can look at a jonl file  and then you can f tune right we F tune  it for 2 hours almost and we got a  gradio application that we saw you could  see that how easy it is to spin up a  gradio that's already there for you and  we tested out with couple of prompts and  then we are ending with all of the hyper  parameters that are important as are  required you should know it we explain  that I hope you understood you got some  clarity now and you will have enough  understanding to now fine tune an llm  this ends our experiment guys in this  video that's all for this video guys I  hope you now have the enough  understanding of how to fine-tune a  large language model and how to select  the optimal hyperparameters for the  fine-tuning task and how can you use  tool like Excel toall that's the low  code uh tool for fine-tuning llms this  was the agenda of the video as well to  give you enough understanding  and where I wanted to cover the  fundamentals of pre-training fine-tuning  and the novel techniques like Laura and  Cur if you have any thoughts or  feedbacks please let me know in the  comment box you can also reach out to me  through my social media channels please  find those details on the channel about  us on the channel  Banner that's all uh for this video  please like the video hit the like icon  and if you haven't subscribed the  Channel please do subscribe the channel  share the video and Channel with your  friends and appear thank you so much for  watching see you in the  next\", metadata={'source': 'mrKuDK9dGlg'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma  # chroma for storing vector store locally\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings # for converting text to embedings\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "load_dotenv()\n",
    "api =os.getenv(\"GOOGLE_API_KEY\") #uncomment this during locally\n",
    "genai.configure(api_key=api) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api= os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(url:str):\n",
    "     \"\"\"\n",
    "     Try to get the transcription of the yt video\n",
    "     input: url\n",
    "     output: yt_transcription\n",
    "     \"\"\"\n",
    "     try:\n",
    "        loader = YoutubeLoader.from_youtube_url(\n",
    "            url, add_video_info=False,\n",
    "        language=[\"en\", \"hi\"],\n",
    "        translation=\"en\",\n",
    "        )\n",
    "        doc = loader.load()\n",
    "        return doc\n",
    "     except:\n",
    "        return \"not able to get transcript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_transcript(video_id):\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "        for transcript in transcript_list:\n",
    "            transcript_text =transcript.translate('en').fetch()\n",
    "        return transcript_text\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"hello everyone welcome to AI anytime  channel in this video we are going to  explore a new large language model  called Jer 7B beta so Jer 7B beta it's a  new release uh by hugging face  H4 uh they also created uh a model  earlier that's that was the Alpha  version of this particular model that  that was named Jer 7B Alpha I do have a  video on that as well please check that  out uh the video title is Jer 7B with  with chain lit okay so in that time we  we just uh inference the model uh in a  chain lit application just to evaluate  on some queries that have it's  performing now in this video we will use  j4 7B beta but not for the uh the stand  alone inference but we going to build a  rag uh system so we going to implement  retrieval augmented gen generation using  jeer 7B  beta uh let's talk about jeer 7B beta  you can see here on I am currently on  their uh hugging F repository and I like  hugging fish H4 who are the creators of  this particular model because they have  given some good open- source large  language models or the language models  to the open source community so one of  their Flagship is Star Chat vaa so they  finetuned star coder and they created a  new finetune modelar chat beta so you  know and they also earlier have created  uh Google flan double XL if I'm not  wrong which was a language model so you  they are they are an expert in fine  tuning llms there is no doubt about it  and now they have fine tuned Mistral and  first they created Alpha version of jeer  7 billion and now they have jeer 7  billion beta there are a couple of  things that they have you know I I talk  because I don't want you to give you a  lot of talks and you know we in Hindi we  call it Gan okay which is available on  their research paper you can find it out  on their repository here we will build a  rag Implement we'll do a rag  implementation I'll try to test it out  if it's working fine uh you know on the  retrieval augmented task so if you look  at here they have used DPO which is an  acronym for direct performance  optimization on the earlier model and  they have fixed like case sens uh type  type sensitiv sensitive or those kind of  things where earlier the responses were  not that uh I say  not that fancy when it comes to the  writing English okay when the model  generates some keywords or the text in  the words in the English language so  they have fixed all those things and  they have used DPO which is which I  assume is is better you know uh for  fine-tuning task in this particular  model if you come down it has performed  really well and surpassed all the other  7B model in the in the same category uh  that you can find it out over here they  have a radar chart for empty bench  evaluation here you can have a look uh  on different uh categories like  reasoning how it performed in writing  Humanities and this green one is j47 you  can look at the uh radius over here and  how it has performed along across all  these categories you know for uh  performed really well you know for  humanities and isem uh task uh writing  uh again did not perform that well for  reasoning which makes sense you know  llmc struggles with uh reasoning  mathematics because most of them are not  uh a knowledge model okay that can do a  reasoning okay uh anyway uh so you can  look at over here how to inference the  risk because it's not rlf or uh like  human in the Lop filtering or something  like that they haven't used it so it can  sometimes generate problematic responses  if you do some prompt uh manipulation or  something like that so you have you can  read read it over here as well now they  have the DPO uh config over here you can  have a look they have the hyper  parameters and all of those things that  that that is available here now we are  not going to use this model of course  because we I'm just the model that I'm  going to use can run on a CPU machine I  make sure that so we're going to again  rely on the amazing Tom jobbins the  block on the hugging face and we're  going to use one of his model that is  called ggf you know which is Unified  format uh for  uh inferencing an AI model on a  community Hardware earlier it was gml  you know but now it's ggf so they have  migrated from gml to ggf now it's an  unified format for all these inferences  on a community Hardware like CPU machine  or a single GPU we're going to use this  model through C Transformers C  Transformers is a python binding for  Transformers in a CN C++ okay so it has  some capabilities that helps you  inference llms uh faster on a CPU  machine or a single GPU so you're going  to utilize this model through CC  Transformers you can again use this  model through Cobalt and AMA and etc etc  you can again use Zama CPP and all to  inflence it as well so I have downloaded  a 5 bit quantize model this time just to  you know be on the better side because  there is a myth I don't know I think  that's a myth when you compress these  models you might uh see a performance  degradation I  do I I've have seen those but I I I  still think that's a myth by the way so  anyway now let's jump  into  our let me just do a clear H okay clear  and you can look at I have a folder here  that's called J for beta okay let me  open a vs code thing and if I look at  the VSS code these are the requirements  that you need you know if you want to  implement a rag on top of uh on top of J  for 7 billion llm we're going to use  chroma DV which is an inmemory Vector  store you can use some uh uh Vector  store Vector store Vector store or  vector database which are selfhosted or  it has a cloud version like we8 Pine con  cand etc etc now here we're going to use  chroma DB you can also use fast if you  want to do it do an inmemory Vector  store implementation sentence  transformer for an embedding model we're  going to use torch as a back end C  Transformers as I explained earlier Lang  chain as an orchestration framework P  PDF as a PDF processing library and  gradio probably you know just to have a  simple interface if some of you wants to  play because the code will be available  on GitHub you can just download and run  it as it is now here you can see I have  downloaded a 5 bit uh small quantized  model OKAY KS of it's a  5 bit quantized you can also use 4 bit  quantized but 5 bit will give you little  better performances okay and and the  configuration is I have an i7 with 16 GB  RAM and 1 TB SSD the current uh machine  that I'm currently going to use now this  is the document that we have if you come  here it says pit care of a dog okay  where we have you know 20 pages in a  document you can have n number of  documents depends if you have a what  kind of vector database you use that  depends on that okay if your  infrastructure supports that please go  ahead and use n number of documents you  can use different Vector stores I have  shown that in my previous videos how to  do a better rag you can watch how to  have multiple Vector stores combine it  together and then uh do a rag  implementation so this is what we're  going to have here right this is a  document 20 Pages looks good you know  that is why I choose this document this  is about the dogs pit care dogs care  okay so let's use that as a Source  document and let's start writing some  code now the first file that I'm going  to create is in. Pi so let me just  create this file where we will you know  download an embedding model we will  write some code to create vectors okay  out of the document and then we'll store  that in an inmemory Vector store that's  that we're using chroma DB there that's  what we're going to do here in this case  let's import OS first and after that I'm  going to use from Lang chain. text  spitter  import recursive character text splitter  this is the text splitter that we're  going to eat that can excuse me that can  split on characters level okay it also  has character text splitter but it's  recursive character text splitter it  also has token text splitter etc etc  that you can also uh uh this is mainly  for the chunking strategy you know how  do you chunk your data now after that  you're going to do from Lang  chain from Lang chain do uh m  embeddings and I'm going to  use BGE embeddings uh I'm not going to  use mpet or mini LM by sentence  Transformers in this video in in most of  my recent videos I have used BG  embeddings which is by Beijing AI  Academy so b a i/b g large in that's the  large embedding model on hugging fish  see I've been using so far that I now  remember it completely uh it's one of  the best open source embedding model  right now and I'm going to use the large  version of that and it is available in h  hugging face BG embedding it's not  available in hugging face embeddings  it's available in this hugging face BGE  embeddings class  now uh the mod by the way uh now what  else we need so we need uh from length  chain dot Vector stores and I'm going to  use here let's use chroma here in this  case I'm just going to write chroma H  it's not small  C H I don't know why  it's chroma okay now we also need a PDF  loader so let's do from Lang chain.  document  store uh document loader by the way so  not document store document store is  available in h tag if I'm not wrong okay  uh import uh uh let's do a p PDF thingy  here so Pi PDF  loader uh okay this looks nice I think  this this is what we need you know in  this uh embedding thingy guys okay so  now let's define the model name first so  I'm going to call it model name let me  just also Define the other variables  model  quags and it also has something called  incode qu quars as well so incode  quars okay now let's look for the model  here so let me show you this is the  model I'm going to use by Bing AI  Academy you can see it over here one of  the best open source embedding models  right now okay have around 70,000  downloads now let's come over here just  use a string to save this like I going  to store this and then the model qua now  the model qu I'm going to use CPU if you  have Cuda you can also Define that for a  faster embeddings creation and storage  now what I'm going to do here is device  CPU okay and then here in the quas it  has a parameter that's called normalize  embeddings if you want to normalize I'll  keep it false for this now okay so let's  uh use  normalize  embeddings and normalize embeddings and  this embeddings is false so let's do  false and then just let's create the  initiate the embeddings over here so in  embeddings and use the hugging face BGE  class from that embeddings module from  Lang chain and here here I'm going to  just do uh model name equals model  name that's what python is guys pretty  much readable programming language now  uh you have model qua and then it goes  model qua and then we have incode qua  which is incode qux excuse  me okay now this is this loads our  embeddings model okay now let me just do  a print here okay and say uh  embeddings model loaded okay  now uh let's see if you know if are we  able to uh just just do  python inest dop so far and just see if  we get that particular print that we  have printed embeddings model loaded  okay uh it so it will take a bit of time  if you're running it for the first time  it will take more time because it has to  download the model W and keep it in cash  okay  but now I already have done it earlier  because I I just use the same V andb you  know most of the time so it's already in  there okay now this is done so now let's  have a loader so loader equals Pi PDF  loader and this is where I'm going to  give the path name you can also use  directory loader if you have n number of  files I will recommend you using  unstructured Library works really good  when you have un structured data you  know that's that performs well these are  all trial and experimentation guys you  have to try there's no uh rule of thumb  that you know uh uh that you can use  okay now in P PDF loader let's do pit.  PDF and what I'm going to do next is  we're going to load this so  documents documents equals  loader do load and now here we're going  to Define our text splitter thing so  text splitter equals recursive character  text spitter I use tab 9 by the way a  coding intellig intelligence assistant  whatever you call it now here I'm going  to do chunk size I'll keep it a little  bigger 1,000 for now and chunk overlap  as 100 again it's really intuitive no  thumb rule that you have to keep  thousand okay it's always trial and  experiment it depends how have you uh  basically ex how you extract the data  from your Source document so it depends  on those things now text splitter is  done now let's have a text and in text  I'm going to write text splitter. split  I'm going to split that I'm just going  to pass the  documents let split split  documents yeah and I'm going to pass the  documents okay now  this this text right now let's just do a  text I just do print this okay let me  just show you so print here and text and  zero okay now what I'm going to do here  is let me just show you python in. Pi  okay  python. and we will get the first of the  list okay on you can see it over here we  got a page content guys it says dogs are  hug hugely popular PS and they are by  the way you know if you are the dog  lovers like me you know in fact there  are eight and half million dogs being  kept as pits in the UK in the United  Kingdom alone that's a huge number by  the way okay now this text has been done  let's just remove this and now what I'm  going to do is I'm going to have a  vector store initiation here so uh  excuse me Vector store variable I'm  going to call it chroma do from  documents if you have to create Vector  stores if you want to persist it or save  it then you have to first do from  documents and you can save it and then  once you load you just have to do chroma  so let's see that so from  documents and here I'm just going to  pass  uh text that's the first thing we have  then the embeddings model so which  variable is embeddings and then now I  also want to define a collection  metadata I will explain I have explained  that previously in many of my uh videos  that how you can Define the config how  you can create a configuration for your  collection met metad that you going to  collect okay now it's a key value pair  it's a dictionary so here now what we're  going to do is  hnsw and is space and this here now you  can  Define which what kind of algorithms you  want to use so it supports dot it  supports cosine it supports L2 different  other algorithms that you can Define you  know so here I'm just going to call it  cosine for now okay now this is  done now here you can also do persist  directory if you want to save that you  know  inside this will this will be a volume  you know that will just store it  somewhere and I'm going to call it  stores and here stores and let's call it  PID  cosine that's it guys you know that we  are done here okay so what we are doing  here if you look at this Vector store we  are passing the text that holds all the  extracted text it has the embeddings  model that we're going to use this is  the embedding model and here we have a  collection metadata where we're going to  use cosine you know when this uh uh this  entire embedding will be stored in a  nend dimensional space or within chroma  DB okay and  then we have persist directory where we  have we are persisting on the local disk  okay uh stores P cosign something like  that okay now let's just do print vector  or let's probably I will not do this  because I'm not going to create this  Vector store because I already have  stores here if you look at over here I  already have created stores for paid  cosign the same name and this is how the  structure looks like so if you are  facing any trouble you know please  comment and I will I'll be happy to help  you there now these are the file this is  how the file looks like after a  successful embedding creation Now what  you have to do in that case you have to  come here and just have to do python you  know in. pi and it will create the  embedding for you it will take probably  a minute depending on how big the  document or how many documents you have  this is how you create an embedding guys  using BGE you know and Lang chain so now  let's save this minimize this create a  file here called  app.py now in this app Pi guys we have  to write all of our code logic so the  first thing that I'm going to do is from  L  chain import prom template or either let  me just uh or let me just do it okay so  from L  chain import  prompt  uh prompt template and I'm also going to  need I don't need LM chain okay so from  Lang chain import prom templates  template okay now from Lang chain. l  lenss and here I'm going to use C  Transformers so you can also use C  Transformers through Lang chain you can  also use C Transformers you know stand  alone you know you can just use from uh  Auto LM do from pre-train you can also  do that that way as well I prefer using  if I'm using Lang chain I will use Lang  chain okay otherwise I will not use Lang  chain so from Lang chain. llms import C  Transformers I like C Transformer thanks  to Mela who the creator of this  wonderful Library now import  OS  and okay uh we need uh again the  vector we need the vector stores guys so  import  choma Ah that's a problem with chroma I  don't know why it's okay now we need a  chain a retrieval chain so let's do that  from length chain do  R from L chain do chain sorry I don't  know what I was writing import retrieval  QA this is fine let's get the B B  embeddings again embeddings import  hugging face BGE embeddings let's import  gradio here as gr and let's import what  else do we need uh for now I think we  don't need anything now here we're going  to Define our local LM llm  path make sure that you have downloaded  this kind of model a ggf model okay so  now J for and you can see I just pasted  over there let's define a  config  uh okay I'll copy it quickly from my  GitHub guys so let me just go on my  GitHub  and get it it I don't know why it's so  slow but anyway uh I'll take it  from this this repository why to write  the same code again and again maybe I  can explain that I just need this part  of the code here okay now let's do that  now what I'm doing here let me just show  you I have a config configuration config  variable that is stores this maximum new  tokens you know repetition penalty all  these inference parameter they are not  hyper parameters by the way they are  inference parameters because you're  going to use this in inference  inferencing the llms now here we have a  temperature let's minimize the  temperature  because for rag I always recommend  minimizing the temperature if you are  are doing an information retrieval on  your knowledge base to keep a check on a  bit of on the hallucination it directly  does not like impact the hallucination  directly but it has some roles to play  when it tries to become creative it  generate a lot of Randomness or random  responses which might be hallucination  now you have top key top Kate excuse me  topy extrem and yeah threads and  something like that okay uh this is  forjust for your CPU you know how many  threads you you have and something like  that okay now here this is I'm using  avx2 because you know I want to just  make it faster in inference and you will  see that how fast it is probably takes  around 40 seconds you know on uh on CPU  and I'm not not also going to stream the  responses if you also use stream the  responses like through grad Al together  because C Transformers provides that by  the way you can also do that it will be  really FAS in 20 seconds we will start  seeing some results  now we are here we are okay with this  guys let me just uh close the charge  okay uh yeah llm is done now let's have  a prom template okay so here I'm going  to define a prom thingy okay so let's  just again go back to my GitHub why to  write the code I'll go to my llama to  Medical chatboard one of the most viewed  video of mine okay on my channel thank  you so much for all of you or to all of  you uh now here I'm going to use  m.p and I'm just going to copy this  custom prom template let's copy that  come back here a very simple prompt  let's call this this let's call it  prompt  template it says use the following  pieces of information and you can do a  lot of creativity here on these  prompting techniques you can do a role  based prompting an instruction based  prompting a chain of thoughts etc etc  I'll keep it simple here now I have  context I have question this looks nice  this looks nice now let's come down  let's bring up the models again we still  need this to pass as an embedding  function because once you ask a query  you need the respective vectors for that  as well right that's why we're going to  use that here now this is also done now  what I'm going to do here I'm going to  call it okay prompt prompt template and  inside this prom template I'm going to  pass  template and template  equals prom  template the variable and then I'm going  to pass my input variables so input  vares and input variables text list so  in this I'm going to pass context so  let's do context and then after that I'm  going to pass question that is  question we are done with this also let  me make it uh like this now prompt is  done after prompt let do let's do a  loading the vector store now we're going  to load the vector store we already have  persist it on in memory right now load  Vector store and here I'm going to write  chroma and then persist  directory so just do a persist directory  from which directory you want to load  that okay so I'm just going to call  Itor and here I have pit  cosign ah I don't know why this is Cap  okay put cosine and then we're going to  have embedding  function and this embedding function is  going to be my embeddings okay this is  how you now load so now you don't have  to basically do it in every run time you  have saved your vectors on in memory on  disk and now just load it that's it now  here let's have a retrieve  rever  oh Retriever and your retriever is load  Vector store dot as retriever so as  retriever so this is amazing I don't  know why I'm not getting that dot as  retriever okay as retriever it becomes a  function and here I'm going to write  search qua you can pass how many  document you want to you know search  within okay like you want to retrieve  not the search within how many you want  to retrieve you know as a retriev Chun  so you can pass it to a large language  model for Generation part now I'm going  do k equal to one I would like to keep  it short okay uh search Casal now let's  do a query here first and in this query  I'm going to say uh what is the  fastest let's ask that question okay  uh I had a question that I really liked  it I wanted to test on some complex  questions uh where did it go  man H yeah this one the fasted recorded  speed of a greyhound dog was 42 mph now  I'm going to ask this question because  it also has numbers and let's see if in  J if first the Retriever and then the  Jer if it's able to retrieve or not the  BG embedding through  uh okay this is going to be this okay  now and not equal I don't know why I did  equal it's colon It's a key value pair  okay search qus now what the fastest  what are the question what is the  fastest gray hound dog okay what is the  fastest  speed for a  Grayhound  dog now here let's do a semantic search  first on the retriever let's first  retrieve and see if it's able to  retrieve the right set of chunks and I'm  going to say retrieve I don't know why  I'm feeling that retriever spelling is  wrong here that I have  typed okay retriever okay now  retriever uh dot it has a method called  get relevant okay get  relevant  documents so it has this method that we  can use and we just pass our query so  let's pass the query here and then just  do  print print semantic search let's see if  it's able to retrieve this okay now  let's run this guys here python app.  okay uh let's see that if it's able to  you know  uh able to retrieve the right set of  chunks and all okay if your retriever is  working fine right that's the that's the  first important of critical thing and  you can see I I got my response okay I  have k equals to one so I got one page  content one document it says docs are  usely popular something like that and it  it is not able to uh  retrieve I think the answer there which  is really surprising what I what I will  do now  is what if I make this k equals to 2 at  least and see if it's able to now  retrieve let's see  that three hound  dog uh and you can also set up some  thresold values now you saw that I  didn't get that 42s anywhere okay in  this  thing  [Music]  [Music]  now yeah you can see it over  here yeah this is the right okay fine  fine fine fine fine and I think I did a  mistake I think it was there also right  let me see that yet why I can't see you  know my eyes are like gone okay because  it's the same  response yeah you can see it over here  42 m per hour okay so let's keep it as  one as well I was not attentive excuse  me for that now we are okay to do the  print uh semantic search and all guys  now now what we're going to do next is  let's he divide a  print uh print  thingy and and here we're going to write  our logic so let's create a let's first  create a for prompt let's call it chain  type  quar and here chain type quar and H this  is again a key value pairer so our  prompt going to be  prompt prompt and now let's create a QA  chain so let's call it QA and I'm going  to use trial QA do from chain type okay  from chain type and  let's let's use here llm equals  llm and let me just go up so you can see  it chain type equals stuff you can also  use rerank refine map rerank Etc depends  what you what kind of ug cases you are  working on if it's a summarization use  the refine you know depends what kind of  use case you are dealing with now  retriever equals retriever  and return Source document so return  underscore Source  documents documents equals  true and then we have chain type qus  equals chain type qu and you can also do  veros equals to  True veros equals to this is now we have  a qn this is our rag chain now okay  which which has just retrieval keyway  because we not looking at the  conversational part of it so it has a  retrieval qf from chain type it has an  llm it has a chain type stuff it has a  retriever thingy it has a return Source  documents blah blah blah right now what  I'm going to do here is let's have a  response variable and I'm going to call  my query on top of it so  QA query and I'm going to just uh  uh run it now let's just do a print  response and see if it's working if we  getting the right response the question  that we ask using jaer the whole agenda  of this video guys is that when I use  jaer Alpha model it was not that good  for rag I will be honest and I love  hugging face H4 by the way I'm a huge  fan of the creators of this particular  llm the fine tune when they fine tune  this on  mral Alpha model was not that good for  rag implementation but when I tried beta  one it was really working fine and  that's why I wanted to create a video  for all of you so if you are looking at  some replacement of llama 2 or Mistral  you can look at J for the beta version  of it and that's why it's a beta version  right now let's do a print response and  see now here what I'm going to do is  let's just run this again Python app.py  and it will take a bit of time now  because it has first it will retrieve  the Retriever and then llm is going to  read those and then form a human like  response for you it says LM is not  defined oh  okay ah okay we have not llm we have uh  let's call it  llm okay  fine oh these are the mistakes if I keep  on if I keep on making this kind of  mistake how will I become a YouTuber  guys then probably I have to do a lot of  edits you know stop the video edit again  merge it  create no but coding is fun you know if  you do coding live you know if you then  it's much better to be honest in in My  Views and that's why a few of my videos  are a little longer but I know I want to  create those videos where I want you to  write codes alongside me you know with  me you know in when I'm writing code in  video you also write in your machine  okay okay  now uh of course I do have my notes and  all you know for some kind of  explanation where required the pointers  and  all okay uh if you can see it entered  into new retrieval QA chain and probably  it takes around 30 to 40 seconds uh for  uh generating that response based on the  retreat R chunks based on the retrieved  chunks that you see the chunks had been  retrieved over here you can see the 42 M  hour and meanwhile it's doing let's uh  let's write the gradio code to save some  time okay so what I'm going to do is  first I'm going to do a sample prompt  here so let's define a sample prompts  and it's going to be a list the first  one I will just keep it again  this so let's keep this here the first  one and I'll just do an ALT G here so  now the next one is why  should why should should we not  fed  chocolates uh now suppose this is the  question that I'm asking to the  dogs to the dogs okay now who the third  question that I can ask is  name two  factors which are  important for the dog scare okay suppose  this is the third question now this is  the sample prompts I'm going to build a  very fast grad application now I'm going  to write a function here Define get  response uh function that we can use it  so we don't have to generate here pass  an input input is going to be prompt so  let's oh no query is going to be  input excuse me sorry query is going to  be input let's have a chain type qua  here again chain type qu prompt  prompt this this is going to be inside a  function and maybe I can just use this  in entire thing here QA thingy so let's  just bring it  here  oh veros and all we can keep it uh true  and now let's have uh let's see that you  know if we got our response  guys I don't know what I  did we got our response fantastic you  know you can see the result by G4 7B  beta it says the fastest record recorded  speed for a gry hound dog is 42 mph  which is the right answer which is  fantastic you know and this is a complex  question trust me when I say it because  it has some numbers dates involved or  whatever it has some numbers and numeric  thing where struggles and it is a part  of that uh PDF as well as you saw right  now you can improve the prom to keep the  model stick with the embeddings the  knowledge base not to its base knowledge  you can do that now here what I'm going  to do next is let's do a response thingy  again here so response equals QA query  and then just return the response so  let's just do return response I'm not  going to do any output passing maybe you  can do that as an exercise if you want  to do now this is done let's create a  variable called iase for the interface  the gradio interface and what I'm going  to do is  gr dot why it's not showing  interface yeah and here I'm going to  define a lot of things the first is  inputs so input is going to be my  input so let's do that because that's  what the function is  and or then we have to define the input  because we haven't defined the input yet  so let's define an input first so what  I'm going to do  here input equals we're going to have a  text box so let's call it text and  inside this text I'm going to write my  label so label is going to be  prompt  oh prompt this is going to be your  prompt and then so  label let's keep it it so level let's  keep it false uh and then we going to  have maximum lines you can let's keep  it probably let's keep it one for now  okay maximum lines then we going to have  placeholder and placeholder is going to  be like enter  your enter your query or something like  that okay and then we have a container  equals false I don't want it to the  middle of it so container equals false  now this is I I going to be my input  with single a text where we're going to  put our questions okay uh I put it  outside  sorry now here let's have an output then  so we're going to have an out uh we  forgot to do one thing guys where the  function this is  really this is really strange because I  need a function the function is going to  be the get response here so Gore  response  uh let's make it structure okay okay now  the function is done which is a callable  function that's what gradio application  will call right now after that I'm going  to have an outputs so outputs equals  here I'm going to have text and after  that output text is done title and title  let's call it my dog pit care uh  rag  implementation using J  for 7B beta let remove my thingy or  let's let's remove my here okay okay I  remove the my thing and now after title  let's have a description if you want  I'll say rag  demo  for j47 B beta llm okay now this is  description after description what we're  going to do is we're going to have  examples and here you're going to pass  our sample prompts guys okay so we have  sample prompts see the examples which  will be in the below I will show that  and now let's have couple of let's say  flagging equals false you know if you  want to do flagging and all you can keep  some logs also a screenshot let's  disable screenshot if  possible uh screenshot equals false true  okay now this is done so just let's do  ifas do  launch okay so we are done with our code  let me just explain what we are doing  here quickly so you would have seen this  earlier like I when I ran it before  writing this gradual let's just uh we  don't need these now okay at least let's  just ah okay let's just do a  control ah excuse me control hash I just  hash those because I don't need  it we also don't need these two  lines oh this three lines by the way  so let's just do a control hash I don't  know why I'm again typing the  wrong but okay oh we also don't need  maybe let's let's also hide this okay we  don't need those things and then we have  sample prompts we have our keyw thing  going  on I don't know why this is giving  a excuse me it's giving a bad feeling  yeah the indentation was was  wrong see the  problem  okay then you have a response and then  you have the response I hope this is  okay okay uh this looks nice now okay so  now we have this this this this this all  okay let's run this now guys okay so  python app  doy  H and let's copy this probably I think I  would been running it on 7860 right  let's see that  7860 I'm not running it so I haven't run  it grad  yet and now once I  you know hit enter here you can see a  simple you know very bad looking  interface where you can type some  question let's ask the same question  first and you can see this is the  question there what is the fastest speed  for a greyhound dog okay and these are  example prompts you can put multiple  examples you can do redo undo lot of  other customization with gradio you can  do you know probably and here we are  using a Jeffer model for for rag  implementation guys I'm focusing on that  because uh it performs good so here we  are not passing any output we only have  the complete output the raw output that  we're going to present okay it's as I  said right it takes around 40 seconds uh  at average probably you know I don't  know why it's taking let's see how much  time it takes for this one okay and it's  running on a CPU machine we are not  utilizing any CA  here you can see it has entered into the  QA chain and it's working it's  generating the response for you okay and  just to tell you that this entire code  will be available on the gith projectory  guys so you can just go and Fork it or  just download it and try it out yourself  that if and you can tell us the entire  uh subscribers of people are going to  watch this video you finding can comment  in the comment box as well oh it took  more than a minute this is surprising  because it was really fast when we were  doing it in  terminal hm  let me just go back to  code I launch  I we have this get response function  defined which takes an input which is  query QA query this is  fine uh okay call LEL function inputs  equals  input  and okay prompt equals prompt let's see  this this is so surprising I don't know  why it's taking that much of time in the  grad  application but anyway we'll wait for it  or are  we it took 2  minutes input  value new this looks good function get  response inputs input text title sample  prompt i. launch anyway uh let's let's  see how much time it  takes oh it took I don't know it took  what is the it took around almost 2 and  a half minutes for this response but in  terminal it was really fast probably  have to check that out that why it's  taking at least it will definitely not  take 2 minutes you know in terminal if  you if you use time to calculate it so  you can see the question here what is  the fastest speed for a greyh house dog  and we got our result the fastest  recorded speed for a greyhound dog was  42 m per hour it gives you the source  documents the page content and the PDF  as well that from which PDF you have you  can pass the output in a beautified  format now this is what I wanted to know  show you guys in this video now you have  a skeleton you can just take my code and  work it make it better run it on a Cuda  machine how do you run this on Cuda  machine it's very easy you know when you  are using C Transformer you have to pass  the number of GPU layers and make sure  that you have cublas and metal or  whatever that you are working with you  have those supports depending on which  operating system you are on guys most of  the time you'll say that GPU layers n  number of GPU layers are not working for  C Transformer there are reasons for it  you might not have the support uh uh for  that operating system probably maybe you  should check it out know if you have the  Cub support if you're using it if you  have a blast because these are softwares  that you need the support uh to  influence it  so that all you know I'll give the code  gith projectory in the description if  you like the video please uh hit a like  icon if you have any thoughts feedbacks  for me please let me know in the comment  box you can also reach out to me through  my social media channels and if you  haven't subscribed the channel yet  please do subscribe the channel and  support the channel guys that's what  motivates me you know uh that's all for  uh today's video guys thank you so much  for watching see you in the next  one\", metadata={'source': 'btuN-rrPhsM'})]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_from_get = get_transcript(\"https://www.youtube.com/watch?v=btuN-rrPhsM&t=2465s\")\n",
    "trans_from_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents=trans_from_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"hello everyone welcome to AI anytime  channel in this video we are going to  explore a new large language model  called Jer 7B beta so Jer 7B beta it's a  new release uh by hugging face  H4 uh they also created uh a model  earlier that's that was the Alpha  version of this particular model that  that was named Jer 7B Alpha I do have a  video on that as well please check that  out uh the video title is Jer 7B with  with chain lit okay so in that time we  we just uh inference the model uh in a\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"time we  we just uh inference the model uh in a  chain lit application just to evaluate  on some queries that have it's  performing now in this video we will use  j4 7B beta but not for the uh the stand  alone inference but we going to build a  rag uh system so we going to implement  retrieval augmented gen generation using  jeer 7B  beta uh let's talk about jeer 7B beta  you can see here on I am currently on  their uh hugging F repository and I like  hugging fish H4 who are the creators of\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"I like  hugging fish H4 who are the creators of  this particular model because they have  given some good open- source large  language models or the language models  to the open source community so one of  their Flagship is Star Chat vaa so they  finetuned star coder and they created a  new finetune modelar chat beta so you  know and they also earlier have created  uh Google flan double XL if I'm not  wrong which was a language model so you  they are they are an expert in fine  tuning llms\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"they are they are an expert in fine  tuning llms there is no doubt about it  and now they have fine tuned Mistral and  first they created Alpha version of jeer  7 billion and now they have jeer 7  billion beta there are a couple of  things that they have you know I I talk  because I don't want you to give you a  lot of talks and you know we in Hindi we  call it Gan okay which is available on  their research paper you can find it out  on their repository here we will build a  rag Implement\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"repository here we will build a  rag Implement we'll do a rag  implementation I'll try to test it out  if it's working fine uh you know on the  retrieval augmented task so if you look  at here they have used DPO which is an  acronym for direct performance  optimization on the earlier model and  they have fixed like case sens uh type  type sensitiv sensitive or those kind of  things where earlier the responses were  not that uh I say  not that fancy when it comes to the  writing English okay\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content='fancy when it comes to the  writing English okay when the model  generates some keywords or the text in  the words in the English language so  they have fixed all those things and  they have used DPO which is which I  assume is is better you know uh for  fine-tuning task in this particular  model if you come down it has performed  really well and surpassed all the other  7B model in the in the same category uh  that you can find it out over here they  have a radar chart for empty bench', metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content='here they  have a radar chart for empty bench  evaluation here you can have a look uh  on different uh categories like  reasoning how it performed in writing  Humanities and this green one is j47 you  can look at the uh radius over here and  how it has performed along across all  these categories you know for uh  performed really well you know for  humanities and isem uh task uh writing  uh again did not perform that well for  reasoning which makes sense you know  llmc struggles with uh', metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"makes sense you know  llmc struggles with uh reasoning  mathematics because most of them are not  uh a knowledge model okay that can do a  reasoning okay uh anyway uh so you can  look at over here how to inference the  risk because it's not rlf or uh like  human in the Lop filtering or something  like that they haven't used it so it can  sometimes generate problematic responses  if you do some prompt uh manipulation or  something like that so you have you can  read read it over here as well now\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"have you can  read read it over here as well now they  have the DPO uh config over here you can  have a look they have the hyper  parameters and all of those things that  that that is available here now we are  not going to use this model of course  because we I'm just the model that I'm  going to use can run on a CPU machine I  make sure that so we're going to again  rely on the amazing Tom jobbins the  block on the hugging face and we're  going to use one of his model that is  called ggf you\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"to use one of his model that is  called ggf you know which is Unified  format uh for  uh inferencing an AI model on a  community Hardware earlier it was gml  you know but now it's ggf so they have  migrated from gml to ggf now it's an  unified format for all these inferences  on a community Hardware like CPU machine  or a single GPU we're going to use this  model through C Transformers C  Transformers is a python binding for  Transformers in a CN C++ okay so it has  some capabilities that helps\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"C++ okay so it has  some capabilities that helps you  inference llms uh faster on a CPU  machine or a single GPU so you're going  to utilize this model through CC  Transformers you can again use this  model through Cobalt and AMA and etc etc  you can again use Zama CPP and all to  inflence it as well so I have downloaded  a 5 bit quantize model this time just to  you know be on the better side because  there is a myth I don't know I think  that's a myth when you compress these  models you might\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"a myth when you compress these  models you might uh see a performance  degradation I  do I I've have seen those but I I I  still think that's a myth by the way so  anyway now let's jump  into  our let me just do a clear H okay clear  and you can look at I have a folder here  that's called J for beta okay let me  open a vs code thing and if I look at  the VSS code these are the requirements  that you need you know if you want to  implement a rag on top of uh on top of J  for 7 billion llm we're\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"on top of uh on top of J  for 7 billion llm we're going to use  chroma DV which is an inmemory Vector  store you can use some uh uh Vector  store Vector store Vector store or  vector database which are selfhosted or  it has a cloud version like we8 Pine con  cand etc etc now here we're going to use  chroma DB you can also use fast if you  want to do it do an inmemory Vector  store implementation sentence  transformer for an embedding model we're  going to use torch as a back end C  Transformers\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"going to use torch as a back end C  Transformers as I explained earlier Lang  chain as an orchestration framework P  PDF as a PDF processing library and  gradio probably you know just to have a  simple interface if some of you wants to  play because the code will be available  on GitHub you can just download and run  it as it is now here you can see I have  downloaded a 5 bit uh small quantized  model OKAY KS of it's a  5 bit quantized you can also use 4 bit  quantized but 5 bit will give you\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"also use 4 bit  quantized but 5 bit will give you little  better performances okay and and the  configuration is I have an i7 with 16 GB  RAM and 1 TB SSD the current uh machine  that I'm currently going to use now this  is the document that we have if you come  here it says pit care of a dog okay  where we have you know 20 pages in a  document you can have n number of  documents depends if you have a what  kind of vector database you use that  depends on that okay if your  infrastructure\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"depends on that okay if your  infrastructure supports that please go  ahead and use n number of documents you  can use different Vector stores I have  shown that in my previous videos how to  do a better rag you can watch how to  have multiple Vector stores combine it  together and then uh do a rag  implementation so this is what we're  going to have here right this is a  document 20 Pages looks good you know  that is why I choose this document this  is about the dogs pit care dogs care  okay\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"this  is about the dogs pit care dogs care  okay so let's use that as a Source  document and let's start writing some  code now the first file that I'm going  to create is in. Pi so let me just  create this file where we will you know  download an embedding model we will  write some code to create vectors okay  out of the document and then we'll store  that in an inmemory Vector store that's  that we're using chroma DB there that's  what we're going to do here in this case  let's import OS\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"going to do here in this case  let's import OS first and after that I'm  going to use from Lang chain. text  spitter  import recursive character text splitter  this is the text splitter that we're  going to eat that can excuse me that can  split on characters level okay it also  has character text splitter but it's  recursive character text splitter it  also has token text splitter etc etc  that you can also uh uh this is mainly  for the chunking strategy you know how  do you chunk your data\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"strategy you know how  do you chunk your data now after that  you're going to do from Lang  chain from Lang chain do uh m  embeddings and I'm going to  use BGE embeddings uh I'm not going to  use mpet or mini LM by sentence  Transformers in this video in in most of  my recent videos I have used BG  embeddings which is by Beijing AI  Academy so b a i/b g large in that's the  large embedding model on hugging fish  see I've been using so far that I now  remember it completely uh it's one of  the\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"I now  remember it completely uh it's one of  the best open source embedding model  right now and I'm going to use the large  version of that and it is available in h  hugging face BG embedding it's not  available in hugging face embeddings  it's available in this hugging face BGE  embeddings class  now uh the mod by the way uh now what  else we need so we need uh from length  chain dot Vector stores and I'm going to  use here let's use chroma here in this  case I'm just going to write chroma H\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"in this  case I'm just going to write chroma H  it's not small  C H I don't know why  it's chroma okay now we also need a PDF  loader so let's do from Lang chain.  document  store uh document loader by the way so  not document store document store is  available in h tag if I'm not wrong okay  uh import uh uh let's do a p PDF thingy  here so Pi PDF  loader uh okay this looks nice I think  this this is what we need you know in  this uh embedding thingy guys okay so  now let's define the model\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"thingy guys okay so  now let's define the model name first so  I'm going to call it model name let me  just also Define the other variables  model  quags and it also has something called  incode qu quars as well so incode  quars okay now let's look for the model  here so let me show you this is the  model I'm going to use by Bing AI  Academy you can see it over here one of  the best open source embedding models  right now okay have around 70,000  downloads now let's come over here just  use a\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"downloads now let's come over here just  use a string to save this like I going  to store this and then the model qua now  the model qu I'm going to use CPU if you  have Cuda you can also Define that for a  faster embeddings creation and storage  now what I'm going to do here is device  CPU okay and then here in the quas it  has a parameter that's called normalize  embeddings if you want to normalize I'll  keep it false for this now okay so let's  uh use  normalize  embeddings and normalize\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"uh use  normalize  embeddings and normalize embeddings and  this embeddings is false so let's do  false and then just let's create the  initiate the embeddings over here so in  embeddings and use the hugging face BGE  class from that embeddings module from  Lang chain and here here I'm going to  just do uh model name equals model  name that's what python is guys pretty  much readable programming language now  uh you have model qua and then it goes  model qua and then we have incode qua  which\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"model qua and then we have incode qua  which is incode qux excuse  me okay now this is this loads our  embeddings model okay now let me just do  a print here okay and say uh  embeddings model loaded okay  now uh let's see if you know if are we  able to uh just just do  python inest dop so far and just see if  we get that particular print that we  have printed embeddings model loaded  okay uh it so it will take a bit of time  if you're running it for the first time  it will take more time\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"it for the first time  it will take more time because it has to  download the model W and keep it in cash  okay  but now I already have done it earlier  because I I just use the same V andb you  know most of the time so it's already in  there okay now this is done so now let's  have a loader so loader equals Pi PDF  loader and this is where I'm going to  give the path name you can also use  directory loader if you have n number of  files I will recommend you using  unstructured Library works\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"recommend you using  unstructured Library works really good  when you have un structured data you  know that's that performs well these are  all trial and experimentation guys you  have to try there's no uh rule of thumb  that you know uh uh that you can use  okay now in P PDF loader let's do pit.  PDF and what I'm going to do next is  we're going to load this so  documents documents equals  loader do load and now here we're going  to Define our text splitter thing so  text splitter equals\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"our text splitter thing so  text splitter equals recursive character  text spitter I use tab 9 by the way a  coding intellig intelligence assistant  whatever you call it now here I'm going  to do chunk size I'll keep it a little  bigger 1,000 for now and chunk overlap  as 100 again it's really intuitive no  thumb rule that you have to keep  thousand okay it's always trial and  experiment it depends how have you uh  basically ex how you extract the data  from your Source document so it depends\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"data  from your Source document so it depends  on those things now text splitter is  done now let's have a text and in text  I'm going to write text splitter. split  I'm going to split that I'm just going  to pass the  documents let split split  documents yeah and I'm going to pass the  documents okay now  this this text right now let's just do a  text I just do print this okay let me  just show you so print here and text and  zero okay now what I'm going to do here  is let me just show you\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"I'm going to do here  is let me just show you python in. Pi  okay  python. and we will get the first of the  list okay on you can see it over here we  got a page content guys it says dogs are  hug hugely popular PS and they are by  the way you know if you are the dog  lovers like me you know in fact there  are eight and half million dogs being  kept as pits in the UK in the United  Kingdom alone that's a huge number by  the way okay now this text has been done  let's just remove this and now\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"has been done  let's just remove this and now what I'm  going to do is I'm going to have a  vector store initiation here so uh  excuse me Vector store variable I'm  going to call it chroma do from  documents if you have to create Vector  stores if you want to persist it or save  it then you have to first do from  documents and you can save it and then  once you load you just have to do chroma  so let's see that so from  documents and here I'm just going to  pass  uh text that's the first thing\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"going to  pass  uh text that's the first thing we have  then the embeddings model so which  variable is embeddings and then now I  also want to define a collection  metadata I will explain I have explained  that previously in many of my uh videos  that how you can Define the config how  you can create a configuration for your  collection met metad that you going to  collect okay now it's a key value pair  it's a dictionary so here now what we're  going to do is  hnsw and is space and this here\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"going to do is  hnsw and is space and this here now you  can  Define which what kind of algorithms you  want to use so it supports dot it  supports cosine it supports L2 different  other algorithms that you can Define you  know so here I'm just going to call it  cosine for now okay now this is  done now here you can also do persist  directory if you want to save that you  know  inside this will this will be a volume  you know that will just store it  somewhere and I'm going to call it  stores\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"it  somewhere and I'm going to call it  stores and here stores and let's call it  PID  cosine that's it guys you know that we  are done here okay so what we are doing  here if you look at this Vector store we  are passing the text that holds all the  extracted text it has the embeddings  model that we're going to use this is  the embedding model and here we have a  collection metadata where we're going to  use cosine you know when this uh uh this  entire embedding will be stored in a  nend\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"this  entire embedding will be stored in a  nend dimensional space or within chroma  DB okay and  then we have persist directory where we  have we are persisting on the local disk  okay uh stores P cosign something like  that okay now let's just do print vector  or let's probably I will not do this  because I'm not going to create this  Vector store because I already have  stores here if you look at over here I  already have created stores for paid  cosign the same name and this is how the\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"paid  cosign the same name and this is how the  structure looks like so if you are  facing any trouble you know please  comment and I will I'll be happy to help  you there now these are the file this is  how the file looks like after a  successful embedding creation Now what  you have to do in that case you have to  come here and just have to do python you  know in. pi and it will create the  embedding for you it will take probably  a minute depending on how big the  document or how many\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"depending on how big the  document or how many documents you have  this is how you create an embedding guys  using BGE you know and Lang chain so now  let's save this minimize this create a  file here called  app.py now in this app Pi guys we have  to write all of our code logic so the  first thing that I'm going to do is from  L  chain import prom template or either let  me just uh or let me just do it okay so  from L  chain import  prompt  uh prompt template and I'm also going to  need I\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"uh prompt template and I'm also going to  need I don't need LM chain okay so from  Lang chain import prom templates  template okay now from Lang chain. l  lenss and here I'm going to use C  Transformers so you can also use C  Transformers through Lang chain you can  also use C Transformers you know stand  alone you know you can just use from uh  Auto LM do from pre-train you can also  do that that way as well I prefer using  if I'm using Lang chain I will use Lang  chain okay otherwise I will\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"I will use Lang  chain okay otherwise I will not use Lang  chain so from Lang chain. llms import C  Transformers I like C Transformer thanks  to Mela who the creator of this  wonderful Library now import  OS  and okay uh we need uh again the  vector we need the vector stores guys so  import  choma Ah that's a problem with chroma I  don't know why it's okay now we need a  chain a retrieval chain so let's do that  from length chain do  R from L chain do chain sorry I don't  know what I was\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"L chain do chain sorry I don't  know what I was writing import retrieval  QA this is fine let's get the B B  embeddings again embeddings import  hugging face BGE embeddings let's import  gradio here as gr and let's import what  else do we need uh for now I think we  don't need anything now here we're going  to Define our local LM llm  path make sure that you have downloaded  this kind of model a ggf model okay so  now J for and you can see I just pasted  over there let's define a  config  uh\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"pasted  over there let's define a  config  uh okay I'll copy it quickly from my  GitHub guys so let me just go on my  GitHub  and get it it I don't know why it's so  slow but anyway uh I'll take it  from this this repository why to write  the same code again and again maybe I  can explain that I just need this part  of the code here okay now let's do that  now what I'm doing here let me just show  you I have a config configuration config  variable that is stores this maximum new  tokens you\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"that is stores this maximum new  tokens you know repetition penalty all  these inference parameter they are not  hyper parameters by the way they are  inference parameters because you're  going to use this in inference  inferencing the llms now here we have a  temperature let's minimize the  temperature  because for rag I always recommend  minimizing the temperature if you are  are doing an information retrieval on  your knowledge base to keep a check on a  bit of on the hallucination it\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"keep a check on a  bit of on the hallucination it directly  does not like impact the hallucination  directly but it has some roles to play  when it tries to become creative it  generate a lot of Randomness or random  responses which might be hallucination  now you have top key top Kate excuse me  topy extrem and yeah threads and  something like that okay uh this is  forjust for your CPU you know how many  threads you you have and something like  that okay now here this is I'm using  avx2\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"like  that okay now here this is I'm using  avx2 because you know I want to just  make it faster in inference and you will  see that how fast it is probably takes  around 40 seconds you know on uh on CPU  and I'm not not also going to stream the  responses if you also use stream the  responses like through grad Al together  because C Transformers provides that by  the way you can also do that it will be  really FAS in 20 seconds we will start  seeing some results  now we are here we are okay\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"seeing some results  now we are here we are okay with this  guys let me just uh close the charge  okay uh yeah llm is done now let's have  a prom template okay so here I'm going  to define a prom thingy okay so let's  just again go back to my GitHub why to  write the code I'll go to my llama to  Medical chatboard one of the most viewed  video of mine okay on my channel thank  you so much for all of you or to all of  you uh now here I'm going to use  m.p and I'm just going to copy this  custom\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"use  m.p and I'm just going to copy this  custom prom template let's copy that  come back here a very simple prompt  let's call this this let's call it  prompt  template it says use the following  pieces of information and you can do a  lot of creativity here on these  prompting techniques you can do a role  based prompting an instruction based  prompting a chain of thoughts etc etc  I'll keep it simple here now I have  context I have question this looks nice  this looks nice now let's come\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"this looks nice  this looks nice now let's come down  let's bring up the models again we still  need this to pass as an embedding  function because once you ask a query  you need the respective vectors for that  as well right that's why we're going to  use that here now this is also done now  what I'm going to do here I'm going to  call it okay prompt prompt template and  inside this prom template I'm going to  pass  template and template  equals prom  template the variable and then I'm going\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"prom  template the variable and then I'm going  to pass my input variables so input  vares and input variables text list so  in this I'm going to pass context so  let's do context and then after that I'm  going to pass question that is  question we are done with this also let  me make it uh like this now prompt is  done after prompt let do let's do a  loading the vector store now we're going  to load the vector store we already have  persist it on in memory right now load  Vector store and here\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"in memory right now load  Vector store and here I'm going to write  chroma and then persist  directory so just do a persist directory  from which directory you want to load  that okay so I'm just going to call  Itor and here I have pit  cosign ah I don't know why this is Cap  okay put cosine and then we're going to  have embedding  function and this embedding function is  going to be my embeddings okay this is  how you now load so now you don't have  to basically do it in every run time you\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"have  to basically do it in every run time you  have saved your vectors on in memory on  disk and now just load it that's it now  here let's have a retrieve  rever  oh Retriever and your retriever is load  Vector store dot as retriever so as  retriever so this is amazing I don't  know why I'm not getting that dot as  retriever okay as retriever it becomes a  function and here I'm going to write  search qua you can pass how many  document you want to you know search  within okay like you want to\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"to you know search  within okay like you want to retrieve  not the search within how many you want  to retrieve you know as a retriev Chun  so you can pass it to a large language  model for Generation part now I'm going  do k equal to one I would like to keep  it short okay uh search Casal now let's  do a query here first and in this query  I'm going to say uh what is the  fastest let's ask that question okay  uh I had a question that I really liked  it I wanted to test on some complex\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"liked  it I wanted to test on some complex  questions uh where did it go  man H yeah this one the fasted recorded  speed of a greyhound dog was 42 mph now  I'm going to ask this question because  it also has numbers and let's see if in  J if first the Retriever and then the  Jer if it's able to retrieve or not the  BG embedding through  uh okay this is going to be this okay  now and not equal I don't know why I did  equal it's colon It's a key value pair  okay search qus now what the fastest\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"value pair  okay search qus now what the fastest  what are the question what is the  fastest gray hound dog okay what is the  fastest  speed for a  Grayhound  dog now here let's do a semantic search  first on the retriever let's first  retrieve and see if it's able to  retrieve the right set of chunks and I'm  going to say retrieve I don't know why  I'm feeling that retriever spelling is  wrong here that I have  typed okay retriever okay now  retriever uh dot it has a method called  get\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"now  retriever uh dot it has a method called  get relevant okay get  relevant  documents so it has this method that we  can use and we just pass our query so  let's pass the query here and then just  do  print print semantic search let's see if  it's able to retrieve this okay now  let's run this guys here python app.  okay uh let's see that if it's able to  you know  uh able to retrieve the right set of  chunks and all okay if your retriever is  working fine right that's the that's the  first\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"working fine right that's the that's the  first important of critical thing and  you can see I I got my response okay I  have k equals to one so I got one page  content one document it says docs are  usely popular something like that and it  it is not able to uh  retrieve I think the answer there which  is really surprising what I what I will  do now  is what if I make this k equals to 2 at  least and see if it's able to now  retrieve let's see  that three hound  dog uh and you can also set up\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"that three hound  dog uh and you can also set up some  thresold values now you saw that I  didn't get that 42s anywhere okay in  this  thing  [Music]  [Music]  now yeah you can see it over  here yeah this is the right okay fine  fine fine fine fine and I think I did a  mistake I think it was there also right  let me see that yet why I can't see you  know my eyes are like gone okay because  it's the same  response yeah you can see it over here  42 m per hour okay so let's keep it as  one as\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"42 m per hour okay so let's keep it as  one as well I was not attentive excuse  me for that now we are okay to do the  print uh semantic search and all guys  now now what we're going to do next is  let's he divide a  print uh print  thingy and and here we're going to write  our logic so let's create a let's first  create a for prompt let's call it chain  type  quar and here chain type quar and H this  is again a key value pairer so our  prompt going to be  prompt prompt and now let's create a\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"going to be  prompt prompt and now let's create a QA  chain so let's call it QA and I'm going  to use trial QA do from chain type okay  from chain type and  let's let's use here llm equals  llm and let me just go up so you can see  it chain type equals stuff you can also  use rerank refine map rerank Etc depends  what you what kind of ug cases you are  working on if it's a summarization use  the refine you know depends what kind of  use case you are dealing with now  retriever equals retriever\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content='are dealing with now  retriever equals retriever  and return Source document so return  underscore Source  documents documents equals  true and then we have chain type qus  equals chain type qu and you can also do  veros equals to  True veros equals to this is now we have  a qn this is our rag chain now okay  which which has just retrieval keyway  because we not looking at the  conversational part of it so it has a  retrieval qf from chain type it has an  llm it has a chain type stuff it has a', metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"has an  llm it has a chain type stuff it has a  retriever thingy it has a return Source  documents blah blah blah right now what  I'm going to do here is let's have a  response variable and I'm going to call  my query on top of it so  QA query and I'm going to just uh  uh run it now let's just do a print  response and see if it's working if we  getting the right response the question  that we ask using jaer the whole agenda  of this video guys is that when I use  jaer Alpha model it was not\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"is that when I use  jaer Alpha model it was not that good  for rag I will be honest and I love  hugging face H4 by the way I'm a huge  fan of the creators of this particular  llm the fine tune when they fine tune  this on  mral Alpha model was not that good for  rag implementation but when I tried beta  one it was really working fine and  that's why I wanted to create a video  for all of you so if you are looking at  some replacement of llama 2 or Mistral  you can look at J for the beta version\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"Mistral  you can look at J for the beta version  of it and that's why it's a beta version  right now let's do a print response and  see now here what I'm going to do is  let's just run this again Python app.py  and it will take a bit of time now  because it has first it will retrieve  the Retriever and then llm is going to  read those and then form a human like  response for you it says LM is not  defined oh  okay ah okay we have not llm we have uh  let's call it  llm okay  fine oh these are\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"uh  let's call it  llm okay  fine oh these are the mistakes if I keep  on if I keep on making this kind of  mistake how will I become a YouTuber  guys then probably I have to do a lot of  edits you know stop the video edit again  merge it  create no but coding is fun you know if  you do coding live you know if you then  it's much better to be honest in in My  Views and that's why a few of my videos  are a little longer but I know I want to  create those videos where I want you to  write codes\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"those videos where I want you to  write codes alongside me you know with  me you know in when I'm writing code in  video you also write in your machine  okay okay  now uh of course I do have my notes and  all you know for some kind of  explanation where required the pointers  and  all okay uh if you can see it entered  into new retrieval QA chain and probably  it takes around 30 to 40 seconds uh for  uh generating that response based on the  retreat R chunks based on the retrieved  chunks that\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"R chunks based on the retrieved  chunks that you see the chunks had been  retrieved over here you can see the 42 M  hour and meanwhile it's doing let's uh  let's write the gradio code to save some  time okay so what I'm going to do is  first I'm going to do a sample prompt  here so let's define a sample prompts  and it's going to be a list the first  one I will just keep it again  this so let's keep this here the first  one and I'll just do an ALT G here so  now the next one is why  should why\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"G here so  now the next one is why  should why should should we not  fed  chocolates uh now suppose this is the  question that I'm asking to the  dogs to the dogs okay now who the third  question that I can ask is  name two  factors which are  important for the dog scare okay suppose  this is the third question now this is  the sample prompts I'm going to build a  very fast grad application now I'm going  to write a function here Define get  response uh function that we can use it  so we don't\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"uh function that we can use it  so we don't have to generate here pass  an input input is going to be prompt so  let's oh no query is going to be  input excuse me sorry query is going to  be input let's have a chain type qua  here again chain type qu prompt  prompt this this is going to be inside a  function and maybe I can just use this  in entire thing here QA thingy so let's  just bring it  here  oh veros and all we can keep it uh true  and now let's have uh let's see that you  know if we\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"now let's have uh let's see that you  know if we got our response  guys I don't know what I  did we got our response fantastic you  know you can see the result by G4 7B  beta it says the fastest record recorded  speed for a gry hound dog is 42 mph  which is the right answer which is  fantastic you know and this is a complex  question trust me when I say it because  it has some numbers dates involved or  whatever it has some numbers and numeric  thing where struggles and it is a part  of that uh\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"where struggles and it is a part  of that uh PDF as well as you saw right  now you can improve the prom to keep the  model stick with the embeddings the  knowledge base not to its base knowledge  you can do that now here what I'm going  to do next is let's do a response thingy  again here so response equals QA query  and then just return the response so  let's just do return response I'm not  going to do any output passing maybe you  can do that as an exercise if you want  to do now this is\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"as an exercise if you want  to do now this is done let's create a  variable called iase for the interface  the gradio interface and what I'm going  to do is  gr dot why it's not showing  interface yeah and here I'm going to  define a lot of things the first is  inputs so input is going to be my  input so let's do that because that's  what the function is  and or then we have to define the input  because we haven't defined the input yet  so let's define an input first so what  I'm going to do\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"define an input first so what  I'm going to do  here input equals we're going to have a  text box so let's call it text and  inside this text I'm going to write my  label so label is going to be  prompt  oh prompt this is going to be your  prompt and then so  label let's keep it it so level let's  keep it false uh and then we going to  have maximum lines you can let's keep  it probably let's keep it one for now  okay maximum lines then we going to have  placeholder and placeholder is going to\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"to have  placeholder and placeholder is going to  be like enter  your enter your query or something like  that okay and then we have a container  equals false I don't want it to the  middle of it so container equals false  now this is I I going to be my input  with single a text where we're going to  put our questions okay uh I put it  outside  sorry now here let's have an output then  so we're going to have an out uh we  forgot to do one thing guys where the  function this is  really this is\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"guys where the  function this is  really this is really strange because I  need a function the function is going to  be the get response here so Gore  response  uh let's make it structure okay okay now  the function is done which is a callable  function that's what gradio application  will call right now after that I'm going  to have an outputs so outputs equals  here I'm going to have text and after  that output text is done title and title  let's call it my dog pit care uh  rag\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"and title  let's call it my dog pit care uh  rag  implementation using J  for 7B beta let remove my thingy or  let's let's remove my here okay okay I  remove the my thing and now after title  let's have a description if you want  I'll say rag  demo  for j47 B beta llm okay now this is  description after description what we're  going to do is we're going to have  examples and here you're going to pass  our sample prompts guys okay so we have  sample prompts see the examples which  will be in the\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"prompts see the examples which  will be in the below I will show that  and now let's have couple of let's say  flagging equals false you know if you  want to do flagging and all you can keep  some logs also a screenshot let's  disable screenshot if  possible uh screenshot equals false true  okay now this is done so just let's do  ifas do  launch okay so we are done with our code  let me just explain what we are doing  here quickly so you would have seen this  earlier like I when I ran it before\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"seen this  earlier like I when I ran it before  writing this gradual let's just uh we  don't need these now okay at least let's  just ah okay let's just do a  control ah excuse me control hash I just  hash those because I don't need  it we also don't need these two  lines oh this three lines by the way  so let's just do a control hash I don't  know why I'm again typing the  wrong but okay oh we also don't need  maybe let's let's also hide this okay we  don't need those things and then we have\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"we  don't need those things and then we have  sample prompts we have our keyw thing  going  on I don't know why this is giving  a excuse me it's giving a bad feeling  yeah the indentation was was  wrong see the  problem  okay then you have a response and then  you have the response I hope this is  okay okay uh this looks nice now okay so  now we have this this this this this all  okay let's run this now guys okay so  python app  doy  H and let's copy this probably I think I  would been running\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"copy this probably I think I  would been running it on 7860 right  let's see that  7860 I'm not running it so I haven't run  it grad  yet and now once I  you know hit enter here you can see a  simple you know very bad looking  interface where you can type some  question let's ask the same question  first and you can see this is the  question there what is the fastest speed  for a greyhound dog okay and these are  example prompts you can put multiple  examples you can do redo undo lot of  other\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"examples you can do redo undo lot of  other customization with gradio you can  do you know probably and here we are  using a Jeffer model for for rag  implementation guys I'm focusing on that  because uh it performs good so here we  are not passing any output we only have  the complete output the raw output that  we're going to present okay it's as I  said right it takes around 40 seconds uh  at average probably you know I don't  know why it's taking let's see how much  time it takes for this\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"taking let's see how much  time it takes for this one okay and it's  running on a CPU machine we are not  utilizing any CA  here you can see it has entered into the  QA chain and it's working it's  generating the response for you okay and  just to tell you that this entire code  will be available on the gith projectory  guys so you can just go and Fork it or  just download it and try it out yourself  that if and you can tell us the entire  uh subscribers of people are going to  watch this video\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"of people are going to  watch this video you finding can comment  in the comment box as well oh it took  more than a minute this is surprising  because it was really fast when we were  doing it in  terminal hm  let me just go back to  code I launch  I we have this get response function  defined which takes an input which is  query QA query this is  fine uh okay call LEL function inputs  equals  input  and okay prompt equals prompt let's see  this this is so surprising I don't know  why it's\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"this this is so surprising I don't know  why it's taking that much of time in the  grad  application but anyway we'll wait for it  or are  we it took 2  minutes input  value new this looks good function get  response inputs input text title sample  prompt i. launch anyway uh let's let's  see how much time it  takes oh it took I don't know it took  what is the it took around almost 2 and  a half minutes for this response but in  terminal it was really fast probably  have to check that out that\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"really fast probably  have to check that out that why it's  taking at least it will definitely not  take 2 minutes you know in terminal if  you if you use time to calculate it so  you can see the question here what is  the fastest speed for a greyh house dog  and we got our result the fastest  recorded speed for a greyhound dog was  42 m per hour it gives you the source  documents the page content and the PDF  as well that from which PDF you have you  can pass the output in a beautified  format\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"you  can pass the output in a beautified  format now this is what I wanted to know  show you guys in this video now you have  a skeleton you can just take my code and  work it make it better run it on a Cuda  machine how do you run this on Cuda  machine it's very easy you know when you  are using C Transformer you have to pass  the number of GPU layers and make sure  that you have cublas and metal or  whatever that you are working with you  have those supports depending on which  operating\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"have those supports depending on which  operating system you are on guys most of  the time you'll say that GPU layers n  number of GPU layers are not working for  C Transformer there are reasons for it  you might not have the support uh uh for  that operating system probably maybe you  should check it out know if you have the  Cub support if you're using it if you  have a blast because these are softwares  that you need the support uh to  influence it  so that all you know I'll give the code\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content=\"it  so that all you know I'll give the code  gith projectory in the description if  you like the video please uh hit a like  icon if you have any thoughts feedbacks  for me please let me know in the comment  box you can also reach out to me through  my social media channels and if you  haven't subscribed the channel yet  please do subscribe the channel and  support the channel guys that's what  motivates me you know uh that's all for  uh today's video guys thank you so much  for watching see\", metadata={'source': 'btuN-rrPhsM'}),\n",
       " Document(page_content='video guys thank you so much  for watching see you in the next  one', metadata={'source': 'btuN-rrPhsM'})]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans =yt_transcript(\"btuN-rrPhsM'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[0][\"duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckURL(BaseModel):\n",
    "    url: str\n",
    "class CheckSearchInput(BaseModel):\n",
    "    textInput: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_store = FAISS.from_documents(documents=texts,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following  text transcript to answer the user's question in detail.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\n",
    "    Only return the  answer below and nothing else.\n",
    "    Detailed answer:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                            temperature=0.2,convert_system_message_to_human=True,google_api_key=api)\n",
    "    \n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "llm=model,\n",
    "chain_type=\"stuff\",\n",
    "retriever=v_store.as_retriever(),\n",
    "return_source_documents = True,\n",
    "chain_type_kwargs= chain_type_kwargs,\n",
    "verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Zehyer 7b is a new large language model released by hugging face. It is a 7B parameter model that has been fine-tuned on a variety of tasks, including text generation, translation, and question answering.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"what is zehyer 7b model\")[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_url(url):\n",
    "    \"\"\"\n",
    "    validate if transcription is available.If available convert the transcript to embendings\n",
    "    and store it in croma vector store\n",
    "    input: url\n",
    "    output: response\n",
    "    \"\"\"\n",
    "    doc = get_transcript(url)\n",
    "    if doc!=\"not able to get transcript\":\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        texts = text_splitter.split_text(text=doc[0].page_content)\n",
    "        vector_index = Chroma.from_texts(texts, embeddings, collection_metadata={\"hnsw:space\": \"cosine\"}).as_retriever(search_kwargs={\"k\":1})\n",
    "    else:\n",
    "        return None\n",
    "    return vector_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = validate_url(\"https://www.youtube.com/watch?v=btuN-rrPhsM&t=2465s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_search(textInput):\n",
    "    \"\"\"\n",
    "    get the output using vecotr and llm\n",
    "    input: text\n",
    "    output: response\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\n",
    "    Only return the helpful answer below and nothing else.\n",
    "    Helpful answer:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "    load_vector_store = Chroma(persist_directory=\"stores/yt_cosine\", embedding_function=embeddings)\n",
    "    retriever = load_vector_store.as_retriever(search_kwargs={\"k\":1})\n",
    "    \n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                            temperature=0.2,convert_system_message_to_human=True,google_api_key=api)\n",
    "    \n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents = True,\n",
    "        chain_type_kwargs= chain_type_kwargs,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    response = qa(textInput)\n",
    "\n",
    "    print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\envs\\geminienv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'what is pet document', 'result': 'I do not have the answer to your question.', 'source_documents': []}\n"
     ]
    }
   ],
   "source": [
    "res = yt_search(\"what is pet document\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
