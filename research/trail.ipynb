{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello everyone welcome to AI anytime  channel so I'm starting a new playlist  called finetuning of large language  models in this playlist I will cover you  know the different aspect of fine-tuning  a large language model how to you know  finetune on your data set you will have  your custom data sets you want to fine  tune llms in this entire playlist we'll  have multiple videos we'll start with  the fundamentals and we starting in this  video as well the fundamentals you know  the different steps of pre-training  fine-tuning and the Noel techniques like  Laura and Kora how to select the right  hyper parameters for fine tuning task  what other tools that you can use to  fine tune like Excel toll for example  that we'll cover in this video which is  a low code fine-tuning tool and you know  how to use Laura and Kora to fine tune  uh with reduced memory uh consumption  now in this playlist we'll have videos  to F tune Lama 2 F tune Mistral fine  tune open Lama and fine tune other open  source llms and also the Clos Source One  like for example you want to find tune  the GPT models for example 3.5 you know  on your data or the Vex AI models like  you know that that basically fuels the  Palm uh Google B the Palm 2 models we  also fine tune with those so this is  completely focused on fine tuning this  entire playlist will have at least 10 to  12 videos but the first video today we  have going to talk about how to you know  F tune an llm and how to select the tool  the techniques and the data set and how  to configure and set it up uh in your  infrastructure for example how to choose  a compute what kind of cloud providers  you can use so all those nuances will  cover in this video this is a beginner  friendly video where we when I'm uh  assuming that you don't have fine-tuning  uh experience prior to this video okay  so let's start our experimentation with  uh this first video where we look at the  fundamentals of pre-training fine-tuning  and the techniques like Laura and  Kora all right uh guys so when we talk  about you know training llms there are  essentially three approaches that we see  and that's what we're going to look at  here so what I'm going to write first is  training uh  llms and we'll understand uh in very  brief about these three approaches so  the three approach that I'm talking so  let me first write the three  approaches so the three approaches that  we're going to have is  pre-training and after pre-training we  have fine  tuning and then then we have on a very  high level there are more but I'm just  writing here which are the commonly used  that we have used to fine tune in the  community so these are three approaches  pre-training fine-tuning Laura and Q  Laura now fine tuning and luras and the  you know even if you go more Downstream  uh I'm keeping this separate because I  will explain why I'm keeping fine tuning  separate uh than Lowa of course you can  use use Laura qora to F tune we'll see  that let's start a bit with definition  we'll go into code we'll also find tune  uh  shortly but let's first try to  understand because when I talk to you  know people who are trying to fine tune  basically they lack the fundamentals or  basic basics of these you know natural  language or the terminologies that we  see so I was talking to many of my you  know um colleagues or Community friends  and I was ask them like what are Q  projections what are projections that  you see in the lowas that you target  modules or whatever they had no idea  about it and that's what I want to cover  here that your fundamentals and Basics  should be covered because rest of the  thing I will show that how easily you  can f tune with tools like Exel all and  we'll see that uh in a bit now what do  you mean by  pre-training now it's pre-training  involves several steps now imagine if  you have a massive data set of Text data  so let me just write it over here you  have  uh massive Text data now the source can  be you know Wikipedia already Twitter  now X and you have you have your own you  know kind of an um your own proprietary  data kind of a thing and these are  mainly in terabytes okay we talking  about if you go to Lama 2 they say two  or three trillion I think it's two  trilon tokens we'll come to the tokens  later but just imagine extracting or  creating two trillion tokens would need  at least even more than terabytes of  data for you so in the pre-training step  is first is that you need a massive text  Data okay in in terabytes once you have  that you then identify uh model  architecture so let me just write here  identify the model  architecture and here I'm covering  because these topics are really vast uh  but I'm just covering on the very high  level but even once you have text Data  the lot of pre-processing steps like you  know removing pii you know looking at  the D duplications kind of a thing a lot  of other things that we look at but  we'll explain that once we are  fine-tuning okay but here just on very  high level to give you a definition of  pre-training fine-tuning and lowas and  qas now  uh once you have a model architecture  chosen or created specifically for the  task at hand then you need a tokenizer  okay so you need a tokenizer Let me just  write it over  here you need a  tokenizer okay that is trained to  appropriately handle your data handle  the data ensuring that it can  efficiently encode and decode text so  here I'm going to write  the tokenizer will play two role one is  of  encoding and decoding not necessarily  always encoding and decoding depends  what kind of task you are executing  we'll see  that now your tokenizer is done then  once you have the tokenizer ready the  data set that you have the massive data  set that you created is then  pre-processed using the tokenizer  vocabulary okay so let me just write it  over here  data set  is  pre-processed data set is  pre-processed using the tokenizers vocab  okay so you create the vocab vocab or  vocabulary of your tokenizers using  libraries like sentence piece that goes  you know that that becomes one of the  additions in the Transformers I will see  that uh shortly now you have data set is  pre-processed using you know tokenizers  vocabulary so let me just write it over  here tokenizers  vocab okay now this is important because  this will convert the raw text that you  have you have a raw text into  a uh into a format that is suitable for  training the model now here you will  have once you have this after this St  you will have a  suitable  you know uh uh  data in a  format for  training right now and this steps  involve you know mapping uh tokens to  the corresponding IDs incorporating  necessarily special token so let me just  write it because once I'm explaining the  config file the yaml file once we find  tune where you have lot of you know  special tokens like EOS BOS blah blah  blah this is how we'll cover that so  suitable data in a format for training  now here what we do now this steps  involves so I will keep it uh like this  so This steps involve mapping tokens we  we you would have seen something called  map using when we are using tokenizer to  map it now mapping tokens let me just  write mapping tokens to the IDS to the  respective or I think it's let's  corresponding might be the right  word corresponding IDs and then you know  uh incorporation of or rather let's  right  incorporating  any special  tokens special tokens okay or attention  mask can be other thing as well I just  recall no attention  mask okay now once the data is  pre-processed now it is ready for uh  training by the way these are the these  are the part of the same so I will now  uh these are the part that depends what  kind of architecture you have chosen but  now your data is ready you can pre-train  your your you ready to go into the  pre-training phase now what happens in  the pre-training phase okay let me let  me write it over here  what  happens in the pre-training  phase now in pre-training Phase let me  just write it the model learns to  predict the next word in a  sentence learns to  predict  the next word in a  sentence that's what happens or not only  this because it can also fill in so  we'll talk about what is filling and  what is Generation fill in missing words  as well so you can do fill in missing  words as well so let me just write it  over  here correct this is what it does now  this process of course involves  optimizing the models parameters you  know like it's trial and error kind of  an experimentation right this model you  have to optimize through different  parameters and that's an iterative  training procedure basically that  maximizes the likelihood of generating  the correct word or sequence of word  given the context now to accomplish this  pre-training  phase we typically employs a variant of  the self-supervised the learning  technique the model is presented with  partially masked input sequences where  certain tokens are of course we intent  intentionally hide that and it must  predict those missing tokens based on  the surrounding  context and because we have a massive  amounts of data right so we can train it  on that now model gradually develops a  reach understanding of language patterns  grammar and the semantic  relationship now once we talk about  filling so you have this fill in missing  words that we are talking that's  something that has been called as masked  language  model masked language model which is  called  MLM and when you talk about  Generation  generation now that has been called as  as are not limited to by the way but  this has been called calar learning  model that's called Cal language  modeling excuse me not learning Cal  language model or  modeling and that's why we use Auto Cal  from know pre-trained or whatever when  you use a Transformers pipeline now  these are the most these are the used  one we will be focusing on this this is  what we're going to focus C language  modeling now unlike masked language  modeling where you know certain tokens  are masked and the model predicts those  missing tokens Cal language modeling  focus on predicting the next word in a  sentence given the preceding  context now this step whatever you just  uh you just uh you just have seen here  on the screen this step is a  pre-training step here your model is now  able to understand the language patterns  Now understand uh the IC relationship  and all of those things basically it  gets the general language knowledge  making the model A proficient language  encoder right but it lacks a specific  knowledge so  after after  pre-training right you have a model now  it capture General  language so let can just write capture  General  language but you know it lacks and this  is what we're going to look at the next  fine-tuning step it lacks specific  knowledge lack specific knowledge about  a particular task or  domain  or domain and that's why we fine tune  because we want to fine tune it for a  specific purpose now to bridge this Gap  a subsequent fine-tuning phase follows  pre-training right that's where fine  tuning comes in guys in the play now  we'll look we'll talk about fine tuning  right now we only covering theoretical  aspect here we'll go into code a bit uh  so uh in a bit now let's talk about fine  tuning if you want to skip this part you  can also skip this you know but I will  recommend that you should have the  knowledge otherwise you will not be able  to build or find you know whatever  you'll be only having very high level  knowledge if you want to achieve that  you don't have to even learn those  things those are so many low code no  code fine tuning tool just upload your  data and get a finetune model but if you  want to make your carer in this field  you should understand everything from  scratch which is required of course when  I say everything don't go and read now  Marco Chain by the way that will not  help  uh now once we talk about finetuning now  after the pre-training phase where where  the model learns General language  knowledge fine tuning  allows let me write with  this now fine tuning  allows us to  specialize specialize the models  capability specialize the llms I'll  write rather llms  capabilities and optimize its  performance and  optimize its  performance  okay or let me make it more descriptive  on  a  narrower or a downstream task or a task  is specific data set or something like  that right let me just write task is  specific data  set okay uh this is what we do now fine  tuning of course if you look at the  pre-training we have covered all of this  high level steps text Data identifying  the model architecture tokenizer blah  blah blah now fing also uh similarly  also it also has many  steps so you have you have a task  specific data set that has been gathered  that that will probably have labeled  examples relevant to the desired task  now for example let's talk about some a  very famous word nowadays in Industry  that has been used called instruct  tuning or instruction tuned model we  call about instruction tuned model now  for instruction tuned  model a data set of instru instruction  and response pair is gathered so first  thing that you need a data set here  again a task specific data set so for  this if you are talking about  instruction uh tuned model you need a  data set of your instruction and  response instruction and  response data set is gathered now these  are basically pairs it can be a question  answer pair now the finetuning data set  size is significantly smaller than than  the you know your pre-training so you  don't need that huge data set now but of  course even even for this case you need  a sufficient uh uh like data set having  you know at least 10,000 question answer  pairs or something like that  now uh when the pre-training model is  initialized with its previously learned  parameters the model is then trained on  task specific data set optimizing its  parameter to minimize a task is specific  loss function loss function means how of  the model was from the desired result  that basically very lemon terms now I'm  writing  here uh task  specific loss  function task specific loss function now  during the fine tuning process the  parameters that we will have from the  pre-train models are then adjusted okay  we we will'll focus on this word a lot  that's called adjusted but let me write  it over here only  now uh the params let me just write like  this the  params of the pre-trained  model of the pre-trained model are  adjusted  are adjusted  using you know and focus on this word  adjusted we always talk about adjusting  weights let's adjust the weight and all  of those adjustment that we do with the  weights it all Bo down to weights guys  when you talk about fine tuning Now  using something called gradient based  optimization algorithms not limited to  but this is what has been used in the  recent fine-tuned model that's called  gradient based optimization  algorithms and if you have worked with  deep learning you will probably know  about this gradient based optimization  alos now one of these alos can be uh  SGD okay let me just write SGD or  Adam uh whatever there are there are a  lot more okay now SGD for example Le I  hope I spelled it right uh sastic and I  hope this is right sastic gradient  descent now the gradients are computed  by back propagating the law so again the  back propagation comes in okay so it the  gradients are then first computed by  back propagation the LW through the  model's layers you will have n numbers  of layers allowing the model to learn  from its mistakes and update its  parameters accordingly now you will have  a back propagation working there for you  in this models layers that helps you the  model will learn from the mistakes every  time and then update the parameter so it  improves itself now to enhance the fine  tuning you will have you know different  additional techniques here so let me  just write it in the one point here more  about fine tuning then we move to Lura  and Q luras and then we go up in a bit  of fine tuning kind of a thing now  uh okay  so you can also improve as I was saying  you can improve through you know uh  learning rate  scheduling okay there are there lot of  other techniques that we can follow uh  learning rate  scheduling and regular regularization  method like dropouts or weight decays or  early stopping to prevent overfitting  and whatnot right now these techniques  help optimize the models generalization  always remember that your fin tuned  model should not memorize but should  generalize okay uh on your unseen data  or the validation of new data now model  generalization and prevent it from  memorizing as I said the train data set  too closely now this is on the fine  tuning step now we are moving towards  some of  the newly  developed  techniques uh in the ecosystem that's  called a  Lowa low rank  adaptation they have a very good  research paper on Laura everybody should  read that paper low rank  adaptation now Laura is interesting  without Laura we would not have seen  this you know uh I'll say the rise of  the open source llms like mral or Jer  whatever you know the way we find unit  because we do not have the resources to  do that right so for people like you  know the US the researchers the  developers the people working in  startups they need these kind of you  know Frameworks to work with now because  see fine tuning is expensive as you know  don't be live in a Dreamland if you are  a a researcher or that you will be able  to create something like llama to with a  limited budget it's not possible when  you see how Mistral has been trained  they had invested Millions into that so  you can of course fine tune a few models  on your small data set but the  performance then again will not be  similar so it's fine tuning age so let  me write ft is  expensive it's simple it's really  expensive that requires you know 100 of  GBS of virtual Rams the vram that's  where GPU comes in guys you know to  train you know multi-billion parameters  models to solve this specific problem  Laura was introduced okay it was  proposed uh you know it's compared to  compared to if you compare to fine  tuning opt opt 175 B with Adam that that  has been the benchmarked in the paper  also Lura can reduce the number of  trainable parameters by 10,000  times and the GPU memory requirement by  over three times just as in right now a  3X memory requirement reduction is still  in the realm of unfeasible for us people  like us because 3x is still not that  right then that's we need Kora we'll  talk about that okay so let's talk about  so if Laura is giving you for example  and we will cover Laura in detail that's  why I'm not writing a lot of things over  here we will because we have to work  with Laura and Kora when we are fine  tuning so we'll focus on that now  Lura uh 3x okay I was saying 3x X now 3x  memory requirement now  imagine but this is also not something  that we can work with it even we need  more and that's where Kora came in  picture Okay so with  Kora and if you don't understand right  now about Lura and K don't worry I will  cover that each and everything in detail  each and every line that we write in  luras and Q the code the confix whatever  the argument that we work with I will  explain each and every line now Kora  just word just add the word quantized so  I'm not writing everything but  quantization quantized okay a quantized  low rank adaptation it uses a library  that's called bits and bytes okay so it  uses a  library called bits and bytes B  andb this is a library that you know it  uses on the Fly By the way and it  achieves a near lossless  quantization so when I talk about  lossless your performance degradation is  not much of course there can be a bit of  performance degradation or there will be  reduction in the performance but that  might be myth nobody has you know has  done a really a research on big sample  that that really happens once you  quantize the llm but of course there can  be a bit of performance degradation but  that's why we calling it bits and  bites and that provides you a lossless  quantization of language models and  applies it to the Lowa procedure because  QA is based on luras by the way now this  results in a massive reductions in  memory  requirement enabling the training of  models as large as like Llama  270b Or you can do that on you know for  example  2x 2x RTX  3090 you can do that you can also uh and  and by the way if you compare let me  just uh write it okay let me just do it  if you compare without Kora this will  take around a00 which is one of the most  used GPU to train llms or create llms  you need which is by the way 800 is 80GB  80GB  GPU now you you need 16 x you need 16  different a00 gpus not one GPU node that  you need you need 16 okay so just  imagine how much it reduce if you use  Kora you know it boils down to 2 RTX 390  and of course if you have 1 a00 much  better okay to do that uh but just  imagine uh and the cost goes down once  you use loraa and  Q okay now let's see the question comes  in that okay we have been talking about  all of this thing how can we achieve our  task how can we you know F tune what  kind of machine we have to use and all  of the question that you will have in  your mind so for that I will tell you  that let's go into fine tuning so let me  just write over  here fine  tuning we are now going into uh fine  tuning  step uh in this tutorial that I'm  creating you know we'll be focusing of  you know 6B or 7B llms not more than  that because I want to keep it as a  tutorial you can explore yourself if you  have compute and all I do have compute  but you should learn and you should of  course do it yourself right now the  first thing that you think will come in  your mind is training  compute that tell me about training  compute and that's what I also you know  get worried okay training comput now for  training of course you need a GPU to do  that we'll talk about model and data set  separately that's something that we'll  have  something uh now if you want to F tune  uh model like Lama 27b or Mistral 7B let  me just write it over here now these are  the model I'm writing because this  models are commercially available to use  with an Enterprise you can make money  you know using these models guys because  you'll be building a product so don't  see that just you'll be making money by  using this model but once you build a  product on top of this now Lama to 7B  and mral 7B if you want to F tune on a  very uh on a very high level I'll say  these are the requirement that you need  the alha talk about  memory and this is based on Uther Uther  AI Transformers math 101 blog post I  will give the link in description if you  want to understand how calcul  calculations work you know to decide a  compute you should look at Transformer  math 101 blog post find the link in  description now now the memory  requirement is around let's keep it like  150 to  195  GB this would be the memory requirement  you know to F  tune okay  now so renting gpus now the next thing  is how to rent  gpus or how to use a GPU there are many  options that you can  consider uh one is uh the most I will  it's not in any order but I will write  in my Preferred Choice okay uh in an  order I rely on  runpod runp pod. I rely on runp pod then  there is V  AI then there is Lambda  labs and then then you have hyperscalers  like big players you know I will just  only write AWS say maker because I have  worked with it a lot so these are the  top and you have your Google collab also  how can I forget collab our life saer  right now run pod is my Preferred Choice  hands down run pod is my preferred  choice I will recommend you to choose  runp or between Lambda Labs but Lambda  Labs service support and all that are  very bad okay so runp pod is something  that I recommend it's extremely easy to  work with and also affordable if you're  looking for cheapest option that you can  afford which is V so let me call it  here cheapest I'm writing cheapest I'm  not writing affordable affordable is  like runp now here security can be a  concern so look at if you looking at  secure cloud or Community Cloud kind of  a thing I recommend runp that's I said  okay now if you have a super computer  you can also do with that but I do not  have  now the next thing is uh once you have  training compute decided once you have a  training compute then you have to gather  data  set I show you that how you can get a  data set if you do not have one the main  source for learning purpose is hugging  face data sets once you have the data  set and I'll cover a few things in data  set because you know once if you have  you are creating your own data set you  have to look at something called Uh  there's few things you have to look at  one is diversity in  data you do not want your models to only  do one very specific task you know you  should have a bit of diversity now  assumed a use case like you know you're  training a chat  model that does not mean that data would  only be about one specific type of chat  right you you will want to diversify the  chat examples you know the samples uh  like different scenarios so model can  learn how to generate outputs of various  type of input now imagine one of your  Ender comes in and start putting about  some question that your model have not  even seen you know so at Le those kind  of things so it it understands the  semantics better now the size of data  set on a very uh like a thumb Ru your  file size should be 10 MB that's how I  decide if you are using a CSV with  question answer payers your file size  should be 10 MB that's how I go with  okay 10 MB of your data okay now or at  least I'll recommend 10,000 question  answer  pair and the quality of data the this is  important because if you look at the  latest model and this is the most  important by the way this is the most  important thing F example F llm by  Microsoft or Orca for example these are  the examples where organizations have  shown that how a better quality data can  help you you know train an llm on a very  smaller size like 3B or 1.5b kind of a  parameter of parms this is important to  understand now we'll not talk about data  at this moment let's let's see that so  to use runp pod what we have to do is to  go to runp  pod uh dashboard here and you can see I  already have a template has been  deployed but it's very easy to do that  you have to come to secure Cloud you can  also look for Community Cloud but I  prefer this it's Community cloud is bit  cheaper because your GPU can be also  availability is an issue on community  Cloud because the same GPU can be rented  by somebody else's as well if you are  not using that now that's on the  community Cloud uh on secure Cloud they  have two things latest generation and  previous generation on latest generation  you will see the latest of gpus like h00  which gives you 80GB vram with 250 GB  Ram you know which which is the most  expensive one over here then you also  have previous generation with a00 then  that's what we're going to use okay so  a00 you can see almost $2 per hour so  one hour training you cost $2 so on for  example if you have decent enough data  set like 2,000 3,000 rows $5 you'll be  able to uh do it now uh for that let me  first show you a couple of things we  have eight eight maximum that you can uh  you know spin up over here the eight  different node of that particular 800  GPU but this is not what we want to do  okay uh so let's see how we can deploy  it uh uh how we can find unit so I  already have deployed what you have to  do you have to click on deploy and it  will deploy it will take a few minutes  to deploy it and then there will be  option to connect once you click on  connect it will show open Jupiter lab  and that's what I did over here I have a  Jupiter lab now the first thing that we  have to do is we have to get clone the  Excel toll so let's get clone so get  clone uh and then we'll take that from  here so come over here let me just click  on  this this this and then you clone you'll  see a folder here now let's let's go  inside the folder in the examples you  will see different models which are  supported you can also see the mial are  also supported very soon in this okay if  you click on mial you will see mial yaml  so mial is also there 8X 7B model by  Mistral the mixture of experts now if  you go back to examples you will see Cod  llama Lama 2 Mamba Mistral MPT pythia  red Pama and some other LMS are also  there  let's open for example open Lama so if  you open open Lama you will see config  yaml now in config yaml you will see all  these details I will explain each and  everything you know in a bit what what  do we mean by loading 8bit loading 4bit  what is Lura R which is not there  because that will be in your QA or Lura  if you click on Lura yal this is a  configuration file which helps you you  know Excel toll basically takes this  single file as an input and F tune your  model here you have to Define your data  set if you look at the data set it says  gp4 llm clean there is nothing but this  is available on your hugging face  repository so basically T the data from  there but you can also do it from  locally as well so if you have local uh  uh machine where you are running this  and you have your data locally you can  also uh uh assign the path from that  local machine as well it has your  validation adapter blah blah blah and  we'll explain that everything uh in a  bit what do we mean by qware bits and  bite things will come in picture you can  see load in 4bit will be true for Q  because we want to load this model in  4bit and something like that right let's  do that so what I'm going to do is uh  first thing let me go back to my folder  here and you will have a requirements  txt let's expand that now you have your  requirements txt now in requirement txt  we'll find out everything okay what are  the requirements thing that you know you  have extra and Cuda you are getting from  here and lot of other things that you  are getting now uh we'll make some  hashes we don't need this or rather what  we can do we can just remove this from  here we do not need this anymore because  we already have Cuda with us over here  now Cuda is not required any other cudas  that you see I do not see any other  cudas over here I'm just looking for  torch Auto packaging pip Transformers  tokenizers bits and bytes accelerate  deep speed you know uh add  fire  pyl data  sets flash attention you can see 2.3.3  sentence piece wend B ws and biases eops  X forers Optimum HF Transformers Kama  Numba Bird score for evaluation evaluate  R rou score for Val evaluation as well  you know we have psychic learn art FS  chat graduate and fine let's save this  here here okay now once you save it I'll  just close this now let's go CD inside  it have to CD so let's do CD so CD XEL  to and you can see ax AO I am always you  know typing it wrong but I'll just CD  inside it and let's do a PWD to find out  what is my present working directory you  can see it Exel tall uh now the next  thing that we have to do is okay we have  to look at the inst installing the  packages let's do what they suggest I'm  going to do pip install  packaging the PIP install packaging will  is a requirement already satisfied the  next thing is PIP  install and then hyphen e which  basically says okay within this  directory or we can you can also take it  from you know their uh I'm saying okay  flash attention and deep speed so let me  just do not need that okay deep speed  okay flash attention and deep  speed uh to let's run  this what what are the thing that I am  making a mistake  uh deep speed flash attention H it  should not be dot here my bad it should  be dot should be the outside of  here okay you have your flash attention  and deep speed thing going on okay over  there it will install everything which  is in your requirements txt you can see  Bird score Numba sentence piece which  the dependency of Transformer helps with  you know vocabulary addition tokenizer  blah blah blah know pep has been  installed bits and bytes accelerate  that's looking good uh let's see it out  over there come  down flash attention make sure you have  flash attention greater than 2.0 flat  flash attention one point will give you  error okay uh let's come  down it's installing it once you install  that you have you can also go to the  data set path if you don't want to use  this data you can also do that with  other data as well but I will keep the  same  data but we can make few changes if you  want let's know because we are relying  on Laura here we'll use Lura so if you  come to Lura okay let me come to Laura  low rank adaptation now once you come to  Laura you can do a bit of changes like  you know let me see what is the  EPO okay uh where is that uh number of  epoch this is too much I don't want four  Epoch okay let's do it for one Epoch  okay so for we'll do it for one  Epoch okay the number of epo become your  number epox become one micro batch size  I will also make it one and gradient  accumulation steps I'll make this eight  okay I will explain that don't worry if  you don't understand these terms we'll  explain each and every this  terminologist which has been written  over here now I made some  changes uh and then I'll go back back  and then I will execute my learnings  okay that's what I'm going to do okay  now let me just do a  save okay uh and if you come you can  also see it over here this how you can  train it out you can see it says uses  this how you can train this is how you  can  inference all right so execution of  learning will happen like that so let's  come to fine tuning here okay now we are  done we'll add few  sales and once you have done all of  these changes let's go back and maybe  you can copy this thing uh quickly let's  copy  this let me come over  here and I have to add  that you can see excelerate launch  hyphen M Exel to CLI train examples it  has open  Llama now for example if currently you  see it says open Lama now you have to go  to open Lama 3B I don't know which one I  changed up okay uh if you go to  Laura yeah uh yeah this is what I also  changed okay fine now let me just uh  close this one low yl and let's run  it now once you run it you will see a  few warnings you can ignore those  warnings don't not have to worry about  those  warnings  you can see it's currently downloading  the data from hugging face it will you  know uh create a train and validation  splits you can see it's currently doing  it will also map the to with the  tokenizers will also do that you can see  it's happening happening over  here you have to wait for all these  process basically what they do guys  right it's imagine this as a low code  fine tuning tool if you don't want to  write a lot of code you just want to  make some changes because they already  have created the yaml for you okay this  is what they have done you can see total  number of steps total number of tokens  over  here find out the total number of tokens  currently getting the model file which  is PCH model. bin the file that it's  getting you can see it's around 6.85 GB  you know runport is basically will have  some cash of course definitely they will  have some cash and contain  cast in the container because you know  you are just fetching it from hugging  fish directly that's why it's bit faster  if you do it of course locally it will  probably be not that FAS okay uh it will  take a bit of time let's see how much  time it takes I don't want to pause the  video because I want to show you each  and everything that goes if you want to  you can skip the video of course you can  see you know extra special tokens EOS  BOS paddings total number of tokens  supervised tokens all those details over  here which have been listed okay that  you can find it out  once the model has been downloaded which  is your pytorch model. bin the next  thing that we have to do it will also  load the model okay so basically once it  load the model in the in that uh once it  loads the model you Cuda kernel should  be there because once it's loading it  needs GPU to load that okay on on a  device that's when we do dot two device  or something to you know pipe that with  CA  okay you can see it says starting  trainer so it has a trainer argument I  will explain that don't worry if you do  not know about trainers and all we'll  explain each and everything in detail  now guys what will happen you see can it  has started training okay you can see  efficiency estimate total number of  tokens per device it's training your on  your data now what I'm going to do is  I'm going to pause the video till it  train or F Tunes it and then we'll look  after that all right uh so you can see  uh it has been fine tuned the model that  we wanted to fine tune we have fine  tuned the llm and you can find out uh  train samples per second train runtime  train steps per second train loss EPO it  has been completed 100% it took more  than 1 hour it took around 2 hours you  know it took 1 hour 55 minutes you can  see it over here uh let's go back here  in the  workspace Exel  all and you can find out here you have a  folder called Laura out now in Lura out  you have your last checkpoint and you  can keep on saving checkpoints we'll  cover that maybe in the upcoming videos  where we will have a lot of videos on  fine tuning now but what I'm trying to  say is that you can also save it on  multiple checkpoints for example if you  want to use a th checkpoints with you  can also do that but you can see here we  have our adapter model. bin and adapter  config right now you can also merge that  with Pip but I'm not covering that part  right now here because you would have no  idea about pip if you a beginner so now  you just you just saw that how we find  tuna model and we got our checkpoint  weights over here you can find out this  is the last checkpoint if you want to  use this which has a config Json which  has if you click on the config Json it  will open a Json file for you then you  have your you know safe tensors okay the  model weights then you have Optimizer  dopy PT then you have your scheder you  have your Trend State you can file out  all the related information over here  you can also close this target module  seven items you can find out all the  different projections I will cover each  and everything in detail what do we mean  by this now it shows that uh sometimes  it gives you error uh if you look at  this gradi if you run this gradio  sometimes in runp it it gives you an  error also because let's try it out try  it out our luck here I just want to show  you that you can also do  that yeah gradio installation has issues  with in within runp pod sometimes when  you are using  it but anyway uh we'll move if it works  it will open a gradio uh for you a  gradio app gradio is a python library  that helps you build you know uh web  apps okay you can see it says running on  local over here okay now it runs on a  local URL okay you have to probably do a  true uh s equals to True something to  you know get it on a URL okay now you  can also click on let's click on this  now once you click on  this it says this share link expires in  72 hours for free permanent hosting and  GPU upgrades run gradate deploy okay now  you can see that we have an axel toall  gradio interface okay now here you can  try it out now if you let's copy this  and see what kind of response or even if  it's generating any response or not so  if you click on  this it will either give you an error or  it says okay you can see give three tips  for staying healthy and you can see it  out over here eat a healthy diet consume  plenty of fruits vegetables whole grains  lean proteins blah blah blah and you can  see the speed as well because we are on  800 right now that you know and it's  generating your responses pretty fast  right look at the tokens per second over  here now you have an interface that that  is working on your data your data that  you have from hugging face for example  we have used Alpa here Alpa 2K  test you can see it's how it says how  can we reduce air pollution let me ask a  similar question but not exactly the  same so what I'm going to ask here  is tell  me the ways I can  reduce pollution let me ask this  question and I'm asking tell me the ways  I can reduce pollution it says make a  conso and when you are working with  python you have to use a prom to look at  the uh basically to par the output a bit  but this is fantastic right now you can  see it  says make a conscious effort to walk  bike or car Pole to work is school or  irand rather than relying on a single  occupancy vehicle huge public  transportation car pooling blah blah  blah Reduce Reuse and recycle talking  about a bit of sustainability over here  now this is fantastic you saw in two  hours at least for one EPO of course  that is fully customizable as we saw in  the uh over there in the yaml files but  this is how easy it is nowadays to F  tune guys we'll now talk about we'll go  back to  our uh cast here and we'll talk about  each and everything in detail okay now  how did we find tune what are the sh  config that we have considered and all  of those things so let's start our  journey with all these parameters now  let's understand how Laura works and  what is Laura and how these parameters  are being decided that we have seen in  the yaml file as well so let me just  write about Laura here so we're going to  talk about Laura  now now it's a Training Method let's see  this as a Training Method let's keep it  lemon so you can understand better it's  a Training  Method so I'm writing it's a Training  Method to designed to ex designed to  expedite the training process of  llms training process of  llms this helps you with of course  memory consumption it reduces the memory  conj by introducing something called  pairs of rank decomposition weight  matrices this is important so what I'm  going to write here is look at this word  here okay something called rank  decomposition and basically that's a  pairs so I'm going to write pairs of  rank  decomposition  matrices  and this basically also known as right  we have a as we also talked about this  earlier called update  matrices  okay and you will have some existing  weights already of the pre-trained llms  that we're going to use so basically you  know it it helps you you know add these  weights okay training this basically the  new added weights  okay now  there are three things that you should  consider when we talking about loraa the  first is the or I'll say three or fours  let's let's understand that okay  preservation of pre-trained Weights so  let me just write it over  here so preservation  of pre-trained  Weights that's the first thing now what  we understand by this like Laura  basically maintains the Frozen state of  previously trained weights that  basically minimize the risk of  catastrophic  forgetting now this particular step  ensures that the model retains it  existing knowledge so let me just write  it over here okay  model  retains the its existing knowledge while  adopting to existing knowledge  while adapting to the new data that's  why we call it right that LM still has  the base knowledge and that's why  sometimes it hallucinates gives it from  the base knowledge right while adapting  to new  data now the second thing which is  important for you to understand the  portability of trained weights can you  port that okay so let me just write it  over here  portability  of trained  weights the rank decomposition matrices  that that gets used in loraa have  significantly fewer parameters of course  right that's why it helps you with the  memory reduction right when you compare  this with original model now this  particular characteristic allows the  trained loraa weights to be easily  transferred and utilized in other  context making them highly portable so  it's highly  portable now the third thing is the  third advantage that we're going to talk  about is integration with attention  layers integration with attention  layer now Lowa matrices are you know  Incorporated basically into the  attention layers of the original model  the adaptation scale parameters we we'll  see that parameter once we are looking  at the code allows control over the  extent to which model adjust to the new  training data and that's where we'll see  about you know alpha or the scales that  has been used now the next one is memory  efficiency that's the advantage that we  also talk about is memory  efficiency now it's improved memory  efficiency as we know opens up you know  the possibility of running fine tune  task unless then as we discussed earlier  3x the required compute for a native  fine tuning process say without Laura  let's look at the Lura hyper parameter  let's look at the code now to understand  uh the Lura hyper parameter so go to  examples of the same exal GitHub  repository let's open any we trained  basically open Lama right find now you  can also open Mistral Also let's open oh  let me open Mistral now to show  you so you can see I'm opening Mistral  and the mistal let's go to  Kora now on the cura I will first talk  about the Laura config let me make it  bigger so you can see it and let's talk  about this these are your Lura hyper  parameters that we're going to talk  about and I will explain that what we  mean by this you find tun any llms you  go to any videos any blog you will see  this code now you should have you should  have the understanding you should know  that what are these means okay so let me  just show you over s it over here that  what are these things mean now on the  memory efficiency  the next thing that we're going to talk  about is Lowa hyper  parameters so I'm just writing Lura  hyper  parameters now the first thing if you go  back you have something called Lura R  which is nothing but the Lowa rank okay  so let me just write first thing you  should know what is Lowa rank okay so  let me just write it like that okay  that's called basically your Lowa rank  low rank adaptation rank what do we mean  by lower rank gu this basically  determines the number of rank  decomposition  matrices rank decomposition is applied  to weight matrices in order to reduce  memory consumption and computational  requirements if you look at the original  Lura paper recommends a rank of R equals  to weight eight okay so by standard let  me just write here you know by the paper  they recommend standard r equal to 8 but  here you can see for mistal you know in  Axel to GitHub repository for mral is  it's has been written  32 and eight is the minimum amount keep  in mind that higher rank if you have  higher  rank leads to better  results better  results and there's one tradeoff now but  for that you need High compute  so now you should know that if you are  getting any errors like okay you have  you know taking a lot of computational  power you can play around this hyper  parameter which lower rank make it eight  from 32 make it 16 or or something like  that you know even know it's a trial and  error  experimentation now the more complex  your data set your rank should be higher  if you have a very complex data set  where you have lot of relationship a lot  of derived relations and so on and so  forth you need your rank to be on the  higher side okay okay now to match a you  know a a full fine tune you can set the  rank to equal to the models hidden size  this is however of course not  recommended because it's a massive waste  of resources now how can you find out  the models hidden size but of course you  have to go through the config Json by  the way okay or so there are two ways of  reading or basically finding out so  let's say is  reading hidden  size  now there are two way one is config  Json and the other is that you can do it  through  Transformers Auto  model Transformers Auto  model I I think I can quickly you know  write the code here so let me just write  the code for you now this is how you  have your auto model so from Transformer  let's quickly get  it so for example from Transformers  import you have Auto model as we as we  do it we do auto model for  Cal so you have your Cal LM that's how  you import you define a model name then  you'll have an  identifier so let's define that model  name whatever then you have your that  you basically get your model now through  Auto model something like that now how  do you find the hidden size this is the  code to get the hidden size  you do it something called Model do  config do hidden State that's how you do  it hidden  size and then just print the hidden  size This Is How We Do It print the  hidden size right let me open uh  here Lama  2 let's go to you know any of this okay  it's fine  you can see it over here hidden size is  here  8192  right this is your hidden size you can  find it like here also and you can also  find it you know through Transformers  Library as on but this is not  recommended because it's a waste of  resource otherwise why would you use  Lura then but this is what it is right  if you want to really achieve those  performances of pre-training uh like how  meta has created Lama 2 or M A has cre  Mistral the next is let's go back next  is Lura Alpha okay so let's write it  Alpha here this is a scaling Factor now  I'm writing Lowa  Alpha now it's a scaling factor for the  Lura  determines the extent to which the model  is adapted towards new TR the alpha  value adjust the contribution of the  update matrices during the training  process or during the train proc  process lower value gives more weight to  the original data so if you have lower  value it gives more weight to the  original and it maintains the models  existing knowledge to a greater extent  so it will it will be more inclined  towards the you know your base knowledge  the base data that you have so let me  just write it so for example you you  will able to uh make a note of it I will  write lower  value Gibs  more  weight to the original  data to the original data and  maintain maintain the  models existing  knowledge knowledge to a greater  extent let's try it like  this okay this is what now if you can  see here it says Lowa  Alpha and that Lowa Alpha is 16 you can  make eight also you can make 16 also  that depends now the next is let's talk  about Lowa Target modules which is again  very important you know talking about  the Lowa Target modules that you see  over there now Lowa Target modules is  one of the important thing so let me  just write  Lowa Target  module oh you cannot see it I just  noticed Lowa Target module now in the  Lowa Target  module here you can determine which  specific weights and matrices are to be  trained the most basic ones to train are  the query vectors that's basically  called Q projection okay you would have  seen that word q projection so the most  basic ones are the query ve query  vectors and value vectors so let me just  write it over here you know query  vectors and then you have value  vectors that's called Q projection Q Pro  and this called V  Pro these are all project projection  matrices the names of these matrices  will differ from model to model of of  course depends on which llm you are  using you can find out the exact names  again there's a what you have to do keep  the same code AS written above just make  one changes which is your layer  names so layer names we you know  basically it's a dictionary that's how  you'll get it let me so it write it over  here model. State  dict model do state dict and then do  keys this is how you will get the and  basically you need a for Loop now  because there will be n number of layers  so you can get the for name in layer  names and then you can print that so  you'll see Q projection you know you  will see K projection you will see V  projection you will see o projection you  will see gate projection you will see  down projection you will see up  projection layer Norm weight blah blah  blah right so there can be n number of  Weights now the naming convention is  very easy model name do layer layer  number component and then it goes like  that so this is what you should note the  Q projection the projection matri Matrix  apply to the query vectors in your  attention mechanism of the Transformer  blocks transforms the input hidden  states to the desired dimensions for  Effective query representation and V  projection is bit different it's called  a value vectors in the tension mechanism  transforms the input hidden states to  the desired dimension for Effective  value representation so these two are  very important Q Pro and V Pro there are  others like that like K Pro which is key  vectors then you have o Pro different so  you can keep on looking at you know  different basically oo are nothing but  the output to the attention  mechanism so you know so however three  or four you know there are outliers as  well we have to look at outliers they  don't follow the naming convention is  specified here that I'm writing but they  have embedding token weights embed  tokens normalization weights normalize  then you have LM heads these are also if  you go back by the way excuse me sorry  then you have your you know which is not  of course here here because this might  not be using it but you have LM head let  me just write it out over here you have  LM  head so you have LM head then you have  embed  tokens then you have embed tokens then  you have normalization Norm these are  also that we consider when we are  training it now the LM head is nothing  but the output layer of a language model  language modeling I will say rather it's  responsible for generating predictions  or scores for the next  token based on the Learned  representation from the preceding layers  the previous one you know basically they  are placed in the bottom so important to  Target if your data data set has custom  syntax LM head is very important let me  just write it over here if your  data if your data has custom syntax  which is you know really  complex embed token they represent the  parameters associated with the embedding  layers of the model is like very  self-explanatory usually placed at the  beginning of the model as it just has to  map input tokens to their Vector  representation you have your input  tokens you have to first convert it to a  vector then the model will be able to  understand right it it needs a numerical  representation it cannot understand your  text Data it's important to Target again  if you have your custom syntax so this  goes same now normalization is very like  know very I say  common it's a normalization layer within  your model used to improve the stability  and convergence so basically helps you  with the convergence of you know deep  neural  networks this is what we look at the  Lura guys then we look at the Q Laura  now in QA we talk  about few things we'll talk about if you  go back here let me just  explain QA bits and bytes will be on  somewhere here so basically just explain  so quantized Lura is an efficient fine  tuning approach you know even makes it  more uh memory reduction so it include  few things the number one is back  propagation so let me just write over  here back  propagation of  gradients through a through a 4bit  quantized you know through a frozen  let's write uh  through a frozen 4bit quantized 4bit  quantized which is very very important  quantized into  Laura that's the first thing the second  thing is it's basically uses of a new  data type called nf4 you would have seen  that  nf4 that's word now what do we mean by  nf4 that's called 4bit normal flat  excuse me not flat flat float 4bit  normal  float now 4bit normal float basically  you know optimally handles normally  distributed weights you will have your  distributed weights it basically helps  you optimize those it's a new data type  you see nf4 so if you make nf4 true you  have to use bits and bytes and all for  of course for that as well now you have  your  nf4 then you also have few other things  like paged optimizers double  quantization you know to reduce use the  average memory footprint even further by  quanti quantizing the quantization  constant now these are the things that  are associated with Laura and qora then  you have your hyper parameters like  batch size and aox now I'll explain that  you don't need the tab or this to  explain that because these are very  common so now you understand this part  this part is very much self-explanatory  a base model code of you know you use  any fine tuning code take it from medium  take it from other YouTube videos  anywhere you will find this code on  very similar 99% of the code will be  same only thing that you have to change  is your data and the way you tokenize  and map those that's it and few of the  times if you are using some other LMS  now your data set is this you have your  data set where you are you know storing  it data set  validations output  directory qora out then you have your  adapter which is qora then the sequence  length you can see it says  8192 P to sequence true Laura I I have  already explained this part here Wenda B  which is weights and biases okay weights  and biases for you know machine learning  experimentation uh tracking and  monitoring now we'll talk about uh a few  things which is important let me first  come down okay this is special tokens we  have covered it now let me just go up  and explain that to  you okay now what is number of epo guys  let's talk about about number of number  of epoch the number of  epoch excuse  me the number of AO is an hyper  parameter in gradient descent you would  have seen the maxima and Minima you  would have seen this that that hyper  parabolic or parabolic graph once we see  the way you know back propagation works  and the model learns and the when we  talk about neural networks hyper  parameters in gradient descent  which controls the number of complete  passes through the training data set  each Epoch involves processing the  entire data set once and the models  parameters are updated after every Epoch  now batch size is a hyper parameter in  again gradient descent that determines  the number of training samples processed  before it's in batches it stands in the  batches how many sample are used in each  iteration to calculate the error and  then adjust the model in your back  propagation steps we'll talk about  stochastic gradient descent it's an  optimiz algorithm as we discussed  earlier to find the best internal  parameters for a  model aiming to minimize performance  measures like logarithmic loss or a mean  square error msse and you can find it  out more on the on internet as well  about that yeah  so now batch gradient descent sastic  gradient descent and mini batch gradient  descent there are three different ways  commonly used you know for training the  models  effectively now  you would have sometime this question  the difference between the batch versus  Epoch so the batch size is the number of  samples processed before the model is  updated the number of epo is the number  of complete passes through the training  data set so these are two different NS  understand with a quick example I'll  just show it this is the last maybe that  I will  show but let me uh show that okay over  here now assume you have a data set so  you have a data set of 200  samples a rows or it can be question  answer pair of rows in a csb and you  choose a batch size of five so for  example batch size of five and AO is  th000 this will not we will not do it in  a fine tuning llm because the computer  to Too Much five now this means the data  set will divided into how many the data  set will be divided into 4 40 batches so  dat will be divided into 40  batches because you have 200 samples it  has to be in 40  batches 40 batches each with five  samples how many each with five samples  right now the model weights will be  updated after each batch of five samples  after each batch of five samples model  weights will update model weights will  get  updated now this will also mean that one  Epoch will involve 40 batches or 40  updates to the model how many one  aoch will involve 40  batches or 40 update this is very  important guys you should know the  fundamentals  to fine tune the best way right now with  1,000 aox the model will will be exposed  to the entire data set 1,000 times so  that is total of how many 40,000 batches  just imagine how much compute power do  you need to do that so the larger B size  results in higher GPU memory right  higher GPU  memory and that's where will be using  gradient accumulation steps you see this  here gradient accumulation steps that's  why it is  important right gradient uh that that  has been used to overcome this problem  okay over  there now you have learning rates you  can see  0.02 learning rate is not that like you  know so SGD stochastic gradient descent  know estimates the errors and the the  way it learns right so think of learning  rate as a no that control the size of  steps taken to improve the model if the  learning rate is too small the model may  take a long time to learn or get stuck  in a suboptimal solution it might might  get a local Minima right so on the other  hand if the learning rate is too large  the model May learn too quickly and end  up with unstable so you need to find the  optimal or the right learning rate right  you know it is important as well now the  learning rate what else do we have so  gradient accumulation is very important  so higher batch sizes results in higher  memory consumption  now that's where we bring gradient  accumulation Ms to fix this it's a  mechanism to split the batch of samples  so you will have your for example you  have a global badge let me just draw it  if I  can let me show it you cannot see if  here Global badge then you will have  your uh mini badge so let me call it MB  so you have  mb0 then you have mb1 mini batch one and  you can you can see it over here that's  called micro batch size two right it's  all associated with it mb1 then you have  mb2 and then you have mb3 now imagine if  you have this then you have grad zero  again this will be the inside only so  you have grad zero you have grad one you  have grad two gradient two basically and  then you have grad three and then you  have Global batch  gradient very very interesting right  Global batch gradients  now into basically it splits into  several mini batches of samples that  will be run sequentially right now I'm  not covering back propagation because I  hope that you know what is back  propagation if you don't I will  recommend to watch you know some of the  videos which are already available on uh  YouTube there will be n number of videos  for that but yeah I think that concludes  for I think this part guys so what we  did in this video you know now you'll be  able to to understand each and  everything over here now you know what  is gradient checkpointing and these are  like warm- up state after how many steps  you want it to warm up how after how  many EPO you want to evaluate the model  on your validation data whatever you  know your saves after how many EPO you  want to save the weights and all of  those things right so these are bit uh  again you can go and there can be others  uh parameters as well but I wanted to  cover as much as possible now what we  did in this video guys so far it's a big  video it's a lengthy video I know but I  will recommend you to watch this video  completely as I said in the beginning as  well we started with the understanding  of pre-training training llms we looked  at the three different approach let me  summarize it we looked at the three  different approaches pre-training fine  tuning and Laura and  Kora we covered all of this  theoretically till high level on here  and then we moved after training compute  to run pod and on the runp we set up  something called Excel toll you know  Excel toll is a low  code uh fine-tuning tool for you so when  I say low code you should understand how  it works but you know it's very very  very powerful it's really important to  use this kind of tool to help on you on  your uh productivity and it helps you  become more efficient once you are fine  tuning it so we looked at Excel toll to  fine tune a large language model on this  particular data which is Alpa data I  also shown that how where you can change  your data you can look at a jonl file  and then you can f tune right we F tune  it for 2 hours almost and we got a  gradio application that we saw you could  see that how easy it is to spin up a  gradio that's already there for you and  we tested out with couple of prompts and  then we are ending with all of the hyper  parameters that are important as are  required you should know it we explain  that I hope you understood you got some  clarity now and you will have enough  understanding to now fine tune an llm  this ends our experiment guys in this  video that's all for this video guys I  hope you now have the enough  understanding of how to fine-tune a  large language model and how to select  the optimal hyperparameters for the  fine-tuning task and how can you use  tool like Excel toall that's the low  code uh tool for fine-tuning llms this  was the agenda of the video as well to  give you enough understanding  and where I wanted to cover the  fundamentals of pre-training fine-tuning  and the novel techniques like Laura and  Cur if you have any thoughts or  feedbacks please let me know in the  comment box you can also reach out to me  through my social media channels please  find those details on the channel about  us on the channel  Banner that's all uh for this video  please like the video hit the like icon  and if you haven't subscribed the  Channel please do subscribe the channel  share the video and Channel with your  friends and appear thank you so much for  watching see you in the  next\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=mrKuDK9dGlg&list=PLrLEqwuz-mRIEtuUEN8sse2XyksKNN4Om\", add_video_info=False,\n",
    "    language=[\"en\", \"id\"],\n",
    "    translation=\"en\",\n",
    ")\n",
    "d = loader.load()\n",
    "d[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"hello everyone welcome to AI anytime  channel so I'm starting a new playlist  called finetuning of large language  models in this playlist I will cover you  know the different aspect of fine-tuning  a large language model how to you know  finetune on your data set you will have  your custom data sets you want to fine  tune llms in this entire playlist we'll  have multiple videos we'll start with  the fundamentals and we starting in this  video as well the fundamentals you know  the different steps of pre-training  fine-tuning and the Noel techniques like  Laura and Kora how to select the right  hyper parameters for fine tuning task  what other tools that you can use to  fine tune like Excel toll for example  that we'll cover in this video which is  a low code fine-tuning tool and you know  how to use Laura and Kora to fine tune  uh with reduced memory uh consumption  now in this playlist we'll have videos  to F tune Lama 2 F tune Mistral fine  tune open Lama and fine tune other open  source llms and also the Clos Source One  like for example you want to find tune  the GPT models for example 3.5 you know  on your data or the Vex AI models like  you know that that basically fuels the  Palm uh Google B the Palm 2 models we  also fine tune with those so this is  completely focused on fine tuning this  entire playlist will have at least 10 to  12 videos but the first video today we  have going to talk about how to you know  F tune an llm and how to select the tool  the techniques and the data set and how  to configure and set it up uh in your  infrastructure for example how to choose  a compute what kind of cloud providers  you can use so all those nuances will  cover in this video this is a beginner  friendly video where we when I'm uh  assuming that you don't have fine-tuning  uh experience prior to this video okay  so let's start our experimentation with  uh this first video where we look at the  fundamentals of pre-training fine-tuning  and the techniques like Laura and  Kora all right uh guys so when we talk  about you know training llms there are  essentially three approaches that we see  and that's what we're going to look at  here so what I'm going to write first is  training uh  llms and we'll understand uh in very  brief about these three approaches so  the three approach that I'm talking so  let me first write the three  approaches so the three approaches that  we're going to have is  pre-training and after pre-training we  have fine  tuning and then then we have on a very  high level there are more but I'm just  writing here which are the commonly used  that we have used to fine tune in the  community so these are three approaches  pre-training fine-tuning Laura and Q  Laura now fine tuning and luras and the  you know even if you go more Downstream  uh I'm keeping this separate because I  will explain why I'm keeping fine tuning  separate uh than Lowa of course you can  use use Laura qora to F tune we'll see  that let's start a bit with definition  we'll go into code we'll also find tune  uh  shortly but let's first try to  understand because when I talk to you  know people who are trying to fine tune  basically they lack the fundamentals or  basic basics of these you know natural  language or the terminologies that we  see so I was talking to many of my you  know um colleagues or Community friends  and I was ask them like what are Q  projections what are projections that  you see in the lowas that you target  modules or whatever they had no idea  about it and that's what I want to cover  here that your fundamentals and Basics  should be covered because rest of the  thing I will show that how easily you  can f tune with tools like Exel all and  we'll see that uh in a bit now what do  you mean by  pre-training now it's pre-training  involves several steps now imagine if  you have a massive data set of Text data  so let me just write it over here you  have  uh massive Text data now the source can  be you know Wikipedia already Twitter  now X and you have you have your own you  know kind of an um your own proprietary  data kind of a thing and these are  mainly in terabytes okay we talking  about if you go to Lama 2 they say two  or three trillion I think it's two  trilon tokens we'll come to the tokens  later but just imagine extracting or  creating two trillion tokens would need  at least even more than terabytes of  data for you so in the pre-training step  is first is that you need a massive text  Data okay in in terabytes once you have  that you then identify uh model  architecture so let me just write here  identify the model  architecture and here I'm covering  because these topics are really vast uh  but I'm just covering on the very high  level but even once you have text Data  the lot of pre-processing steps like you  know removing pii you know looking at  the D duplications kind of a thing a lot  of other things that we look at but  we'll explain that once we are  fine-tuning okay but here just on very  high level to give you a definition of  pre-training fine-tuning and lowas and  qas now  uh once you have a model architecture  chosen or created specifically for the  task at hand then you need a tokenizer  okay so you need a tokenizer Let me just  write it over  here you need a  tokenizer okay that is trained to  appropriately handle your data handle  the data ensuring that it can  efficiently encode and decode text so  here I'm going to write  the tokenizer will play two role one is  of  encoding and decoding not necessarily  always encoding and decoding depends  what kind of task you are executing  we'll see  that now your tokenizer is done then  once you have the tokenizer ready the  data set that you have the massive data  set that you created is then  pre-processed using the tokenizer  vocabulary okay so let me just write it  over here  data set  is  pre-processed data set is  pre-processed using the tokenizers vocab  okay so you create the vocab vocab or  vocabulary of your tokenizers using  libraries like sentence piece that goes  you know that that becomes one of the  additions in the Transformers I will see  that uh shortly now you have data set is  pre-processed using you know tokenizers  vocabulary so let me just write it over  here tokenizers  vocab okay now this is important because  this will convert the raw text that you  have you have a raw text into  a uh into a format that is suitable for  training the model now here you will  have once you have this after this St  you will have a  suitable  you know uh uh  data in a  format for  training right now and this steps  involve you know mapping uh tokens to  the corresponding IDs incorporating  necessarily special token so let me just  write it because once I'm explaining the  config file the yaml file once we find  tune where you have lot of you know  special tokens like EOS BOS blah blah  blah this is how we'll cover that so  suitable data in a format for training  now here what we do now this steps  involves so I will keep it uh like this  so This steps involve mapping tokens we  we you would have seen something called  map using when we are using tokenizer to  map it now mapping tokens let me just  write mapping tokens to the IDS to the  respective or I think it's let's  corresponding might be the right  word corresponding IDs and then you know  uh incorporation of or rather let's  right  incorporating  any special  tokens special tokens okay or attention  mask can be other thing as well I just  recall no attention  mask okay now once the data is  pre-processed now it is ready for uh  training by the way these are the these  are the part of the same so I will now  uh these are the part that depends what  kind of architecture you have chosen but  now your data is ready you can pre-train  your your you ready to go into the  pre-training phase now what happens in  the pre-training phase okay let me let  me write it over here  what  happens in the pre-training  phase now in pre-training Phase let me  just write it the model learns to  predict the next word in a  sentence learns to  predict  the next word in a  sentence that's what happens or not only  this because it can also fill in so  we'll talk about what is filling and  what is Generation fill in missing words  as well so you can do fill in missing  words as well so let me just write it  over  here correct this is what it does now  this process of course involves  optimizing the models parameters you  know like it's trial and error kind of  an experimentation right this model you  have to optimize through different  parameters and that's an iterative  training procedure basically that  maximizes the likelihood of generating  the correct word or sequence of word  given the context now to accomplish this  pre-training  phase we typically employs a variant of  the self-supervised the learning  technique the model is presented with  partially masked input sequences where  certain tokens are of course we intent  intentionally hide that and it must  predict those missing tokens based on  the surrounding  context and because we have a massive  amounts of data right so we can train it  on that now model gradually develops a  reach understanding of language patterns  grammar and the semantic  relationship now once we talk about  filling so you have this fill in missing  words that we are talking that's  something that has been called as masked  language  model masked language model which is  called  MLM and when you talk about  Generation  generation now that has been called as  as are not limited to by the way but  this has been called calar learning  model that's called Cal language  modeling excuse me not learning Cal  language model or  modeling and that's why we use Auto Cal  from know pre-trained or whatever when  you use a Transformers pipeline now  these are the most these are the used  one we will be focusing on this this is  what we're going to focus C language  modeling now unlike masked language  modeling where you know certain tokens  are masked and the model predicts those  missing tokens Cal language modeling  focus on predicting the next word in a  sentence given the preceding  context now this step whatever you just  uh you just uh you just have seen here  on the screen this step is a  pre-training step here your model is now  able to understand the language patterns  Now understand uh the IC relationship  and all of those things basically it  gets the general language knowledge  making the model A proficient language  encoder right but it lacks a specific  knowledge so  after after  pre-training right you have a model now  it capture General  language so let can just write capture  General  language but you know it lacks and this  is what we're going to look at the next  fine-tuning step it lacks specific  knowledge lack specific knowledge about  a particular task or  domain  or domain and that's why we fine tune  because we want to fine tune it for a  specific purpose now to bridge this Gap  a subsequent fine-tuning phase follows  pre-training right that's where fine  tuning comes in guys in the play now  we'll look we'll talk about fine tuning  right now we only covering theoretical  aspect here we'll go into code a bit uh  so uh in a bit now let's talk about fine  tuning if you want to skip this part you  can also skip this you know but I will  recommend that you should have the  knowledge otherwise you will not be able  to build or find you know whatever  you'll be only having very high level  knowledge if you want to achieve that  you don't have to even learn those  things those are so many low code no  code fine tuning tool just upload your  data and get a finetune model but if you  want to make your carer in this field  you should understand everything from  scratch which is required of course when  I say everything don't go and read now  Marco Chain by the way that will not  help  uh now once we talk about finetuning now  after the pre-training phase where where  the model learns General language  knowledge fine tuning  allows let me write with  this now fine tuning  allows us to  specialize specialize the models  capability specialize the llms I'll  write rather llms  capabilities and optimize its  performance and  optimize its  performance  okay or let me make it more descriptive  on  a  narrower or a downstream task or a task  is specific data set or something like  that right let me just write task is  specific data  set okay uh this is what we do now fine  tuning of course if you look at the  pre-training we have covered all of this  high level steps text Data identifying  the model architecture tokenizer blah  blah blah now fing also uh similarly  also it also has many  steps so you have you have a task  specific data set that has been gathered  that that will probably have labeled  examples relevant to the desired task  now for example let's talk about some a  very famous word nowadays in Industry  that has been used called instruct  tuning or instruction tuned model we  call about instruction tuned model now  for instruction tuned  model a data set of instru instruction  and response pair is gathered so first  thing that you need a data set here  again a task specific data set so for  this if you are talking about  instruction uh tuned model you need a  data set of your instruction and  response instruction and  response data set is gathered now these  are basically pairs it can be a question  answer pair now the finetuning data set  size is significantly smaller than than  the you know your pre-training so you  don't need that huge data set now but of  course even even for this case you need  a sufficient uh uh like data set having  you know at least 10,000 question answer  pairs or something like that  now uh when the pre-training model is  initialized with its previously learned  parameters the model is then trained on  task specific data set optimizing its  parameter to minimize a task is specific  loss function loss function means how of  the model was from the desired result  that basically very lemon terms now I'm  writing  here uh task  specific loss  function task specific loss function now  during the fine tuning process the  parameters that we will have from the  pre-train models are then adjusted okay  we we will'll focus on this word a lot  that's called adjusted but let me write  it over here only  now uh the params let me just write like  this the  params of the pre-trained  model of the pre-trained model are  adjusted  are adjusted  using you know and focus on this word  adjusted we always talk about adjusting  weights let's adjust the weight and all  of those adjustment that we do with the  weights it all Bo down to weights guys  when you talk about fine tuning Now  using something called gradient based  optimization algorithms not limited to  but this is what has been used in the  recent fine-tuned model that's called  gradient based optimization  algorithms and if you have worked with  deep learning you will probably know  about this gradient based optimization  alos now one of these alos can be uh  SGD okay let me just write SGD or  Adam uh whatever there are there are a  lot more okay now SGD for example Le I  hope I spelled it right uh sastic and I  hope this is right sastic gradient  descent now the gradients are computed  by back propagating the law so again the  back propagation comes in okay so it the  gradients are then first computed by  back propagation the LW through the  model's layers you will have n numbers  of layers allowing the model to learn  from its mistakes and update its  parameters accordingly now you will have  a back propagation working there for you  in this models layers that helps you the  model will learn from the mistakes every  time and then update the parameter so it  improves itself now to enhance the fine  tuning you will have you know different  additional techniques here so let me  just write it in the one point here more  about fine tuning then we move to Lura  and Q luras and then we go up in a bit  of fine tuning kind of a thing now  uh okay  so you can also improve as I was saying  you can improve through you know uh  learning rate  scheduling okay there are there lot of  other techniques that we can follow uh  learning rate  scheduling and regular regularization  method like dropouts or weight decays or  early stopping to prevent overfitting  and whatnot right now these techniques  help optimize the models generalization  always remember that your fin tuned  model should not memorize but should  generalize okay uh on your unseen data  or the validation of new data now model  generalization and prevent it from  memorizing as I said the train data set  too closely now this is on the fine  tuning step now we are moving towards  some of  the newly  developed  techniques uh in the ecosystem that's  called a  Lowa low rank  adaptation they have a very good  research paper on Laura everybody should  read that paper low rank  adaptation now Laura is interesting  without Laura we would not have seen  this you know uh I'll say the rise of  the open source llms like mral or Jer  whatever you know the way we find unit  because we do not have the resources to  do that right so for people like you  know the US the researchers the  developers the people working in  startups they need these kind of you  know Frameworks to work with now because  see fine tuning is expensive as you know  don't be live in a Dreamland if you are  a a researcher or that you will be able  to create something like llama to with a  limited budget it's not possible when  you see how Mistral has been trained  they had invested Millions into that so  you can of course fine tune a few models  on your small data set but the  performance then again will not be  similar so it's fine tuning age so let  me write ft is  expensive it's simple it's really  expensive that requires you know 100 of  GBS of virtual Rams the vram that's  where GPU comes in guys you know to  train you know multi-billion parameters  models to solve this specific problem  Laura was introduced okay it was  proposed uh you know it's compared to  compared to if you compare to fine  tuning opt opt 175 B with Adam that that  has been the benchmarked in the paper  also Lura can reduce the number of  trainable parameters by 10,000  times and the GPU memory requirement by  over three times just as in right now a  3X memory requirement reduction is still  in the realm of unfeasible for us people  like us because 3x is still not that  right then that's we need Kora we'll  talk about that okay so let's talk about  so if Laura is giving you for example  and we will cover Laura in detail that's  why I'm not writing a lot of things over  here we will because we have to work  with Laura and Kora when we are fine  tuning so we'll focus on that now  Lura uh 3x okay I was saying 3x X now 3x  memory requirement now  imagine but this is also not something  that we can work with it even we need  more and that's where Kora came in  picture Okay so with  Kora and if you don't understand right  now about Lura and K don't worry I will  cover that each and everything in detail  each and every line that we write in  luras and Q the code the confix whatever  the argument that we work with I will  explain each and every line now Kora  just word just add the word quantized so  I'm not writing everything but  quantization quantized okay a quantized  low rank adaptation it uses a library  that's called bits and bytes okay so it  uses a  library called bits and bytes B  andb this is a library that you know it  uses on the Fly By the way and it  achieves a near lossless  quantization so when I talk about  lossless your performance degradation is  not much of course there can be a bit of  performance degradation or there will be  reduction in the performance but that  might be myth nobody has you know has  done a really a research on big sample  that that really happens once you  quantize the llm but of course there can  be a bit of performance degradation but  that's why we calling it bits and  bites and that provides you a lossless  quantization of language models and  applies it to the Lowa procedure because  QA is based on luras by the way now this  results in a massive reductions in  memory  requirement enabling the training of  models as large as like Llama  270b Or you can do that on you know for  example  2x 2x RTX  3090 you can do that you can also uh and  and by the way if you compare let me  just uh write it okay let me just do it  if you compare without Kora this will  take around a00 which is one of the most  used GPU to train llms or create llms  you need which is by the way 800 is 80GB  80GB  GPU now you you need 16 x you need 16  different a00 gpus not one GPU node that  you need you need 16 okay so just  imagine how much it reduce if you use  Kora you know it boils down to 2 RTX 390  and of course if you have 1 a00 much  better okay to do that uh but just  imagine uh and the cost goes down once  you use loraa and  Q okay now let's see the question comes  in that okay we have been talking about  all of this thing how can we achieve our  task how can we you know F tune what  kind of machine we have to use and all  of the question that you will have in  your mind so for that I will tell you  that let's go into fine tuning so let me  just write over  here fine  tuning we are now going into uh fine  tuning  step uh in this tutorial that I'm  creating you know we'll be focusing of  you know 6B or 7B llms not more than  that because I want to keep it as a  tutorial you can explore yourself if you  have compute and all I do have compute  but you should learn and you should of  course do it yourself right now the  first thing that you think will come in  your mind is training  compute that tell me about training  compute and that's what I also you know  get worried okay training comput now for  training of course you need a GPU to do  that we'll talk about model and data set  separately that's something that we'll  have  something uh now if you want to F tune  uh model like Lama 27b or Mistral 7B let  me just write it over here now these are  the model I'm writing because this  models are commercially available to use  with an Enterprise you can make money  you know using these models guys because  you'll be building a product so don't  see that just you'll be making money by  using this model but once you build a  product on top of this now Lama to 7B  and mral 7B if you want to F tune on a  very uh on a very high level I'll say  these are the requirement that you need  the alha talk about  memory and this is based on Uther Uther  AI Transformers math 101 blog post I  will give the link in description if you  want to understand how calcul  calculations work you know to decide a  compute you should look at Transformer  math 101 blog post find the link in  description now now the memory  requirement is around let's keep it like  150 to  195  GB this would be the memory requirement  you know to F  tune okay  now so renting gpus now the next thing  is how to rent  gpus or how to use a GPU there are many  options that you can  consider uh one is uh the most I will  it's not in any order but I will write  in my Preferred Choice okay uh in an  order I rely on  runpod runp pod. I rely on runp pod then  there is V  AI then there is Lambda  labs and then then you have hyperscalers  like big players you know I will just  only write AWS say maker because I have  worked with it a lot so these are the  top and you have your Google collab also  how can I forget collab our life saer  right now run pod is my Preferred Choice  hands down run pod is my preferred  choice I will recommend you to choose  runp or between Lambda Labs but Lambda  Labs service support and all that are  very bad okay so runp pod is something  that I recommend it's extremely easy to  work with and also affordable if you're  looking for cheapest option that you can  afford which is V so let me call it  here cheapest I'm writing cheapest I'm  not writing affordable affordable is  like runp now here security can be a  concern so look at if you looking at  secure cloud or Community Cloud kind of  a thing I recommend runp that's I said  okay now if you have a super computer  you can also do with that but I do not  have  now the next thing is uh once you have  training compute decided once you have a  training compute then you have to gather  data  set I show you that how you can get a  data set if you do not have one the main  source for learning purpose is hugging  face data sets once you have the data  set and I'll cover a few things in data  set because you know once if you have  you are creating your own data set you  have to look at something called Uh  there's few things you have to look at  one is diversity in  data you do not want your models to only  do one very specific task you know you  should have a bit of diversity now  assumed a use case like you know you're  training a chat  model that does not mean that data would  only be about one specific type of chat  right you you will want to diversify the  chat examples you know the samples uh  like different scenarios so model can  learn how to generate outputs of various  type of input now imagine one of your  Ender comes in and start putting about  some question that your model have not  even seen you know so at Le those kind  of things so it it understands the  semantics better now the size of data  set on a very uh like a thumb Ru your  file size should be 10 MB that's how I  decide if you are using a CSV with  question answer payers your file size  should be 10 MB that's how I go with  okay 10 MB of your data okay now or at  least I'll recommend 10,000 question  answer  pair and the quality of data the this is  important because if you look at the  latest model and this is the most  important by the way this is the most  important thing F example F llm by  Microsoft or Orca for example these are  the examples where organizations have  shown that how a better quality data can  help you you know train an llm on a very  smaller size like 3B or 1.5b kind of a  parameter of parms this is important to  understand now we'll not talk about data  at this moment let's let's see that so  to use runp pod what we have to do is to  go to runp  pod uh dashboard here and you can see I  already have a template has been  deployed but it's very easy to do that  you have to come to secure Cloud you can  also look for Community Cloud but I  prefer this it's Community cloud is bit  cheaper because your GPU can be also  availability is an issue on community  Cloud because the same GPU can be rented  by somebody else's as well if you are  not using that now that's on the  community Cloud uh on secure Cloud they  have two things latest generation and  previous generation on latest generation  you will see the latest of gpus like h00  which gives you 80GB vram with 250 GB  Ram you know which which is the most  expensive one over here then you also  have previous generation with a00 then  that's what we're going to use okay so  a00 you can see almost $2 per hour so  one hour training you cost $2 so on for  example if you have decent enough data  set like 2,000 3,000 rows $5 you'll be  able to uh do it now uh for that let me  first show you a couple of things we  have eight eight maximum that you can uh  you know spin up over here the eight  different node of that particular 800  GPU but this is not what we want to do  okay uh so let's see how we can deploy  it uh uh how we can find unit so I  already have deployed what you have to  do you have to click on deploy and it  will deploy it will take a few minutes  to deploy it and then there will be  option to connect once you click on  connect it will show open Jupiter lab  and that's what I did over here I have a  Jupiter lab now the first thing that we  have to do is we have to get clone the  Excel toll so let's get clone so get  clone uh and then we'll take that from  here so come over here let me just click  on  this this this and then you clone you'll  see a folder here now let's let's go  inside the folder in the examples you  will see different models which are  supported you can also see the mial are  also supported very soon in this okay if  you click on mial you will see mial yaml  so mial is also there 8X 7B model by  Mistral the mixture of experts now if  you go back to examples you will see Cod  llama Lama 2 Mamba Mistral MPT pythia  red Pama and some other LMS are also  there  let's open for example open Lama so if  you open open Lama you will see config  yaml now in config yaml you will see all  these details I will explain each and  everything you know in a bit what what  do we mean by loading 8bit loading 4bit  what is Lura R which is not there  because that will be in your QA or Lura  if you click on Lura yal this is a  configuration file which helps you you  know Excel toll basically takes this  single file as an input and F tune your  model here you have to Define your data  set if you look at the data set it says  gp4 llm clean there is nothing but this  is available on your hugging face  repository so basically T the data from  there but you can also do it from  locally as well so if you have local uh  uh machine where you are running this  and you have your data locally you can  also uh uh assign the path from that  local machine as well it has your  validation adapter blah blah blah and  we'll explain that everything uh in a  bit what do we mean by qware bits and  bite things will come in picture you can  see load in 4bit will be true for Q  because we want to load this model in  4bit and something like that right let's  do that so what I'm going to do is uh  first thing let me go back to my folder  here and you will have a requirements  txt let's expand that now you have your  requirements txt now in requirement txt  we'll find out everything okay what are  the requirements thing that you know you  have extra and Cuda you are getting from  here and lot of other things that you  are getting now uh we'll make some  hashes we don't need this or rather what  we can do we can just remove this from  here we do not need this anymore because  we already have Cuda with us over here  now Cuda is not required any other cudas  that you see I do not see any other  cudas over here I'm just looking for  torch Auto packaging pip Transformers  tokenizers bits and bytes accelerate  deep speed you know uh add  fire  pyl data  sets flash attention you can see 2.3.3  sentence piece wend B ws and biases eops  X forers Optimum HF Transformers Kama  Numba Bird score for evaluation evaluate  R rou score for Val evaluation as well  you know we have psychic learn art FS  chat graduate and fine let's save this  here here okay now once you save it I'll  just close this now let's go CD inside  it have to CD so let's do CD so CD XEL  to and you can see ax AO I am always you  know typing it wrong but I'll just CD  inside it and let's do a PWD to find out  what is my present working directory you  can see it Exel tall uh now the next  thing that we have to do is okay we have  to look at the inst installing the  packages let's do what they suggest I'm  going to do pip install  packaging the PIP install packaging will  is a requirement already satisfied the  next thing is PIP  install and then hyphen e which  basically says okay within this  directory or we can you can also take it  from you know their uh I'm saying okay  flash attention and deep speed so let me  just do not need that okay deep speed  okay flash attention and deep  speed uh to let's run  this what what are the thing that I am  making a mistake  uh deep speed flash attention H it  should not be dot here my bad it should  be dot should be the outside of  here okay you have your flash attention  and deep speed thing going on okay over  there it will install everything which  is in your requirements txt you can see  Bird score Numba sentence piece which  the dependency of Transformer helps with  you know vocabulary addition tokenizer  blah blah blah know pep has been  installed bits and bytes accelerate  that's looking good uh let's see it out  over there come  down flash attention make sure you have  flash attention greater than 2.0 flat  flash attention one point will give you  error okay uh let's come  down it's installing it once you install  that you have you can also go to the  data set path if you don't want to use  this data you can also do that with  other data as well but I will keep the  same  data but we can make few changes if you  want let's know because we are relying  on Laura here we'll use Lura so if you  come to Lura okay let me come to Laura  low rank adaptation now once you come to  Laura you can do a bit of changes like  you know let me see what is the  EPO okay uh where is that uh number of  epoch this is too much I don't want four  Epoch okay let's do it for one Epoch  okay so for we'll do it for one  Epoch okay the number of epo become your  number epox become one micro batch size  I will also make it one and gradient  accumulation steps I'll make this eight  okay I will explain that don't worry if  you don't understand these terms we'll  explain each and every this  terminologist which has been written  over here now I made some  changes uh and then I'll go back back  and then I will execute my learnings  okay that's what I'm going to do okay  now let me just do a  save okay uh and if you come you can  also see it over here this how you can  train it out you can see it says uses  this how you can train this is how you  can  inference all right so execution of  learning will happen like that so let's  come to fine tuning here okay now we are  done we'll add few  sales and once you have done all of  these changes let's go back and maybe  you can copy this thing uh quickly let's  copy  this let me come over  here and I have to add  that you can see excelerate launch  hyphen M Exel to CLI train examples it  has open  Llama now for example if currently you  see it says open Lama now you have to go  to open Lama 3B I don't know which one I  changed up okay uh if you go to  Laura yeah uh yeah this is what I also  changed okay fine now let me just uh  close this one low yl and let's run  it now once you run it you will see a  few warnings you can ignore those  warnings don't not have to worry about  those  warnings  you can see it's currently downloading  the data from hugging face it will you  know uh create a train and validation  splits you can see it's currently doing  it will also map the to with the  tokenizers will also do that you can see  it's happening happening over  here you have to wait for all these  process basically what they do guys  right it's imagine this as a low code  fine tuning tool if you don't want to  write a lot of code you just want to  make some changes because they already  have created the yaml for you okay this  is what they have done you can see total  number of steps total number of tokens  over  here find out the total number of tokens  currently getting the model file which  is PCH model. bin the file that it's  getting you can see it's around 6.85 GB  you know runport is basically will have  some cash of course definitely they will  have some cash and contain  cast in the container because you know  you are just fetching it from hugging  fish directly that's why it's bit faster  if you do it of course locally it will  probably be not that FAS okay uh it will  take a bit of time let's see how much  time it takes I don't want to pause the  video because I want to show you each  and everything that goes if you want to  you can skip the video of course you can  see you know extra special tokens EOS  BOS paddings total number of tokens  supervised tokens all those details over  here which have been listed okay that  you can find it out  once the model has been downloaded which  is your pytorch model. bin the next  thing that we have to do it will also  load the model okay so basically once it  load the model in the in that uh once it  loads the model you Cuda kernel should  be there because once it's loading it  needs GPU to load that okay on on a  device that's when we do dot two device  or something to you know pipe that with  CA  okay you can see it says starting  trainer so it has a trainer argument I  will explain that don't worry if you do  not know about trainers and all we'll  explain each and everything in detail  now guys what will happen you see can it  has started training okay you can see  efficiency estimate total number of  tokens per device it's training your on  your data now what I'm going to do is  I'm going to pause the video till it  train or F Tunes it and then we'll look  after that all right uh so you can see  uh it has been fine tuned the model that  we wanted to fine tune we have fine  tuned the llm and you can find out uh  train samples per second train runtime  train steps per second train loss EPO it  has been completed 100% it took more  than 1 hour it took around 2 hours you  know it took 1 hour 55 minutes you can  see it over here uh let's go back here  in the  workspace Exel  all and you can find out here you have a  folder called Laura out now in Lura out  you have your last checkpoint and you  can keep on saving checkpoints we'll  cover that maybe in the upcoming videos  where we will have a lot of videos on  fine tuning now but what I'm trying to  say is that you can also save it on  multiple checkpoints for example if you  want to use a th checkpoints with you  can also do that but you can see here we  have our adapter model. bin and adapter  config right now you can also merge that  with Pip but I'm not covering that part  right now here because you would have no  idea about pip if you a beginner so now  you just you just saw that how we find  tuna model and we got our checkpoint  weights over here you can find out this  is the last checkpoint if you want to  use this which has a config Json which  has if you click on the config Json it  will open a Json file for you then you  have your you know safe tensors okay the  model weights then you have Optimizer  dopy PT then you have your scheder you  have your Trend State you can file out  all the related information over here  you can also close this target module  seven items you can find out all the  different projections I will cover each  and everything in detail what do we mean  by this now it shows that uh sometimes  it gives you error uh if you look at  this gradi if you run this gradio  sometimes in runp it it gives you an  error also because let's try it out try  it out our luck here I just want to show  you that you can also do  that yeah gradio installation has issues  with in within runp pod sometimes when  you are using  it but anyway uh we'll move if it works  it will open a gradio uh for you a  gradio app gradio is a python library  that helps you build you know uh web  apps okay you can see it says running on  local over here okay now it runs on a  local URL okay you have to probably do a  true uh s equals to True something to  you know get it on a URL okay now you  can also click on let's click on this  now once you click on  this it says this share link expires in  72 hours for free permanent hosting and  GPU upgrades run gradate deploy okay now  you can see that we have an axel toall  gradio interface okay now here you can  try it out now if you let's copy this  and see what kind of response or even if  it's generating any response or not so  if you click on  this it will either give you an error or  it says okay you can see give three tips  for staying healthy and you can see it  out over here eat a healthy diet consume  plenty of fruits vegetables whole grains  lean proteins blah blah blah and you can  see the speed as well because we are on  800 right now that you know and it's  generating your responses pretty fast  right look at the tokens per second over  here now you have an interface that that  is working on your data your data that  you have from hugging face for example  we have used Alpa here Alpa 2K  test you can see it's how it says how  can we reduce air pollution let me ask a  similar question but not exactly the  same so what I'm going to ask here  is tell  me the ways I can  reduce pollution let me ask this  question and I'm asking tell me the ways  I can reduce pollution it says make a  conso and when you are working with  python you have to use a prom to look at  the uh basically to par the output a bit  but this is fantastic right now you can  see it  says make a conscious effort to walk  bike or car Pole to work is school or  irand rather than relying on a single  occupancy vehicle huge public  transportation car pooling blah blah  blah Reduce Reuse and recycle talking  about a bit of sustainability over here  now this is fantastic you saw in two  hours at least for one EPO of course  that is fully customizable as we saw in  the uh over there in the yaml files but  this is how easy it is nowadays to F  tune guys we'll now talk about we'll go  back to  our uh cast here and we'll talk about  each and everything in detail okay now  how did we find tune what are the sh  config that we have considered and all  of those things so let's start our  journey with all these parameters now  let's understand how Laura works and  what is Laura and how these parameters  are being decided that we have seen in  the yaml file as well so let me just  write about Laura here so we're going to  talk about Laura  now now it's a Training Method let's see  this as a Training Method let's keep it  lemon so you can understand better it's  a Training  Method so I'm writing it's a Training  Method to designed to ex designed to  expedite the training process of  llms training process of  llms this helps you with of course  memory consumption it reduces the memory  conj by introducing something called  pairs of rank decomposition weight  matrices this is important so what I'm  going to write here is look at this word  here okay something called rank  decomposition and basically that's a  pairs so I'm going to write pairs of  rank  decomposition  matrices  and this basically also known as right  we have a as we also talked about this  earlier called update  matrices  okay and you will have some existing  weights already of the pre-trained llms  that we're going to use so basically you  know it it helps you you know add these  weights okay training this basically the  new added weights  okay now  there are three things that you should  consider when we talking about loraa the  first is the or I'll say three or fours  let's let's understand that okay  preservation of pre-trained Weights so  let me just write it over  here so preservation  of pre-trained  Weights that's the first thing now what  we understand by this like Laura  basically maintains the Frozen state of  previously trained weights that  basically minimize the risk of  catastrophic  forgetting now this particular step  ensures that the model retains it  existing knowledge so let me just write  it over here okay  model  retains the its existing knowledge while  adopting to existing knowledge  while adapting to the new data that's  why we call it right that LM still has  the base knowledge and that's why  sometimes it hallucinates gives it from  the base knowledge right while adapting  to new  data now the second thing which is  important for you to understand the  portability of trained weights can you  port that okay so let me just write it  over here  portability  of trained  weights the rank decomposition matrices  that that gets used in loraa have  significantly fewer parameters of course  right that's why it helps you with the  memory reduction right when you compare  this with original model now this  particular characteristic allows the  trained loraa weights to be easily  transferred and utilized in other  context making them highly portable so  it's highly  portable now the third thing is the  third advantage that we're going to talk  about is integration with attention  layers integration with attention  layer now Lowa matrices are you know  Incorporated basically into the  attention layers of the original model  the adaptation scale parameters we we'll  see that parameter once we are looking  at the code allows control over the  extent to which model adjust to the new  training data and that's where we'll see  about you know alpha or the scales that  has been used now the next one is memory  efficiency that's the advantage that we  also talk about is memory  efficiency now it's improved memory  efficiency as we know opens up you know  the possibility of running fine tune  task unless then as we discussed earlier  3x the required compute for a native  fine tuning process say without Laura  let's look at the Lura hyper parameter  let's look at the code now to understand  uh the Lura hyper parameter so go to  examples of the same exal GitHub  repository let's open any we trained  basically open Lama right find now you  can also open Mistral Also let's open oh  let me open Mistral now to show  you so you can see I'm opening Mistral  and the mistal let's go to  Kora now on the cura I will first talk  about the Laura config let me make it  bigger so you can see it and let's talk  about this these are your Lura hyper  parameters that we're going to talk  about and I will explain that what we  mean by this you find tun any llms you  go to any videos any blog you will see  this code now you should have you should  have the understanding you should know  that what are these means okay so let me  just show you over s it over here that  what are these things mean now on the  memory efficiency  the next thing that we're going to talk  about is Lowa hyper  parameters so I'm just writing Lura  hyper  parameters now the first thing if you go  back you have something called Lura R  which is nothing but the Lowa rank okay  so let me just write first thing you  should know what is Lowa rank okay so  let me just write it like that okay  that's called basically your Lowa rank  low rank adaptation rank what do we mean  by lower rank gu this basically  determines the number of rank  decomposition  matrices rank decomposition is applied  to weight matrices in order to reduce  memory consumption and computational  requirements if you look at the original  Lura paper recommends a rank of R equals  to weight eight okay so by standard let  me just write here you know by the paper  they recommend standard r equal to 8 but  here you can see for mistal you know in  Axel to GitHub repository for mral is  it's has been written  32 and eight is the minimum amount keep  in mind that higher rank if you have  higher  rank leads to better  results better  results and there's one tradeoff now but  for that you need High compute  so now you should know that if you are  getting any errors like okay you have  you know taking a lot of computational  power you can play around this hyper  parameter which lower rank make it eight  from 32 make it 16 or or something like  that you know even know it's a trial and  error  experimentation now the more complex  your data set your rank should be higher  if you have a very complex data set  where you have lot of relationship a lot  of derived relations and so on and so  forth you need your rank to be on the  higher side okay okay now to match a you  know a a full fine tune you can set the  rank to equal to the models hidden size  this is however of course not  recommended because it's a massive waste  of resources now how can you find out  the models hidden size but of course you  have to go through the config Json by  the way okay or so there are two ways of  reading or basically finding out so  let's say is  reading hidden  size  now there are two way one is config  Json and the other is that you can do it  through  Transformers Auto  model Transformers Auto  model I I think I can quickly you know  write the code here so let me just write  the code for you now this is how you  have your auto model so from Transformer  let's quickly get  it so for example from Transformers  import you have Auto model as we as we  do it we do auto model for  Cal so you have your Cal LM that's how  you import you define a model name then  you'll have an  identifier so let's define that model  name whatever then you have your that  you basically get your model now through  Auto model something like that now how  do you find the hidden size this is the  code to get the hidden size  you do it something called Model do  config do hidden State that's how you do  it hidden  size and then just print the hidden  size This Is How We Do It print the  hidden size right let me open uh  here Lama  2 let's go to you know any of this okay  it's fine  you can see it over here hidden size is  here  8192  right this is your hidden size you can  find it like here also and you can also  find it you know through Transformers  Library as on but this is not  recommended because it's a waste of  resource otherwise why would you use  Lura then but this is what it is right  if you want to really achieve those  performances of pre-training uh like how  meta has created Lama 2 or M A has cre  Mistral the next is let's go back next  is Lura Alpha okay so let's write it  Alpha here this is a scaling Factor now  I'm writing Lowa  Alpha now it's a scaling factor for the  Lura  determines the extent to which the model  is adapted towards new TR the alpha  value adjust the contribution of the  update matrices during the training  process or during the train proc  process lower value gives more weight to  the original data so if you have lower  value it gives more weight to the  original and it maintains the models  existing knowledge to a greater extent  so it will it will be more inclined  towards the you know your base knowledge  the base data that you have so let me  just write it so for example you you  will able to uh make a note of it I will  write lower  value Gibs  more  weight to the original  data to the original data and  maintain maintain the  models existing  knowledge knowledge to a greater  extent let's try it like  this okay this is what now if you can  see here it says Lowa  Alpha and that Lowa Alpha is 16 you can  make eight also you can make 16 also  that depends now the next is let's talk  about Lowa Target modules which is again  very important you know talking about  the Lowa Target modules that you see  over there now Lowa Target modules is  one of the important thing so let me  just write  Lowa Target  module oh you cannot see it I just  noticed Lowa Target module now in the  Lowa Target  module here you can determine which  specific weights and matrices are to be  trained the most basic ones to train are  the query vectors that's basically  called Q projection okay you would have  seen that word q projection so the most  basic ones are the query ve query  vectors and value vectors so let me just  write it over here you know query  vectors and then you have value  vectors that's called Q projection Q Pro  and this called V  Pro these are all project projection  matrices the names of these matrices  will differ from model to model of of  course depends on which llm you are  using you can find out the exact names  again there's a what you have to do keep  the same code AS written above just make  one changes which is your layer  names so layer names we you know  basically it's a dictionary that's how  you'll get it let me so it write it over  here model. State  dict model do state dict and then do  keys this is how you will get the and  basically you need a for Loop now  because there will be n number of layers  so you can get the for name in layer  names and then you can print that so  you'll see Q projection you know you  will see K projection you will see V  projection you will see o projection you  will see gate projection you will see  down projection you will see up  projection layer Norm weight blah blah  blah right so there can be n number of  Weights now the naming convention is  very easy model name do layer layer  number component and then it goes like  that so this is what you should note the  Q projection the projection matri Matrix  apply to the query vectors in your  attention mechanism of the Transformer  blocks transforms the input hidden  states to the desired dimensions for  Effective query representation and V  projection is bit different it's called  a value vectors in the tension mechanism  transforms the input hidden states to  the desired dimension for Effective  value representation so these two are  very important Q Pro and V Pro there are  others like that like K Pro which is key  vectors then you have o Pro different so  you can keep on looking at you know  different basically oo are nothing but  the output to the attention  mechanism so you know so however three  or four you know there are outliers as  well we have to look at outliers they  don't follow the naming convention is  specified here that I'm writing but they  have embedding token weights embed  tokens normalization weights normalize  then you have LM heads these are also if  you go back by the way excuse me sorry  then you have your you know which is not  of course here here because this might  not be using it but you have LM head let  me just write it out over here you have  LM  head so you have LM head then you have  embed  tokens then you have embed tokens then  you have normalization Norm these are  also that we consider when we are  training it now the LM head is nothing  but the output layer of a language model  language modeling I will say rather it's  responsible for generating predictions  or scores for the next  token based on the Learned  representation from the preceding layers  the previous one you know basically they  are placed in the bottom so important to  Target if your data data set has custom  syntax LM head is very important let me  just write it over here if your  data if your data has custom syntax  which is you know really  complex embed token they represent the  parameters associated with the embedding  layers of the model is like very  self-explanatory usually placed at the  beginning of the model as it just has to  map input tokens to their Vector  representation you have your input  tokens you have to first convert it to a  vector then the model will be able to  understand right it it needs a numerical  representation it cannot understand your  text Data it's important to Target again  if you have your custom syntax so this  goes same now normalization is very like  know very I say  common it's a normalization layer within  your model used to improve the stability  and convergence so basically helps you  with the convergence of you know deep  neural  networks this is what we look at the  Lura guys then we look at the Q Laura  now in QA we talk  about few things we'll talk about if you  go back here let me just  explain QA bits and bytes will be on  somewhere here so basically just explain  so quantized Lura is an efficient fine  tuning approach you know even makes it  more uh memory reduction so it include  few things the number one is back  propagation so let me just write over  here back  propagation of  gradients through a through a 4bit  quantized you know through a frozen  let's write uh  through a frozen 4bit quantized 4bit  quantized which is very very important  quantized into  Laura that's the first thing the second  thing is it's basically uses of a new  data type called nf4 you would have seen  that  nf4 that's word now what do we mean by  nf4 that's called 4bit normal flat  excuse me not flat flat float 4bit  normal  float now 4bit normal float basically  you know optimally handles normally  distributed weights you will have your  distributed weights it basically helps  you optimize those it's a new data type  you see nf4 so if you make nf4 true you  have to use bits and bytes and all for  of course for that as well now you have  your  nf4 then you also have few other things  like paged optimizers double  quantization you know to reduce use the  average memory footprint even further by  quanti quantizing the quantization  constant now these are the things that  are associated with Laura and qora then  you have your hyper parameters like  batch size and aox now I'll explain that  you don't need the tab or this to  explain that because these are very  common so now you understand this part  this part is very much self-explanatory  a base model code of you know you use  any fine tuning code take it from medium  take it from other YouTube videos  anywhere you will find this code on  very similar 99% of the code will be  same only thing that you have to change  is your data and the way you tokenize  and map those that's it and few of the  times if you are using some other LMS  now your data set is this you have your  data set where you are you know storing  it data set  validations output  directory qora out then you have your  adapter which is qora then the sequence  length you can see it says  8192 P to sequence true Laura I I have  already explained this part here Wenda B  which is weights and biases okay weights  and biases for you know machine learning  experimentation uh tracking and  monitoring now we'll talk about uh a few  things which is important let me first  come down okay this is special tokens we  have covered it now let me just go up  and explain that to  you okay now what is number of epo guys  let's talk about about number of number  of epoch the number of  epoch excuse  me the number of AO is an hyper  parameter in gradient descent you would  have seen the maxima and Minima you  would have seen this that that hyper  parabolic or parabolic graph once we see  the way you know back propagation works  and the model learns and the when we  talk about neural networks hyper  parameters in gradient descent  which controls the number of complete  passes through the training data set  each Epoch involves processing the  entire data set once and the models  parameters are updated after every Epoch  now batch size is a hyper parameter in  again gradient descent that determines  the number of training samples processed  before it's in batches it stands in the  batches how many sample are used in each  iteration to calculate the error and  then adjust the model in your back  propagation steps we'll talk about  stochastic gradient descent it's an  optimiz algorithm as we discussed  earlier to find the best internal  parameters for a  model aiming to minimize performance  measures like logarithmic loss or a mean  square error msse and you can find it  out more on the on internet as well  about that yeah  so now batch gradient descent sastic  gradient descent and mini batch gradient  descent there are three different ways  commonly used you know for training the  models  effectively now  you would have sometime this question  the difference between the batch versus  Epoch so the batch size is the number of  samples processed before the model is  updated the number of epo is the number  of complete passes through the training  data set so these are two different NS  understand with a quick example I'll  just show it this is the last maybe that  I will  show but let me uh show that okay over  here now assume you have a data set so  you have a data set of 200  samples a rows or it can be question  answer pair of rows in a csb and you  choose a batch size of five so for  example batch size of five and AO is  th000 this will not we will not do it in  a fine tuning llm because the computer  to Too Much five now this means the data  set will divided into how many the data  set will be divided into 4 40 batches so  dat will be divided into 40  batches because you have 200 samples it  has to be in 40  batches 40 batches each with five  samples how many each with five samples  right now the model weights will be  updated after each batch of five samples  after each batch of five samples model  weights will update model weights will  get  updated now this will also mean that one  Epoch will involve 40 batches or 40  updates to the model how many one  aoch will involve 40  batches or 40 update this is very  important guys you should know the  fundamentals  to fine tune the best way right now with  1,000 aox the model will will be exposed  to the entire data set 1,000 times so  that is total of how many 40,000 batches  just imagine how much compute power do  you need to do that so the larger B size  results in higher GPU memory right  higher GPU  memory and that's where will be using  gradient accumulation steps you see this  here gradient accumulation steps that's  why it is  important right gradient uh that that  has been used to overcome this problem  okay over  there now you have learning rates you  can see  0.02 learning rate is not that like you  know so SGD stochastic gradient descent  know estimates the errors and the the  way it learns right so think of learning  rate as a no that control the size of  steps taken to improve the model if the  learning rate is too small the model may  take a long time to learn or get stuck  in a suboptimal solution it might might  get a local Minima right so on the other  hand if the learning rate is too large  the model May learn too quickly and end  up with unstable so you need to find the  optimal or the right learning rate right  you know it is important as well now the  learning rate what else do we have so  gradient accumulation is very important  so higher batch sizes results in higher  memory consumption  now that's where we bring gradient  accumulation Ms to fix this it's a  mechanism to split the batch of samples  so you will have your for example you  have a global badge let me just draw it  if I  can let me show it you cannot see if  here Global badge then you will have  your uh mini badge so let me call it MB  so you have  mb0 then you have mb1 mini batch one and  you can you can see it over here that's  called micro batch size two right it's  all associated with it mb1 then you have  mb2 and then you have mb3 now imagine if  you have this then you have grad zero  again this will be the inside only so  you have grad zero you have grad one you  have grad two gradient two basically and  then you have grad three and then you  have Global batch  gradient very very interesting right  Global batch gradients  now into basically it splits into  several mini batches of samples that  will be run sequentially right now I'm  not covering back propagation because I  hope that you know what is back  propagation if you don't I will  recommend to watch you know some of the  videos which are already available on uh  YouTube there will be n number of videos  for that but yeah I think that concludes  for I think this part guys so what we  did in this video you know now you'll be  able to to understand each and  everything over here now you know what  is gradient checkpointing and these are  like warm- up state after how many steps  you want it to warm up how after how  many EPO you want to evaluate the model  on your validation data whatever you  know your saves after how many EPO you  want to save the weights and all of  those things right so these are bit uh  again you can go and there can be others  uh parameters as well but I wanted to  cover as much as possible now what we  did in this video guys so far it's a big  video it's a lengthy video I know but I  will recommend you to watch this video  completely as I said in the beginning as  well we started with the understanding  of pre-training training llms we looked  at the three different approach let me  summarize it we looked at the three  different approaches pre-training fine  tuning and Laura and  Kora we covered all of this  theoretically till high level on here  and then we moved after training compute  to run pod and on the runp we set up  something called Excel toll you know  Excel toll is a low  code uh fine-tuning tool for you so when  I say low code you should understand how  it works but you know it's very very  very powerful it's really important to  use this kind of tool to help on you on  your uh productivity and it helps you  become more efficient once you are fine  tuning it so we looked at Excel toll to  fine tune a large language model on this  particular data which is Alpa data I  also shown that how where you can change  your data you can look at a jonl file  and then you can f tune right we F tune  it for 2 hours almost and we got a  gradio application that we saw you could  see that how easy it is to spin up a  gradio that's already there for you and  we tested out with couple of prompts and  then we are ending with all of the hyper  parameters that are important as are  required you should know it we explain  that I hope you understood you got some  clarity now and you will have enough  understanding to now fine tune an llm  this ends our experiment guys in this  video that's all for this video guys I  hope you now have the enough  understanding of how to fine-tune a  large language model and how to select  the optimal hyperparameters for the  fine-tuning task and how can you use  tool like Excel toall that's the low  code uh tool for fine-tuning llms this  was the agenda of the video as well to  give you enough understanding  and where I wanted to cover the  fundamentals of pre-training fine-tuning  and the novel techniques like Laura and  Cur if you have any thoughts or  feedbacks please let me know in the  comment box you can also reach out to me  through my social media channels please  find those details on the channel about  us on the channel  Banner that's all uh for this video  please like the video hit the like icon  and if you haven't subscribed the  Channel please do subscribe the channel  share the video and Channel with your  friends and appear thank you so much for  watching see you in the  next\", metadata={'source': 'mrKuDK9dGlg'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\envs\\geminienv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.vectorstores import Chroma  # chroma for storing vector store locally\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings # for converting text to embedings\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "load_dotenv()\n",
    "api =os.getenv(\"GOOGLE_API_KEY\") #uncomment this during locally\n",
    "genai.configure(api_key=api) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(url:str):\n",
    "     \"\"\"\n",
    "     Try to get the transcription of the yt video\n",
    "     input: url\n",
    "     output: yt_transcription\n",
    "     \"\"\"\n",
    "     try:\n",
    "        loader = YoutubeLoader.from_youtube_url(\n",
    "            url, add_video_info=False,\n",
    "        language=[\"en\", \"hi\"],\n",
    "        translation=\"en\",\n",
    "        )\n",
    "        doc = loader.load()\n",
    "        return doc\n",
    "     except:\n",
    "        return \"not able to get transcript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_transcript(video_id):\n",
    "    try:\n",
    "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "        for transcript in transcript_list:\n",
    "            transcript_text =transcript.translate('en').fetch()\n",
    "        return transcript_text\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Hi Guys my name is Nitesh and you welcome to mine its name is GRU Gated Recurrent Unit so in this video we will understand in great detail first of all if LSTM was performing so well then there is a need to use GR.  Why and after that we will understand its architecture in great detail. To explain the architecture, I will explain with examples and finally we will see the key differences between LSTM and GR usage. Okay, the video may be a little long because it has been explained in great detail. Yes but I am pretty sure if you watch the whole video GR you will understand so let's start the video so guy let's start studying GR usage so in very simple words if I tell you GR usage is nothing but an RN Architecture  Ok like there was simple RNN which we read first then there was LSTM which we read last similarly GR use are nothing but RN architecture which we use to process sequential data ok once let us discuss that GR use exists  Why do we do it? So if you remember, when we read Simple RNN, I told you that although simple RNNs are good and can do their job, but there is a huge flock in Simple RNN that as soon as your sequence  What happens is that it becomes very large, if there is a very long sentence from Let's, then because of the vanishing gradient problem and the exploding gradient problem, your simple RNN is not able to retain long term context. Right, so this was the biggest problem that we have.  Last read in LSTM I told you that it is very complex we maintain two different contact long term memory and short term memory and here we are able to solve that vanishing gradient and exploding gradient problem so that is why you use LSTM so here  But there is a very valid question that if the problem is solved with LSTM then why does GRU exist, then I will give you the answer, first of all a little history, LSTM came in around 97, okay and GRU is quite recent in 2014.  Yes, the biggest reason for the arrival of RU was that if you talk about LSTM, its architecture is a bit complex and that too is too much. Now if the number of parameters is more in a neural network architecture then it is an obvious thing.  That is, the time required for training will be more, especially on large data sets, so this was a big floss. If you want, you can go and test it yourself. You take a big data set and try to train LSTM on it. Do and you would see because of a lot of parameters evolved your training time will be a lot so solves this very problem Show me this diagram and this diagram and you will understand that it looks a little simple in comparison to the Atlas LSTM, okay in fact here in LSTM there are three gates if you remember here there are only two gates and because of this  Simpler Architecture You have less number of parameters and if there are less number of parameters then the time taken in training will be less but the good part is that even two we have a little simpler architecture we have less number of gates in fact here we have  There are not two separate memories, we have only one memory, here it is like there are two, so despite all these differences, the performance of GRU is comparable to STM, in fact, it has been proven that there are certain data sets, where there are certain problems.  GR also outperforms LSTM but it is not so everywhere. There are some places where LSTM performs better, so it is not that GR is like better in every sense. GR just provides you one quality.  And that is that its architecture is simple, number of parameters is less, training time is less, when you go to solve the performance comparator problem, you will have to run both of them and see which one is giving better performance. There is a chance of  Whether LSTM performs better or there is a chance that RU performs better, whatever it is, from this entire discussion it is understood that GR is something which we have to study because it can be a tool which  Further we can use it, so now in this video I will teach you three more things, first of all we will discuss the architecture of GR use, the whole thing which looks like a circuit, we will discuss what is going on inside this box. After that I will try to explain to you through an example how GR use works exactly. Architecture is understood but by taking an example there will be more clarity and when we read both these things then in the last we will do a Detail Comparison Between LSTM and GR Usage So in this video we are going to cover this so before discussing the architecture let us once discuss what is the big idea behind GR OK like if you talk about LSTM  So in LSTM we read that the big idea was that we wanted to keep short term context and long term context separate and that is why if you notice, in the architecture of LSTM you have separate cell states and you have hidden  The state is different and if you want, you can extend this cell state from any time stamp to a much further time stamp. Okay, so this was the big idea in LSTM and there are three gates by the way so there was forget. Gate there was input gate and there was output gate and the function of these gates was to control the flow of information. It is okay because their value is between zero to one, so we had read all this, now the question comes that in GRU.  What is the big idea, the big idea of \\u200b\\u200bGRU is that first of all GRU says that I do not need two separate states to carry long term contact and short term context, so that is why you will notice.  There is no concept of cell states in RU, here there is simply hidden state and G RU is designed in such a way that it carries long term context and short term context only on a single state. How will we read that? Also, here you do not have three gates, you have only two gates, one is named Reset Gate and the other is named Update Gate, so this Reset Gate, this Update Gate, how does it manipulate this hidden state? And how the information reaches from one time stamp to another, this is what we have to understand. In this video, I just wanted to give you this big picture that what new is GRU bringing, so simply put, you work with one state instead of two.  You are doing and manipulating the same to carry the long term context as well as the short term context, so now we will discuss it in detail when we will read its architecture, so guy, once before reading the architecture.  You will have to understand a little setup, with that your work becomes easier and before reading the setup, I would like to give you an advice, so if you have studied LSTM, then you will not find GR use difficult, meaning honestly in my experience.  LSTM is more difficult to read and understand in comparison to GRU but what GRU is is a bit confusing, it means that you will think that you have understood but when you go to use it or else  If you go to explain then you will suddenly feel that yes, some concepts may not be understood properly, so just be careful while learning GR use, I will try to explain to you very easily what is the meaning of everything here but  Just be careful, this is my advice. Now let me tell you what is the goal of this architecture of GR use, what do we want to achieve, so there is only one goal here, our goal is to stamp it for any time. You already know that GRU operates on our time stamp t e 1 t e 2. Every time you are sending a word, in every time stamp you give two inputs to this G RU. One is your previous hidden state and second.  Is the input xt of the current time stamp and the job of our GRU is to combine these two and calculate the hidden state for the current time stamp. So this is the goal. This is how it is being extracted from the input. This is what we have to understand. Okay. And in this process, we will read what is the meaning of all these reset gates or what is the meaning of update gate, okay but in the overview, this is our goal, how to calculate the output from the given input, now here you will get two Three things must be visible. Let me first clear you what all these things are. So first of all you must be seeing these terms: T minus and HT We have to understand, after that you will be seeing yellow boxes like this where it is the sign of Sigmod or the sign of H, so what is all this, we will have to understand once again, apart from this you will be seeing one more thing and  These are pink lines, not lines, circles, so cross is there, there is plus, there is there, there is this yard, one is the box with minus, what is this, we also have to understand, okay, so let me explain to you one by one, what are these three things, so the most  First of all, let us talk about all these terms AT MIVE AT EXT RT JT HT Bar or Tilt Sign, whatever it is, first of all I will tell you the names of all of them. A basically represents hidden state through which our information is progressing at different times. In the steps, HT becomes the previous hidden state, HT becomes the for time stamp, HT becomes the current hidden state, HT becomes the current input, our reset gate becomes JT, our update gate becomes and this HT is called candidate hidden state, so first of all  So you should know what all these are called. Now if I ask you simply what do you think mathematically speaking is all this then what will you answer? I tell you the answer to this question is that all these  All of the vectors are all of them are nothing but vectors means set of numbers. Okay now how many dimensions can there be? Here I have made a four dimensional vector but it can be of 40 dimensions, it can also be of 100 dimensions.  They can be of any number of dimensions. Okay, now here you have to remember one thing that if you leave xt, if you forget xt for a while, then these are all the vectors of ba ht my ht reset.  Get update gate and this candidate hidden gate, the dimensions of all these will always be the same, they will always be the same, okay, so if AT is a four dimension vector then it is automatically proving that HT will also be four dimension, RT will also be four dimension.  It will be Jati, it will also be L, and this one will also be four dimensional. You can't say that about Learned what are all these called, are all of them vectors and if you remove xt from these then what is the dimensionality of all the other vectors. Dimensionality means how many numbers are there in this box and what is always the same.  But it depends, we will discuss it after some time but I really hope you have understood the first thing about the setup, now we move on to the next thing, the next thing is these yellow boxes, what are these yellow boxes, so these yellow boxes are nothing but give R Neural Network Layers Okay, fully connected layers means you can actually represent this box like this, what are all these nodes which you studied in Artificial Neural Network, these are all nodes, now how many such nodes will there be, you will tell that is a hyper.  Parameter is So late you said that the number of nodes and hidden units is equal to f, so this means that there are five hidden units inside this box and the activation function in all of them is sigmod. This is the sign of sigmod.  Now there are two interesting things, first, if you tell the number of hidden units as five, then in all three of these boxes there will be five units. If you tell six here, then there will be six in all three of these boxes. If you tell here, 10.  So there will be 10 in these three, it will never happen that there are 10 units here, there are six units here and there are two units here, this cannot happen by design. If you said that there are 10 here, then here also you will get only 10.  And here also you have to keep only 10, so the first thing you have to remember is that all of these three are nothing but neural network layers with some activation function like here is sigmo id, here is sigmo id but here pe is not a. Okay but all these neural network layers are fully connected layers A and A second very interesting point is that the number of nodes in these layers will always be equal. Now one more interesting thing let me tell you and that interesting thing is that you are here.  As much as you specify the number of hidden units, you say let's say 'thr' here also three and here also three will be exactly the same dimension of all these vectors, this is ht myv is ht is rt is juti and this is the candidate hidden state in all these.  The dimensions will be exactly the same as you have mentioned here and this is a big plus point for now, later on you will understand logically why this is happening because I will show you the complete drawing but this is just to make things very clear for now.  Simplifies, so you should know this. Okay, so we understood this, we also understood this, now let's talk about it, all these are point wise operations like this, point wise multiplication, this, point wise addition, this, point.  Point wise or minus point wise means element wise, so you take values \\u200b\\u200blike here HT is coming from here and RT is going from here, now HT is min and AT are both vectors and the dimension of both is same, suppose AT is a.  BC is C and RT is D A so this sign is simply saying that if you do element wise multiplication then you will get a new vector where the first item will be AD, the second item will be B and the third item will be CA. Similarly if you do plus.  You get this A P S D B P E C P A Okay so these pink boxes are nothing but element wise operations Okay so the setup I explained to you there were three components to explain these vectors these neural network layers and these element wise operations these  Now if you are comfortable with this then trust me you are more than ready to understand how this HT is being calculated and in the process of understanding this we will understand this entire architecture in detail. One more thing, since I do n't want to teach you any ajman, then let's clear one more thing here before moving ahead that this is the XT which is my input on current time stamp T, why is it a vector, okay?  Although I had explained this question to you while teaching RNN, I had also explained it while teaching LSTM, let's do it again, no problem, so suppose we are working on a sentiment analysis problem where we have a text and its  Review is and let's say we have three texts so the first text is cat mat rat review one is second is cat rat rat third is mat rat mat this is our data now obviously this is our rn architecture g ru these words so  If we are not able to understand then what will we have to do, we will have to somehow convert these words into facts, this entire sentence will have to be vectorized into numbers, so here we use text vectorization techniques, there are multiple techniques, I have taught you NLP play.  In the list, you can use a simple technique like OE and bag of words and you can use a little advanced level techniques like aapka word to wake gaya or you can use any other form of embedding, so let me explain you in the examples of OE. How did the vector of How will you represent like this and how will you rip rot Going by this logic How will you write this sentence You will write this instead of cat Instead of rot this and this instead of rot Similarly how will you write the third sentence Instead of mat this rot  Change this and again change to Matt, this is my sentence one, this is my sentence two, this is my sentence three, now what I have to do is send one sentence at a time, sorry, one word, so when we  If we are processing this entire sentence, then we will get three time stamps, in the first time stamp, my XT will be this thing, then in the second time stamp, my XT will be this thing and then in the third time stamp, my XT will be this and this.  thing and this way my ave work will be finished then I enter at at the first time stamp this will be my first xt this will be my second xt and this will be my third xt and so on so this is how you convert an input into  A vector so this will be some vector, now it can be any type of vector depending on which text vectorization technique you are using, this is just for you to clarify, so I told you, so guy, we have done all the preparation now we  Are you ready to understand the architecture of GRU? So a little while ago I told you that the whole goal of GRU is that you are getting these two things: 'For Any Time Stamp' and from this you have to calculate this thing.  You are getting this, you are also getting this, you have to calculate this, this is the goal, but this whole work is actually divided into four steps, so we will first see those four steps once and then we will go through all the steps one by one in detail.  If you understand then the first step is what you do there, you calculate RT which we call reset gate, this is step one, it is okay to calculate this thing, step two is calculating this thing which we call candidate hidden state, after that we calculate. Jati which we call update gate and once we are done with these three steps the last step is to actually [music] calculate ht which you call current hidden state so if I try to explain it to you with a flow diagram then you will get ht  What is needed is that you get it from two things, one is AT -1 and the other is HT. Tilt sign is basically candidate cell state and to combine these two you use update gate. OK, HT is needed. HT will be formed by the combination of these two and that. Who decides the combination? Okay, now the HT tilt sign is the candidate hidden state. It is actually made up of two things, one is your HT MIVE and the other is your EXT but plays a very important role here. Your RT CH is your recent gate I.  M Sorry Write Reset Gate Sorry Reset Gate Okay so this is the flow diagram T is why I said first of all what are you calculating you are calculating this this is your step one is calculating this so that you use this in  Apply the combination of both and from the combination of these two, you calculate this in the second step. Okay, once this is calculated, then with the help of this, we need the combination of these two. So this is step three, apply this and finally when these three  Once the steps are done, then you go to step four and you calculate this, this is the flow diagram of GRU, the flow of chronological information will be like this, so what we will do now is we will see these four steps one by one and  We will also try to understand what is happening at the intuitive level, what is going on, okay, then you have to study Guy's Architecture, but if you remember, I told you a little while ago that the main goal is to use GR.  What it means is that for any time step t, you get the hidden state of the previous time step and get the input of the current time step and from this you have to calculate the hidden state of the current time step. So let's do one thing, let's do a conceptual discussion. Intuitively speaking, once we develop the understanding of what it is, mathematically, we know that it is a vector, it is a vector, meaning it is a set of numbers, but if you understand what this set of numbers represents. Neither the following video will be very easy, now to explain this whole thing, I am going to use the story which I had shared with you in LSTM again, so if you remember, while teaching LSTM, I gave you a very interesting lesson.  A story was told that there was a king named Vikram who was very strong and powerful. He had an enemy named Kali. There was a fight between them and Vikram died. Vikram had a son who was exactly like his father. He sought revenge from Kali. attacked but he also died; finally Vikram's son, who was Vikram's grandson, grew up and he also fought with Kali and he finally killed Kali and he took revenge for his father and his  I told you this story of Dada. Now what will I do? Using this story, I will try to explain to you the concept of HSN Hidden State and give you an idea of \\u200b\\u200bwhat these numbers represent. Okay, so if I give you a very simple one.  Let me answer the word, what is this hidden state, then you can actually call it memory of the system, it is nothing else, it is memory, basically when you are putting some words in this system time step by time, then you want that  This is GRU or for that matter which is LSTM, these people keep a memory, keep a contact that what is going on, right, let's take an example, suppose we have to put this entire story in this GRU and then we have to Ask, tell me whether the ending of this story is a happy ending or a sad ending. We are doing sentiment analysis, so let's assume for a while that we are putting one complete sentence here at a time, so we would like to.  Like suppose you are at this step right now, we should know some memory of the story till now, only then we will be able to process the next sentence and the next sentence and so on, so in a way this is the hidden state.  It actually works as a memory and in this memory you store contacts so in a way these are numbers because hidden state is basically a vector set of numbers so these numbers actually intuitively speaking they represent some form of content like I am telling you.  Let me give you an example. Let's assume for a moment that this h of ours is mine or this one is at. Both these vectors are of four dimensions.  It will always appear to you in the form of four numbers. Okay, now the question comes that what do these four numbers represent collectively? These four numbers tell the context of your story so far. Okay, now these four numbers individually each number represents something.  One aspect of the story is going on like we can assume that the first number is representing power, so whenever a king is being introduced and it is being said about him that he was very powerful.  If that was there then it is coming in the power aspect of that story. Second number lets representatives conflict, fight happened. This is conflict. Third number representatives tragedy means someone died. Fourth number representatives revenge. Someone took revenge. It is possible that this story might happen to someone.  According to this, if we are going with four numbers, then these four numbers can be that I am telling four aspects of the story, it is not necessary that by the way, I have given an example from myself, it is not necessary that it should be this.  There could be something else, actually you can't figure out what it is representing, it's just numbers, this is the big problem of deep learning, it is a black box, we don't know what is going on inside, but just to explain.  I gave you this example, so let's say if at some point at some point this is the point, then at this point we can think that we have only this connection of the story that it has been told about someone who  It is very powerful, as soon as this aspect increases, we will understand that till this point in the story, some kings and people have been introduced and there has been a fight between them. As soon as this aspect increases, we will understand that something went wrong, someone died and so on.  As soon as this b increases, we will understand that the revenge has been taken. Okay, so let me give you a rough intuition going forward sentence by sentence as to how our GRU tries to keep the memory. So let's say we have first time step.  In Let's Seti Equal, we fed this entire sentence through XT, so what is this sentence saying, there was a King Vikram very strong and very powerful, so there is a chance that now our HT can actually accept it in the beginning.  That is, in the very beginning, HT MIVE was complete GG, then this first sentence entered through XT, then what became AT, AT became these representatives that we have now in the story and it is just related to the power of power of a king. Description happened now what happened in the next time step we went to the next time step is E 2 so now this HT which was there became AT MIVE for the next time step okay and we gave the input from here where we said delay was  Enemy King Kali So here we described one more king so it became one from one and increased because here we have the contact of two kings, we have the concept of power of two kings and the conflate is our increased on the pouf.  Gone y because here enemy word is used tragedy is still zero revenge is still zero this is our second time step is over t e for 3 now this becomes our ht myve and we sent third sentence from ext both head  A Var and Kali killed Vikram. Now in our next step, the power decreased a little bit. Why did it decrease because Vikram died, so whatever power contact was related to him, we reduced it. Varus conflict increased a little more because a proper Var has happened. Tragedy has happened.  Also increased because Vikram died and Revenge is now zero and our teeth are also finished T F P Let's go T F P Vikram has a son Vikram Jr. He grew up and became very strong just like his father so again our power  The main thing has increased a bit, the conflate has reduced a bit, because maybe there is a period going on in between where there was no fight or tragedy, the tragedy has also reduced a bit, philosophy speaking has become an old thing, Vikram's death revenge is still zero, then  The time came, step five also attacked, Kali but got killed, so here the point became 8, the conflict point became 8, the tragedy increased because now in our mind or in the mind of this Ji Ru, there is a connection of the death of two kings, so the tragedy increased.  Revenge also increased because Vikram's son had come in return and attacked him. Let's go to the next time step, Vikram Junior to have a son called Vikram Super Junior and when he grew up he also burst Kali, it came back to one because one.  And the king has entered, the conflict is very high in Pow 9, the fight is happening for the third time, the tragedy is a little less and from Revenge Pow and finally in the last sentence it is written And he killed Kali and took revenge of his father and grandfather so here we are.  Points came to san because Kali is dead so power is now reduced a bit, conflict is also reduced a bit because war is over tragedy actually reduced because Kali is the villain so we are not feeling tragic on his death and Vengeance  Went on one and this is the final contact When you came out after reading the story, someone asked you, tell me brother, how did you like the story, then you will tell, there was a lot of masala in the story, there was power from P8, there was conflict because there were a lot of wars, there was path tragedy.  People died but there was a happy ending in the end and the revenge was brotherly so this is how the GRU and for that matter LSTM they are trying to maintain a contact through hidden state. Now our whole goal is what we have to understand completely.  This is how these chains are happening, how this came from this, how this came from this, how this came from this, how this is coming from this, these chains are happening, this is where your two gates have a lot of role, one of them is the reset gate and the second one.  One is the update gate, both of which are the main role of this transition and that is what we have to learn but I really hope that what we just discussed has given you a little bit of intuition as to how you can switch the context through the manipulation of these vectors.  Are you able to change it step by step? Okay then or let's move on to the architecture. Well, one thing and now I have explained to you by applying kind of our intelligence that how HT is formed from HT myv which is our previous memory or context.  That's right, we did this on the basis of our interpretation on the basis of our discussion. Once you also have to understand before entering into the architecture that how GRU does this work, so essentially GRU performs this task where it has to do with previous contact previous memory.  From the current context to the current memory, it does this work in two steps. So in the first step, what a g ru does is that it makes the thing that we know from it.  Let's say candidate memory or candidate hidden state and then it creates HT from this candidate hidden state, so this is step one and this is step two. Okay, now let me explain to you a little with an example how step one is executed.  And how is step two being executed? So for a moment, let us assume that we have started reading the story and somewhere in the middle, you can assume that we have read the part that Raja Hai Ek Vikram is very powerful.  He is very strong, he has an enemy, Kali and there was a fight between them and Kali killed Vikram. Suppose we have read the story till now, then the summary of HT minus and whatever is there will be in these three lines.  And I have told you that our summary is covering four aspects, first power, second third tragedy and fourth revenge. So let's accept it for a while, according to what we have understood of the story with our common sense.  Let's assume the power at this point is around Pf because first Vikram was introduced that he is very powerful but when he died then it came to our mind that he might not be that powerful. The conflict point is six but just a fight has happened, tragedy.  It is very high because just now Vikram has died. Revenge is now zero or very close to zero because no element of revenge has come yet, so this is our current HT mive, past memory till now is ok, what do we have to do now? That as soon as we are given a new sentence ch is the current input like in our case the next sentence is this one Vikram is the son of Vikram Junior who grew up and he also became very strong just like his father this is the input we are given On the basis of this current input, a new memory has to be formed which is a candidate memory, that is, this memory is the candidate to become HT. Okay, so we have to create an HT tilde which will become HT in the future. Okay, but it will become HT bar.  How with the help of XT and past memory till now Okay, so this sentence we just got where we are talking about a new king, so if you go to form memory on its basis, then the value of power will increase a little. Because again we are talking about a new king and praising him, the conflict will be reduced a bit because now when he is growing up, there is no fight at that time, the tragedy will also be reduced a bit because of Vikram.  It's been some time since he died and the aspect of revenge will increase a bit because in the story you are able to sense that Vikram's son will try something to take revenge. Right, this is your HT tilde like this is your candidate hidden state.  So this is our step one, this is step one where you took your old context old memory, took the current input and on the basis of that you formed a candidate memory which has the potential to become the next memory, but we directly created this candidate memory.  Ca n't make it. Just think about it, can we make it a HT directly? We can't make it because it is very heavily inclined towards the current input. Now it is also possible that B is not that important in the current input. Point of the overall story. From the view, it is HT bar or tilt, we have built memory according to the current context, but whether it is good overall, we do not know, so what you do is that you have HT-1 now. Summary of the story till now and you have HT tailed or bar which you achieved by changing it into HT human on the basis of current input. Now what you do is in the second step you balance these two, how is the balance done and we  Will read but you balance these two and go somewhere in between. Now if the current input is very important from the perspective of the story, then we will give it more weightage. If the current input is, let's say, equally important from the perspective of the story. So we will give more weightage to it so it remains dynamic but by balancing between these two the event final context is fine like if you do equal balancing between these two then there is a chance that the power will come somewhere around 65 K. Your conflict will come equal to F Your revenge will come equal to P Sorry tragedy will come equal to S And your revenge will come equal to Pat So this is your HT after the input of EXT This is your contest or memory so far about the overall  The power of the story has increased a bit, the conflict has decreased a bit, the tragedy has decreased a bit but the revenge has increased because there is a new king, so you actually do this transition in two steps, how to reach AT MI and HT, so now we have just  Understood that HT MIVE is the way to access HT from the past memory, which is the way to access the current memory, it is a two step process, first you are creating this candidate memory from HT MIVE and then using the candidate memory You are making HT, okay, but to make it clear, I took a few excerpts and explained to you that like we assumed that this is HT Myve and then based on the current input, I wrote that it is possible that our candidate memory  Ho but again, this calculation is also done by GRU itself, so a little bit, we will further break down this two step process, how actually it is becoming HT tilt from HT move and how it is becoming HT from HT, so actually.  This is where the concept of gates comes in. Okay, so this conversion is done with the help of a gate which we call the reset gate. You can see in our diagram that this gate's job is to get this conversion done and this conversion.  This happens because of our second gate which we call update gate and we call it Jati and you will see it here in this diagram. Okay, so essentially we understood that from HT MIVE you can update RT.  By taking help, you are making candidate memory and then by taking help from candidate memory, you are making current memory or current hidden state. Okay, so do one thing, before moving ahead, I am giving you a flow chart that shows the whole thing.  How it works Like the mathematical overview Okay, so first of all, what happens is you have ht myv which is your current memory and you have xt which is your current sentence or current paragraph is current word whatever.  You make RT by mixing these two. What is RT? Your reset gate is fine. After that what you do is you make this by mixing AT and HT. Okay by the way it is called model or reset.  Hidden State, what is all this, let me explain to you in a moment. As soon as you get this, you again take XT and mix these two and through this process you get candidate memory. Okay, after that what do you do? On the side again you take ht my 1 which is this thing and ekty and combine these two to make z t which is your update get and then as soon as you have this it comes you go back ht t  You take -1 and mix these three and you make HT. Like this is the mathematical flow chart of GRU so you can see easily here this is the first step where we are making HT tilt from AT MIVE.  And from HT tilt, finally this is the second step where we are making HT. Okay, now from here, who is helping you to reach this stage, who is helping you? And from here to reach here in the second step. Who is helping you? Okay, now I know, at this point you may have some questions curiosity in your mind that first of all, what is this RTÉ representative doing, what is this gender representative doing or is this model hidden state?  What is Xetra representing? So now we have to understand all this. Come on guys, now we are finally going to talk about the architecture and we will do it step by step, so the first thing that will happen in this entire architecture is the calculation of RT.  The reset gate will be calculated. Okay, before telling you how to calculate, I would like to tell you once that what is the reset gate, what is it? Mathematically speaking it is a vector and its shape or dimension will be exactly the same as that of h t -1. We have discussed this so h t -1 We had discussed that Power Conflict Tragedy and Revenge is made up of four things so similarly your RT will also contain four numbers Ok but at the same time RT is also a get get  This means that the four numbers inside RT will all be between zero to one. Well, if this number is zero then you should understand that RT is closed for that particular dimension. If it is one then it means RT.  Is open and fully open for that particular dimension, so RT actually works like a gate. Okay, so why is it called reset gate, let me explain it to you once, for that we will have to go to our story again, so in our story. Suppose we have read the first three sentences, Vikram has entered, Kali has entered and there is a war between them and Vikram is dead, now we have the context till now and the new sentence is  About Vikram Jr. how he is very strong like his father Okay so what the reset gate does exactly is it resets some of the dimensions of your h-1 based on the current input Okay once I go through the example  Let me explain like suppose at this point after these three sentences the value of AT is actually the value of h t -1 this is power this is 6 conflate is point from tragedies point from and revenge point and so now as soon as I got this new sentence that Vikram Junior Vikram  He is the son of and is as powerful and strong as his father, so actually what you are trying to do is that you are trying to reset some parts of the old memory, like, on the basis of this sentence, I am actually keeping a lot of this information. I would like, because here we are talking about the power of Vikram Junior, so we would like to retain a lot of this information, this is conflate since there is nothing related to conflate in our current statement, we would like to reset this part a little and lower it.  The next one would like to be a tragedy since no one is dying here, there is no sadness, so we would like to lower it a bit, would like to reset it and since maybe Vikram's son grows up and takes revenge, then a little bit of revenge is infusing into the story.  So we would like to make it a little higher or in other words we would like to retain most of its components so after this step it may be that your RT value looks like this so point 8 patv and pana what about this  What this means is that when I am saying that this is the first dimension P8 in RT, this means that I am trying to retain 80 of my assisting past memory because it seems important to me. What about this point two?  This means that in the memory of the assisting conflict, I want to retain it only at 20, the rest I want to delete because according to the current statement it seems that now the conflict is no longer there, I want to retain the tragedy also at only 10.  I want to because now that's an old thing but I want to retain Re at 90 even though it is low, it is low but I want to retain whatever is at 90 because I feel that revenge is going forward.  This moment is going to be an important aspect, so RT Atley is a gate which is in the form of a vector and its job is to reset things from your past memory based on the current input. Okay, now if you understand this.  Let me explain to you how to calculate, it is very simple so all you have to do is focus on this diagram, in fact I do one thing and at the same time I create my own diagram so that you do not focus on unnecessary things. Do so, first of all we have this is our context being passed here is AT minus and and these two are our HT okay and from here our current input is going so first of all what happens is that you take AT minus And you take xt and you take these two and put them into a neural network layer, a sigmod neural network layer, and this gives you RT. Okay, mathematically speaking xt is equal to sigmod of some weight. Let's call this weight double R.  Dot product the and xt2 -1 is a four dimension vector so we know that this neural network layer will actually have four nodes in it. Okay what we're doing is we're putting our four dimension h -1 in it and for a while  Let us assume that we are representing every sentence of our story in three numbers, so basically x is a three dimension vector, so I am representing it with a different color, okay and now this thing is a fully connected layer, okay so  Total here you have sen and here you have four so here you have this l aa that will be a 7 ks 4 28 weights and how much bias will there be four bases so these are your parameters right in this place  Now take a look at your [music] this is 7 crosses and here the weight is or say it like this, transpose is taken from 1 cross and here the weight is 7 crosses 4 so when this dot product is done then you will get 1 cross.  7 dot product 7 cross 4 These two are the same you will get no cross 4 In this you are adding bias Chch is 1 cross 4 This will give you a vector of one cross 4 and when you apply sigmod on top of that you will get again no cross four  You will get a vector of f is nothing but your RT. Okay, so now we have just opened up and seen how the RT is being calculated, so this is the operation here where we are calculating this RT. This is nothing but a weighted. Some weights are over here. Some weights are over here and we're training the weights through back propagation and then we're putting it inside a sigmo id because we want the answer to be between zero and one because after all RT is a  Gate is ok, so that is how RT is created, what is gate, we understood what is its purpose, we understood and how it is calculated exactly, we tried that too, now we will move ahead and in the next step we will see RT and  Infact RT by combining HT and XT will create this candidate cell candidate hidden state. So let's move forward in the Guice architecture. We have calculated our reset gate and this is the current state of the architecture. Now in this flow diagram.  But if you go, we have reached here, okay, now what we have to do is that we have to calculate this quantity by combining this reset gate and the past memory till now and what is this quantity, at all this is nothing but the model memory or  Then reset memory, then you do not have to do anything, from here you will bring HT, this is AT and from here you will mix AT in it and here you will do a small mathematical operation which is called point wise multiplication, element wise multiplication, so basically  You multiply this vector of yours and this vector of yours element wise and by doing this the quantity you get is nothing. If you are multiplying then this word becomes 48. We have retained it at 80.  of this is pot 12 this is actually going to be 07 sorry and this 09 So here's guys our model past memory based on my current input Model created reseteer, this was your power, in power we had a little reseteer in conflict, in this we reset a lot based on the current input, next was tragedy, in this also we reset a lot and last was our revenge, in this we reset very less so basically we have  There is a model past memory, now what will we do, we will take it with my current input, basically I am saying that we will take this product forward and merge it with XT, okay, we have merged it with XT and taken it forward. Will go and put it in a neural network layer this time this layer is a layer and the output I will get from this is my candidate hidden state so basically mathematically speaking the operation we are doing here is this this is a of this  Let's consider the whole weight. Doublets call it H for rar p for candidate's weight and you are concatenating two things here, one you concatenate this and you concolor with this. Okay if you talk about this neural  There are also four nodes in the network layer, okay here you have these four inputs which are basically representing this and then here you have three more inputs and this is a fully connected layer. This is a fully connected layer and this is  I am calling it DB, okay and how much will this be? 7 cross will be 4 and the bases here I am calling BC okay. Again you can verify the shape by finding the dot product of the dimensions of these weights. But or that's how you're going to get your candidate hidden state or candidate memory okay so once you have this you did the dot product with ext again and got the nach out so now you got this okay now maybe based on According to our data so far, it is possible that in the current input we had talked about Raja Naya Raja, then it is possible that it may have gone from Pot S, it may have come to point two, it may have come to point one.  And this has gone to point two, Something like that could happen, okay, so this is our proposal, we are kind of proposing that this should be the HT but we do not consider it to be directly HT, okay, I told you, you  You cannot consider it as HT directly. If you consider it as HT then in this way you are relying too much on the current input which is not necessarily very important from the point of view of the overall story. So now we have to work in just two steps.  Well, we have to take this quantity and we have to take this quantity and we have to play a balancing game between these two using quantity which we will calculate next so guys now we will move ahead and we have reached here now we will collect Will do our update gate f is this thing and to calculate the update gate you again need two things, your past memory till this moment and your current input and by mixing these two you will get Jati. If you talk about architecture point of From the view, you go ahead from this place, in this place you have both the things, there is XT, there is also HT, there is also MIV, you just take it further and put it in a neural network layer which will be SIGMOD and from its output you will get JTI.  Mathematically speaking JT is nothing but sigmod of dd plus bj OK again this will be the neural network layer it will have four nodes OK and from here you will get AT MIV and from here you will get EXT here which will be your fully connected layer.  That will again be a cross four and here you will have four bases. Okay, you can again do the shape matching, dimension matching, you can try. The bigger question is whether the shape is finally a vector exactly the same size as the shape. AT is my or HT is the candidate one and at the same time this is also a get get means the four numbers inside it will be between zero to one. Now let me tell you what is the purpose of Jati so we have this candidate.  There is hidden state but we cannot make it directly HT because it is heavily skewed. On top of the current input, we have to give value to our past memory as well as past context but we do not know how much value to give. Now this can happen, isn't it yours?  Which is the current input, it is very important like really important, meaning this is such a line that after reading it, the story that was in your reader's mind till now gets completely reversed. Imagine that you were reading a murder mystery, till now someone else was reading it.  You were feeling that suddenly after reading the last line of the murder you came to know that no, someone else had committed the murder and because of that the entire contact story in your mind got completely changed. They say, times change.  c Emotions have changed If there was that kind of input then in that case you will obviously have to give more input to the candidate who is heavily relying on the current one and you will give less value to the old one but it may happen that the current one is singing in your story.  This is happening in Bollywood movies, then it is not so important, then the candidate hidden state created by it is also not that important, so in that case, you have to value it more, then basically the work of Jati is that  When you train it on the data, it automatically extracts and gives you a set of four numbers. The athlete in this case already knows how to draw the balance between HT MIVE and HT Tilde and  This happens in the training process after applying back propagation so basically Jati if you have then now you are ready for the final step now you know how to balance the previous memory which is AT -1 and the candidate hidden state which is AT. Tilde and then by combining these two, you will finally make HT. So guys, we have the candidate cell state, we have the previous memory. Let's assume for a moment that we have a Jathi also calculated and the Jathi is let's say p v 782.  Suppose this is okay after training, then what you have to do now is to calculate HT by mixing these three quantities and the formula of HT is a very simple mathematical operation, you will understand it here also. K so basically what you do is just one second so basically what you do is you calculate n minus jti and multiply it element wise from at my and you take jti and multiply it element wise candidate one With and between these two, you do element wise addition. Okay, this is the operation to calculate HT, meaning here if you see this was your Jati, then you are running an operation on this Jati here on minus. And by taking it and mixing it with AT-1, here you are fine, I am talking about this operation, then you are also taking out Jati from here and taking HT Tilt from here and here  But between these two, you are doing this operation and then this one coming from here and this one coming here, by combining these two, you are doing a plus operation i.e. this one and that is.  HOW YOU ARE GETTING HD Okay now the intuition is very simple, if the JT is high, if the JT is very high then you are overvaluing the candidate. If the JT is low then you are overvaluing the past memory, so think about it.  Look, if Jati is lower than let's point w then 1 min Jati how much will it be 9 So I am overvaluing h -1 and here z is t1 so I am undervaluing this so in a way here  Since I have started 1 mine and here I have done z, then I have created the balance on the basis of past memory and current memory, it is ok, I hope you understand this and that is how you get your AT like I  Can do a calculation in front of you so if we apply this then 1 - z will come out to be 97 I'm sorry 32 8 and I have to multiply this element wise with 6671 so this will come to you 54 188 point 1408  This is what came out, this is your quantity, okay, and then what I will do is multiply this and this, this will give me 07 14408 PO 04 if I am correct, okay, and now I will add these two, add this and this. So here I will get 6132 p 22 end pv Here is my final HT Now think for yourself, see on which stage we were, where we were giving the introduction of Vikram's son, and after giving the introduction, here is yours.  The power which increased a little bit was PS0, then it became PS1 because we talked about an additional king, after that there was CONFLET CONFLET, the first point was S, it came down to point 32, it got reduced because there is no fight going on right now.  There was a lot of tragedy, that too has reduced because it has been a long time since Vikram died and this was point one and it has become 122. This is revenge. The angle of revenge is increasing a bit because we feel that what happened to Vikram Son, he will take revenge, so this is the way we reached here from this HT MIVE, so once just to surprise, I am making the flow chart again. By holding the AT MIVE XT, you are calculating the reset gate, whose job is to find out. Which dimension has to be reset which is not useful according to the current input then by combining HT MIVE and RESET GATE gate you are creating reset past memory or model past memory using this operation and in this you just You are adding that you are getting your candidate hidden state. Now at the same time, what are you doing. You cannot directly make the candidate hidden state as the final state, so you are again calculating the Jati using h t -1 and acti. Chch is your  Update gate which finds out what should be the balance between h-1 and AT tilde and finally you are calculating AT by combining all three of AT and AT so that's the whole thing we discussed and this is  The Antius Structure Now if you look at this architecture with love then you should understand everything so guys before ending the video one more time we will discuss what are the main differences between LSTM and GR usage so here I have noted down some differences. The first difference is that you already know that the number of gates is more in LSTM, there is input gate, FOR gate and output gate. In comparison too. That GR uses only two gates, reset and update. Okay, we had discussed this well. If you talk further about memory units, then in LSTM you have cell state and hidden state also. In GR, only hidden.  State is ok this is the second difference parameter count is also a big point of difference in LSTM the parameter count is more and this is the formula by the way if you have an input whose size is D and the hidden size So this is the formula to calculate the total number of parameters that will be present in the LSTM cell where as in comparison to that look here that same formula contains three instead of four so the number of parameters is less this you copy once  You can check by taking pen. On the fourth point, if we move, the computational complexity gets more parameters. In comparison to GR use, where they are faster to compute especially on smaller data sets and when computational resources are limited. If you use more  If we talk about performance then LSTM in many tasks is especially more complex than GR. Okay, so if we have a slightly complex set, then LSTM has experimentally proven to give slightly better results. Can it outperform STM on certain tasks? We are talking about GR specifically.  When data is limited and tasks are similar they can also train faster due to fewer parameters So basically if you have a smaller data set a little less complexity The choice between LSTM and GRU ofen comes down to Empir Kal testing It is said that by running both Look depending on the data set and the task one might out perform the other how ever GR is used due to their simplicity of being the first choice when starting out try on GR if you want to improve the results try applying LSTM GR is not improving. Leave the use. Okay, so with that, we covered in great detail how gated recurrent units work. I really hope you liked this video. If you liked it, please like it. Very hard work.  It takes great effort to make such videos and if you have not subscribed to the channel please do subscribe, see you in the next video.\", metadata={'source': 'QQfZAoNGQmE'})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_from_get = get_transcript(\"https://www.youtube.com/watch?v=QQfZAoNGQmE&t=2178s\")\n",
    "trans_from_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents=trans_from_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hi Guys my name is Nitesh and you welcome to mine its name is GRU Gated Recurrent Unit so in this video we will understand in great detail first of all if LSTM was performing so well then there is a need to use GR.  Why and after that we will understand its architecture in great detail. To explain the architecture, I will explain with examples and finally we will see the key differences between LSTM and GR usage. Okay, the video may be a little long because it has been explained in great detail.', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"because it has been explained in great detail. Yes but I am pretty sure if you watch the whole video GR you will understand so let's start the video so guy let's start studying GR usage so in very simple words if I tell you GR usage is nothing but an RN Architecture  Ok like there was simple RNN which we read first then there was LSTM which we read last similarly GR use are nothing but RN architecture which we use to process sequential data ok once let us discuss that GR use exists  Why do we\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"once let us discuss that GR use exists  Why do we do it? So if you remember, when we read Simple RNN, I told you that although simple RNNs are good and can do their job, but there is a huge flock in Simple RNN that as soon as your sequence  What happens is that it becomes very large, if there is a very long sentence from Let's, then because of the vanishing gradient problem and the exploding gradient problem, your simple RNN is not able to retain long term context. Right, so this was the\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='retain long term context. Right, so this was the biggest problem that we have.  Last read in LSTM I told you that it is very complex we maintain two different contact long term memory and short term memory and here we are able to solve that vanishing gradient and exploding gradient problem so that is why you use LSTM so here  But there is a very valid question that if the problem is solved with LSTM then why does GRU exist, then I will give you the answer, first of all a little history, LSTM', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='the answer, first of all a little history, LSTM came in around 97, okay and GRU is quite recent in 2014.  Yes, the biggest reason for the arrival of RU was that if you talk about LSTM, its architecture is a bit complex and that too is too much. Now if the number of parameters is more in a neural network architecture then it is an obvious thing.  That is, the time required for training will be more, especially on large data sets, so this was a big floss. If you want, you can go and test it', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='a big floss. If you want, you can go and test it yourself. You take a big data set and try to train LSTM on it. Do and you would see because of a lot of parameters evolved your training time will be a lot so solves this very problem Show me this diagram and this diagram and you will understand that it looks a little simple in comparison to the Atlas LSTM, okay in fact here in LSTM there are three gates if you remember here there are only two gates and because of this  Simpler Architecture You', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='and because of this  Simpler Architecture You have less number of parameters and if there are less number of parameters then the time taken in training will be less but the good part is that even two we have a little simpler architecture we have less number of gates in fact here we have  There are not two separate memories, we have only one memory, here it is like there are two, so despite all these differences, the performance of GRU is comparable to STM, in fact, it has been proven that there', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='to STM, in fact, it has been proven that there are certain data sets, where there are certain problems.  GR also outperforms LSTM but it is not so everywhere. There are some places where LSTM performs better, so it is not that GR is like better in every sense. GR just provides you one quality.  And that is that its architecture is simple, number of parameters is less, training time is less, when you go to solve the performance comparator problem, you will have to run both of them and see which', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='you will have to run both of them and see which one is giving better performance. There is a chance of  Whether LSTM performs better or there is a chance that RU performs better, whatever it is, from this entire discussion it is understood that GR is something which we have to study because it can be a tool which  Further we can use it, so now in this video I will teach you three more things, first of all we will discuss the architecture of GR use, the whole thing which looks like a circuit, we', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='the whole thing which looks like a circuit, we will discuss what is going on inside this box. After that I will try to explain to you through an example how GR use works exactly. Architecture is understood but by taking an example there will be more clarity and when we read both these things then in the last we will do a Detail Comparison Between LSTM and GR Usage So in this video we are going to cover this so before discussing the architecture let us once discuss what is the big idea behind GR', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='us once discuss what is the big idea behind GR OK like if you talk about LSTM  So in LSTM we read that the big idea was that we wanted to keep short term context and long term context separate and that is why if you notice, in the architecture of LSTM you have separate cell states and you have hidden  The state is different and if you want, you can extend this cell state from any time stamp to a much further time stamp. Okay, so this was the big idea in LSTM and there are three gates by the way', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='idea in LSTM and there are three gates by the way so there was forget. Gate there was input gate and there was output gate and the function of these gates was to control the flow of information. It is okay because their value is between zero to one, so we had read all this, now the question comes that in GRU.  What is the big idea, the big idea of \\u200b\\u200bGRU is that first of all GRU says that I do not need two separate states to carry long term contact and short term context, so that is why you will', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='and short term context, so that is why you will notice.  There is no concept of cell states in RU, here there is simply hidden state and G RU is designed in such a way that it carries long term context and short term context only on a single state. How will we read that? Also, here you do not have three gates, you have only two gates, one is named Reset Gate and the other is named Update Gate, so this Reset Gate, this Update Gate, how does it manipulate this hidden state? And how the', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='does it manipulate this hidden state? And how the information reaches from one time stamp to another, this is what we have to understand. In this video, I just wanted to give you this big picture that what new is GRU bringing, so simply put, you work with one state instead of two.  You are doing and manipulating the same to carry the long term context as well as the short term context, so now we will discuss it in detail when we will read its architecture, so guy, once before reading the', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='its architecture, so guy, once before reading the architecture.  You will have to understand a little setup, with that your work becomes easier and before reading the setup, I would like to give you an advice, so if you have studied LSTM, then you will not find GR use difficult, meaning honestly in my experience.  LSTM is more difficult to read and understand in comparison to GRU but what GRU is is a bit confusing, it means that you will think that you have understood but when you go to use it', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='you have understood but when you go to use it or else  If you go to explain then you will suddenly feel that yes, some concepts may not be understood properly, so just be careful while learning GR use, I will try to explain to you very easily what is the meaning of everything here but  Just be careful, this is my advice. Now let me tell you what is the goal of this architecture of GR use, what do we want to achieve, so there is only one goal here, our goal is to stamp it for any time. You', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='here, our goal is to stamp it for any time. You already know that GRU operates on our time stamp t e 1 t e 2. Every time you are sending a word, in every time stamp you give two inputs to this G RU. One is your previous hidden state and second.  Is the input xt of the current time stamp and the job of our GRU is to combine these two and calculate the hidden state for the current time stamp. So this is the goal. This is how it is being extracted from the input. This is what we have to', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='extracted from the input. This is what we have to understand. Okay. And in this process, we will read what is the meaning of all these reset gates or what is the meaning of update gate, okay but in the overview, this is our goal, how to calculate the output from the given input, now here you will get two Three things must be visible. Let me first clear you what all these things are. So first of all you must be seeing these terms: T minus and HT We have to understand, after that you will be', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='HT We have to understand, after that you will be seeing yellow boxes like this where it is the sign of Sigmod or the sign of H, so what is all this, we will have to understand once again, apart from this you will be seeing one more thing and  These are pink lines, not lines, circles, so cross is there, there is plus, there is there, there is this yard, one is the box with minus, what is this, we also have to understand, okay, so let me explain to you one by one, what are these three things, so', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='you one by one, what are these three things, so the most  First of all, let us talk about all these terms AT MIVE AT EXT RT JT HT Bar or Tilt Sign, whatever it is, first of all I will tell you the names of all of them. A basically represents hidden state through which our information is progressing at different times. In the steps, HT becomes the previous hidden state, HT becomes the for time stamp, HT becomes the current hidden state, HT becomes the current input, our reset gate becomes JT,', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='the current input, our reset gate becomes JT, our update gate becomes and this HT is called candidate hidden state, so first of all  So you should know what all these are called. Now if I ask you simply what do you think mathematically speaking is all this then what will you answer? I tell you the answer to this question is that all these  All of the vectors are all of them are nothing but vectors means set of numbers. Okay now how many dimensions can there be? Here I have made a four', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='dimensions can there be? Here I have made a four dimensional vector but it can be of 40 dimensions, it can also be of 100 dimensions.  They can be of any number of dimensions. Okay, now here you have to remember one thing that if you leave xt, if you forget xt for a while, then these are all the vectors of ba ht my ht reset.  Get update gate and this candidate hidden gate, the dimensions of all these will always be the same, they will always be the same, okay, so if AT is a four dimension', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"be the same, okay, so if AT is a four dimension vector then it is automatically proving that HT will also be four dimension, RT will also be four dimension.  It will be Jati, it will also be L, and this one will also be four dimensional. You can't say that about Learned what are all these called, are all of them vectors and if you remove xt from these then what is the dimensionality of all the other vectors. Dimensionality means how many numbers are there in this box and what is always the\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='are there in this box and what is always the same.  But it depends, we will discuss it after some time but I really hope you have understood the first thing about the setup, now we move on to the next thing, the next thing is these yellow boxes, what are these yellow boxes, so these yellow boxes are nothing but give R Neural Network Layers Okay, fully connected layers means you can actually represent this box like this, what are all these nodes which you studied in Artificial Neural Network,', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='which you studied in Artificial Neural Network, these are all nodes, now how many such nodes will there be, you will tell that is a hyper.  Parameter is So late you said that the number of nodes and hidden units is equal to f, so this means that there are five hidden units inside this box and the activation function in all of them is sigmod. This is the sign of sigmod.  Now there are two interesting things, first, if you tell the number of hidden units as five, then in all three of these boxes', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='units as five, then in all three of these boxes there will be five units. If you tell six here, then there will be six in all three of these boxes. If you tell here, 10.  So there will be 10 in these three, it will never happen that there are 10 units here, there are six units here and there are two units here, this cannot happen by design. If you said that there are 10 here, then here also you will get only 10.  And here also you have to keep only 10, so the first thing you have to remember is', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='10, so the first thing you have to remember is that all of these three are nothing but neural network layers with some activation function like here is sigmo id, here is sigmo id but here pe is not a. Okay but all these neural network layers are fully connected layers A and A second very interesting point is that the number of nodes in these layers will always be equal. Now one more interesting thing let me tell you and that interesting thing is that you are here.  As much as you specify the', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"is that you are here.  As much as you specify the number of hidden units, you say let's say 'thr' here also three and here also three will be exactly the same dimension of all these vectors, this is ht myv is ht is rt is juti and this is the candidate hidden state in all these.  The dimensions will be exactly the same as you have mentioned here and this is a big plus point for now, later on you will understand logically why this is happening because I will show you the complete drawing but this\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"I will show you the complete drawing but this is just to make things very clear for now.  Simplifies, so you should know this. Okay, so we understood this, we also understood this, now let's talk about it, all these are point wise operations like this, point wise multiplication, this, point wise addition, this, point.  Point wise or minus point wise means element wise, so you take values \\u200b\\u200blike here HT is coming from here and RT is going from here, now HT is min and AT are both vectors and the\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='now HT is min and AT are both vectors and the dimension of both is same, suppose AT is a.  BC is C and RT is D A so this sign is simply saying that if you do element wise multiplication then you will get a new vector where the first item will be AD, the second item will be B and the third item will be CA. Similarly if you do plus.  You get this A P S D B P E C P A Okay so these pink boxes are nothing but element wise operations Okay so the setup I explained to you there were three components to', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"I explained to you there were three components to explain these vectors these neural network layers and these element wise operations these  Now if you are comfortable with this then trust me you are more than ready to understand how this HT is being calculated and in the process of understanding this we will understand this entire architecture in detail. One more thing, since I do n't want to teach you any ajman, then let's clear one more thing here before moving ahead that this is the XT\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"here before moving ahead that this is the XT which is my input on current time stamp T, why is it a vector, okay?  Although I had explained this question to you while teaching RNN, I had also explained it while teaching LSTM, let's do it again, no problem, so suppose we are working on a sentiment analysis problem where we have a text and its  Review is and let's say we have three texts so the first text is cat mat rat review one is second is cat rat rat third is mat rat mat this is our data now\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='rat rat third is mat rat mat this is our data now obviously this is our rn architecture g ru these words so  If we are not able to understand then what will we have to do, we will have to somehow convert these words into facts, this entire sentence will have to be vectorized into numbers, so here we use text vectorization techniques, there are multiple techniques, I have taught you NLP play.  In the list, you can use a simple technique like OE and bag of words and you can use a little advanced', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='bag of words and you can use a little advanced level techniques like aapka word to wake gaya or you can use any other form of embedding, so let me explain you in the examples of OE. How did the vector of How will you represent like this and how will you rip rot Going by this logic How will you write this sentence You will write this instead of cat Instead of rot this and this instead of rot Similarly how will you write the third sentence Instead of mat this rot  Change this and again change to', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='of mat this rot  Change this and again change to Matt, this is my sentence one, this is my sentence two, this is my sentence three, now what I have to do is send one sentence at a time, sorry, one word, so when we  If we are processing this entire sentence, then we will get three time stamps, in the first time stamp, my XT will be this thing, then in the second time stamp, my XT will be this thing and then in the third time stamp, my XT will be this and this.  thing and this way my ave work', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='be this and this.  thing and this way my ave work will be finished then I enter at at the first time stamp this will be my first xt this will be my second xt and this will be my third xt and so on so this is how you convert an input into  A vector so this will be some vector, now it can be any type of vector depending on which text vectorization technique you are using, this is just for you to clarify, so I told you, so guy, we have done all the preparation now we  Are you ready to understand', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"preparation now we  Are you ready to understand the architecture of GRU? So a little while ago I told you that the whole goal of GRU is that you are getting these two things: 'For Any Time Stamp' and from this you have to calculate this thing.  You are getting this, you are also getting this, you have to calculate this, this is the goal, but this whole work is actually divided into four steps, so we will first see those four steps once and then we will go through all the steps one by one in\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='we will go through all the steps one by one in detail.  If you understand then the first step is what you do there, you calculate RT which we call reset gate, this is step one, it is okay to calculate this thing, step two is calculating this thing which we call candidate hidden state, after that we calculate. Jati which we call update gate and once we are done with these three steps the last step is to actually [music] calculate ht which you call current hidden state so if I try to explain it', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='current hidden state so if I try to explain it to you with a flow diagram then you will get ht  What is needed is that you get it from two things, one is AT -1 and the other is HT. Tilt sign is basically candidate cell state and to combine these two you use update gate. OK, HT is needed. HT will be formed by the combination of these two and that. Who decides the combination? Okay, now the HT tilt sign is the candidate hidden state. It is actually made up of two things, one is your HT MIVE and', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='made up of two things, one is your HT MIVE and the other is your EXT but plays a very important role here. Your RT CH is your recent gate I.  M Sorry Write Reset Gate Sorry Reset Gate Okay so this is the flow diagram T is why I said first of all what are you calculating you are calculating this this is your step one is calculating this so that you use this in  Apply the combination of both and from the combination of these two, you calculate this in the second step. Okay, once this is', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='this in the second step. Okay, once this is calculated, then with the help of this, we need the combination of these two. So this is step three, apply this and finally when these three  Once the steps are done, then you go to step four and you calculate this, this is the flow diagram of GRU, the flow of chronological information will be like this, so what we will do now is we will see these four steps one by one and  We will also try to understand what is happening at the intuitive level, what', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"what is happening at the intuitive level, what is going on, okay, then you have to study Guy's Architecture, but if you remember, I told you a little while ago that the main goal is to use GR.  What it means is that for any time step t, you get the hidden state of the previous time step and get the input of the current time step and from this you have to calculate the hidden state of the current time step. So let's do one thing, let's do a conceptual discussion. Intuitively speaking, once we\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='discussion. Intuitively speaking, once we develop the understanding of what it is, mathematically, we know that it is a vector, it is a vector, meaning it is a set of numbers, but if you understand what this set of numbers represents. Neither the following video will be very easy, now to explain this whole thing, I am going to use the story which I had shared with you in LSTM again, so if you remember, while teaching LSTM, I gave you a very interesting lesson.  A story was told that there was a', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"lesson.  A story was told that there was a king named Vikram who was very strong and powerful. He had an enemy named Kali. There was a fight between them and Vikram died. Vikram had a son who was exactly like his father. He sought revenge from Kali. attacked but he also died; finally Vikram's son, who was Vikram's grandson, grew up and he also fought with Kali and he finally killed Kali and he took revenge for his father and his  I told you this story of Dada. Now what will I do? Using this\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='story of Dada. Now what will I do? Using this story, I will try to explain to you the concept of HSN Hidden State and give you an idea of \\u200b\\u200bwhat these numbers represent. Okay, so if I give you a very simple one.  Let me answer the word, what is this hidden state, then you can actually call it memory of the system, it is nothing else, it is memory, basically when you are putting some words in this system time step by time, then you want that  This is GRU or for that matter which is LSTM, these', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"is GRU or for that matter which is LSTM, these people keep a memory, keep a contact that what is going on, right, let's take an example, suppose we have to put this entire story in this GRU and then we have to Ask, tell me whether the ending of this story is a happy ending or a sad ending. We are doing sentiment analysis, so let's assume for a while that we are putting one complete sentence here at a time, so we would like to.  Like suppose you are at this step right now, we should know some\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='are at this step right now, we should know some memory of the story till now, only then we will be able to process the next sentence and the next sentence and so on, so in a way this is the hidden state.  It actually works as a memory and in this memory you store contacts so in a way these are numbers because hidden state is basically a vector set of numbers so these numbers actually intuitively speaking they represent some form of content like I am telling you.  Let me give you an example.', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"I am telling you.  Let me give you an example. Let's assume for a moment that this h of ours is mine or this one is at. Both these vectors are of four dimensions.  It will always appear to you in the form of four numbers. Okay, now the question comes that what do these four numbers represent collectively? These four numbers tell the context of your story so far. Okay, now these four numbers individually each number represents something.  One aspect of the story is going on like we can assume\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='of the story is going on like we can assume that the first number is representing power, so whenever a king is being introduced and it is being said about him that he was very powerful.  If that was there then it is coming in the power aspect of that story. Second number lets representatives conflict, fight happened. This is conflict. Third number representatives tragedy means someone died. Fourth number representatives revenge. Someone took revenge. It is possible that this story might happen', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"It is possible that this story might happen to someone.  According to this, if we are going with four numbers, then these four numbers can be that I am telling four aspects of the story, it is not necessary that by the way, I have given an example from myself, it is not necessary that it should be this.  There could be something else, actually you can't figure out what it is representing, it's just numbers, this is the big problem of deep learning, it is a black box, we don't know what is going\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"it is a black box, we don't know what is going on inside, but just to explain.  I gave you this example, so let's say if at some point at some point this is the point, then at this point we can think that we have only this connection of the story that it has been told about someone who  It is very powerful, as soon as this aspect increases, we will understand that till this point in the story, some kings and people have been introduced and there has been a fight between them. As soon as this\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"has been a fight between them. As soon as this aspect increases, we will understand that something went wrong, someone died and so on.  As soon as this b increases, we will understand that the revenge has been taken. Okay, so let me give you a rough intuition going forward sentence by sentence as to how our GRU tries to keep the memory. So let's say we have first time step.  In Let's Seti Equal, we fed this entire sentence through XT, so what is this sentence saying, there was a King Vikram\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='is this sentence saying, there was a King Vikram very strong and very powerful, so there is a chance that now our HT can actually accept it in the beginning.  That is, in the very beginning, HT MIVE was complete GG, then this first sentence entered through XT, then what became AT, AT became these representatives that we have now in the story and it is just related to the power of power of a king. Description happened now what happened in the next time step we went to the next time step is E 2', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='time step we went to the next time step is E 2 so now this HT which was there became AT MIVE for the next time step okay and we gave the input from here where we said delay was  Enemy King Kali So here we described one more king so it became one from one and increased because here we have the contact of two kings, we have the concept of power of two kings and the conflate is our increased on the pouf.  Gone y because here enemy word is used tragedy is still zero revenge is still zero this is', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='is still zero revenge is still zero this is our second time step is over t e for 3 now this becomes our ht myve and we sent third sentence from ext both head  A Var and Kali killed Vikram. Now in our next step, the power decreased a little bit. Why did it decrease because Vikram died, so whatever power contact was related to him, we reduced it. Varus conflict increased a little more because a proper Var has happened. Tragedy has happened.  Also increased because Vikram died and Revenge is now', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"increased because Vikram died and Revenge is now zero and our teeth are also finished T F P Let's go T F P Vikram has a son Vikram Jr. He grew up and became very strong just like his father so again our power  The main thing has increased a bit, the conflate has reduced a bit, because maybe there is a period going on in between where there was no fight or tragedy, the tragedy has also reduced a bit, philosophy speaking has become an old thing, Vikram's death revenge is still zero, then  The\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"Vikram's death revenge is still zero, then  The time came, step five also attacked, Kali but got killed, so here the point became 8, the conflict point became 8, the tragedy increased because now in our mind or in the mind of this Ji Ru, there is a connection of the death of two kings, so the tragedy increased.  Revenge also increased because Vikram's son had come in return and attacked him. Let's go to the next time step, Vikram Junior to have a son called Vikram Super Junior and when he grew\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='a son called Vikram Super Junior and when he grew up he also burst Kali, it came back to one because one.  And the king has entered, the conflict is very high in Pow 9, the fight is happening for the third time, the tragedy is a little less and from Revenge Pow and finally in the last sentence it is written And he killed Kali and took revenge of his father and grandfather so here we are.  Points came to san because Kali is dead so power is now reduced a bit, conflict is also reduced a bit', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='now reduced a bit, conflict is also reduced a bit because war is over tragedy actually reduced because Kali is the villain so we are not feeling tragic on his death and Vengeance  Went on one and this is the final contact When you came out after reading the story, someone asked you, tell me brother, how did you like the story, then you will tell, there was a lot of masala in the story, there was power from P8, there was conflict because there were a lot of wars, there was path tragedy.  People', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='a lot of wars, there was path tragedy.  People died but there was a happy ending in the end and the revenge was brotherly so this is how the GRU and for that matter LSTM they are trying to maintain a contact through hidden state. Now our whole goal is what we have to understand completely.  This is how these chains are happening, how this came from this, how this came from this, how this came from this, how this is coming from this, these chains are happening, this is where your two gates have', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"are happening, this is where your two gates have a lot of role, one of them is the reset gate and the second one.  One is the update gate, both of which are the main role of this transition and that is what we have to learn but I really hope that what we just discussed has given you a little bit of intuition as to how you can switch the context through the manipulation of these vectors.  Are you able to change it step by step? Okay then or let's move on to the architecture. Well, one thing and\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"move on to the architecture. Well, one thing and now I have explained to you by applying kind of our intelligence that how HT is formed from HT myv which is our previous memory or context.  That's right, we did this on the basis of our interpretation on the basis of our discussion. Once you also have to understand before entering into the architecture that how GRU does this work, so essentially GRU performs this task where it has to do with previous contact previous memory.  From the current\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"contact previous memory.  From the current context to the current memory, it does this work in two steps. So in the first step, what a g ru does is that it makes the thing that we know from it.  Let's say candidate memory or candidate hidden state and then it creates HT from this candidate hidden state, so this is step one and this is step two. Okay, now let me explain to you a little with an example how step one is executed.  And how is step two being executed? So for a moment, let us assume\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='being executed? So for a moment, let us assume that we have started reading the story and somewhere in the middle, you can assume that we have read the part that Raja Hai Ek Vikram is very powerful.  He is very strong, he has an enemy, Kali and there was a fight between them and Kali killed Vikram. Suppose we have read the story till now, then the summary of HT minus and whatever is there will be in these three lines.  And I have told you that our summary is covering four aspects, first power,', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"summary is covering four aspects, first power, second third tragedy and fourth revenge. So let's accept it for a while, according to what we have understood of the story with our common sense.  Let's assume the power at this point is around Pf because first Vikram was introduced that he is very powerful but when he died then it came to our mind that he might not be that powerful. The conflict point is six but just a fight has happened, tragedy.  It is very high because just now Vikram has died.\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='It is very high because just now Vikram has died. Revenge is now zero or very close to zero because no element of revenge has come yet, so this is our current HT mive, past memory till now is ok, what do we have to do now? That as soon as we are given a new sentence ch is the current input like in our case the next sentence is this one Vikram is the son of Vikram Junior who grew up and he also became very strong just like his father this is the input we are given On the basis of this current', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='input we are given On the basis of this current input, a new memory has to be formed which is a candidate memory, that is, this memory is the candidate to become HT. Okay, so we have to create an HT tilde which will become HT in the future. Okay, but it will become HT bar.  How with the help of XT and past memory till now Okay, so this sentence we just got where we are talking about a new king, so if you go to form memory on its basis, then the value of power will increase a little. Because', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"value of power will increase a little. Because again we are talking about a new king and praising him, the conflict will be reduced a bit because now when he is growing up, there is no fight at that time, the tragedy will also be reduced a bit because of Vikram.  It's been some time since he died and the aspect of revenge will increase a bit because in the story you are able to sense that Vikram's son will try something to take revenge. Right, this is your HT tilde like this is your candidate\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"this is your HT tilde like this is your candidate hidden state.  So this is our step one, this is step one where you took your old context old memory, took the current input and on the basis of that you formed a candidate memory which has the potential to become the next memory, but we directly created this candidate memory.  Ca n't make it. Just think about it, can we make it a HT directly? We can't make it because it is very heavily inclined towards the current input. Now it is also possible\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='the current input. Now it is also possible that B is not that important in the current input. Point of the overall story. From the view, it is HT bar or tilt, we have built memory according to the current context, but whether it is good overall, we do not know, so what you do is that you have HT-1 now. Summary of the story till now and you have HT tailed or bar which you achieved by changing it into HT human on the basis of current input. Now what you do is in the second step you balance these', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"you do is in the second step you balance these two, how is the balance done and we  Will read but you balance these two and go somewhere in between. Now if the current input is very important from the perspective of the story, then we will give it more weightage. If the current input is, let's say, equally important from the perspective of the story. So we will give more weightage to it so it remains dynamic but by balancing between these two the event final context is fine like if you do equal\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='event final context is fine like if you do equal balancing between these two then there is a chance that the power will come somewhere around 65 K. Your conflict will come equal to F Your revenge will come equal to P Sorry tragedy will come equal to S And your revenge will come equal to Pat So this is your HT after the input of EXT This is your contest or memory so far about the overall  The power of the story has increased a bit, the conflict has decreased a bit, the tragedy has decreased a', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='has decreased a bit, the tragedy has decreased a bit but the revenge has increased because there is a new king, so you actually do this transition in two steps, how to reach AT MI and HT, so now we have just  Understood that HT MIVE is the way to access HT from the past memory, which is the way to access the current memory, it is a two step process, first you are creating this candidate memory from HT MIVE and then using the candidate memory You are making HT, okay, but to make it clear, I took', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='are making HT, okay, but to make it clear, I took a few excerpts and explained to you that like we assumed that this is HT Myve and then based on the current input, I wrote that it is possible that our candidate memory  Ho but again, this calculation is also done by GRU itself, so a little bit, we will further break down this two step process, how actually it is becoming HT tilt from HT move and how it is becoming HT from HT, so actually.  This is where the concept of gates comes in. Okay, so', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"is where the concept of gates comes in. Okay, so this conversion is done with the help of a gate which we call the reset gate. You can see in our diagram that this gate's job is to get this conversion done and this conversion.  This happens because of our second gate which we call update gate and we call it Jati and you will see it here in this diagram. Okay, so essentially we understood that from HT MIVE you can update RT.  By taking help, you are making candidate memory and then by taking\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='are making candidate memory and then by taking help from candidate memory, you are making current memory or current hidden state. Okay, so do one thing, before moving ahead, I am giving you a flow chart that shows the whole thing.  How it works Like the mathematical overview Okay, so first of all, what happens is you have ht myv which is your current memory and you have xt which is your current sentence or current paragraph is current word whatever.  You make RT by mixing these two. What is RT?', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='You make RT by mixing these two. What is RT? Your reset gate is fine. After that what you do is you make this by mixing AT and HT. Okay by the way it is called model or reset.  Hidden State, what is all this, let me explain to you in a moment. As soon as you get this, you again take XT and mix these two and through this process you get candidate memory. Okay, after that what do you do? On the side again you take ht my 1 which is this thing and ekty and combine these two to make z t which is', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='ekty and combine these two to make z t which is your update get and then as soon as you have this it comes you go back ht t  You take -1 and mix these three and you make HT. Like this is the mathematical flow chart of GRU so you can see easily here this is the first step where we are making HT tilt from AT MIVE.  And from HT tilt, finally this is the second step where we are making HT. Okay, now from here, who is helping you to reach this stage, who is helping you? And from here to reach here', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='who is helping you? And from here to reach here in the second step. Who is helping you? Okay, now I know, at this point you may have some questions curiosity in your mind that first of all, what is this RTÉ representative doing, what is this gender representative doing or is this model hidden state?  What is Xetra representing? So now we have to understand all this. Come on guys, now we are finally going to talk about the architecture and we will do it step by step, so the first thing that will', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='do it step by step, so the first thing that will happen in this entire architecture is the calculation of RT.  The reset gate will be calculated. Okay, before telling you how to calculate, I would like to tell you once that what is the reset gate, what is it? Mathematically speaking it is a vector and its shape or dimension will be exactly the same as that of h t -1. We have discussed this so h t -1 We had discussed that Power Conflict Tragedy and Revenge is made up of four things so similarly', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='Revenge is made up of four things so similarly your RT will also contain four numbers Ok but at the same time RT is also a get get  This means that the four numbers inside RT will all be between zero to one. Well, if this number is zero then you should understand that RT is closed for that particular dimension. If it is one then it means RT.  Is open and fully open for that particular dimension, so RT actually works like a gate. Okay, so why is it called reset gate, let me explain it to you', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='is it called reset gate, let me explain it to you once, for that we will have to go to our story again, so in our story. Suppose we have read the first three sentences, Vikram has entered, Kali has entered and there is a war between them and Vikram is dead, now we have the context till now and the new sentence is  About Vikram Jr. how he is very strong like his father Okay so what the reset gate does exactly is it resets some of the dimensions of your h-1 based on the current input Okay once I', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='your h-1 based on the current input Okay once I go through the example  Let me explain like suppose at this point after these three sentences the value of AT is actually the value of h t -1 this is power this is 6 conflate is point from tragedies point from and revenge point and so now as soon as I got this new sentence that Vikram Junior Vikram  He is the son of and is as powerful and strong as his father, so actually what you are trying to do is that you are trying to reset some parts of the', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='is that you are trying to reset some parts of the old memory, like, on the basis of this sentence, I am actually keeping a lot of this information. I would like, because here we are talking about the power of Vikram Junior, so we would like to retain a lot of this information, this is conflate since there is nothing related to conflate in our current statement, we would like to reset this part a little and lower it.  The next one would like to be a tragedy since no one is dying here, there is', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"be a tragedy since no one is dying here, there is no sadness, so we would like to lower it a bit, would like to reset it and since maybe Vikram's son grows up and takes revenge, then a little bit of revenge is infusing into the story.  So we would like to make it a little higher or in other words we would like to retain most of its components so after this step it may be that your RT value looks like this so point 8 patv and pana what about this  What this means is that when I am saying that\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='What this means is that when I am saying that this is the first dimension P8 in RT, this means that I am trying to retain 80 of my assisting past memory because it seems important to me. What about this point two?  This means that in the memory of the assisting conflict, I want to retain it only at 20, the rest I want to delete because according to the current statement it seems that now the conflict is no longer there, I want to retain the tragedy also at only 10.  I want to because now', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"tragedy also at only 10.  I want to because now that's an old thing but I want to retain Re at 90 even though it is low, it is low but I want to retain whatever is at 90 because I feel that revenge is going forward.  This moment is going to be an important aspect, so RT Atley is a gate which is in the form of a vector and its job is to reset things from your past memory based on the current input. Okay, now if you understand this.  Let me explain to you how to calculate, it is very simple so\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='to you how to calculate, it is very simple so all you have to do is focus on this diagram, in fact I do one thing and at the same time I create my own diagram so that you do not focus on unnecessary things. Do so, first of all we have this is our context being passed here is AT minus and and these two are our HT okay and from here our current input is going so first of all what happens is that you take AT minus And you take xt and you take these two and put them into a neural network layer, a', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"two and put them into a neural network layer, a sigmod neural network layer, and this gives you RT. Okay, mathematically speaking xt is equal to sigmod of some weight. Let's call this weight double R.  Dot product the and xt2 -1 is a four dimension vector so we know that this neural network layer will actually have four nodes in it. Okay what we're doing is we're putting our four dimension h -1 in it and for a while  Let us assume that we are representing every sentence of our story in three\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='representing every sentence of our story in three numbers, so basically x is a three dimension vector, so I am representing it with a different color, okay and now this thing is a fully connected layer, okay so  Total here you have sen and here you have four so here you have this l aa that will be a 7 ks 4 28 weights and how much bias will there be four bases so these are your parameters right in this place  Now take a look at your [music] this is 7 crosses and here the weight is or say it like', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='7 crosses and here the weight is or say it like this, transpose is taken from 1 cross and here the weight is 7 crosses 4 so when this dot product is done then you will get 1 cross.  7 dot product 7 cross 4 These two are the same you will get no cross 4 In this you are adding bias Chch is 1 cross 4 This will give you a vector of one cross 4 and when you apply sigmod on top of that you will get again no cross four  You will get a vector of f is nothing but your RT. Okay, so now we have just', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"is nothing but your RT. Okay, so now we have just opened up and seen how the RT is being calculated, so this is the operation here where we are calculating this RT. This is nothing but a weighted. Some weights are over here. Some weights are over here and we're training the weights through back propagation and then we're putting it inside a sigmo id because we want the answer to be between zero and one because after all RT is a  Gate is ok, so that is how RT is created, what is gate, we\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"so that is how RT is created, what is gate, we understood what is its purpose, we understood and how it is calculated exactly, we tried that too, now we will move ahead and in the next step we will see RT and  Infact RT by combining HT and XT will create this candidate cell candidate hidden state. So let's move forward in the Guice architecture. We have calculated our reset gate and this is the current state of the architecture. Now in this flow diagram.  But if you go, we have reached here,\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='diagram.  But if you go, we have reached here, okay, now what we have to do is that we have to calculate this quantity by combining this reset gate and the past memory till now and what is this quantity, at all this is nothing but the model memory or  Then reset memory, then you do not have to do anything, from here you will bring HT, this is AT and from here you will mix AT in it and here you will do a small mathematical operation which is called point wise multiplication, element wise', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"is called point wise multiplication, element wise multiplication, so basically  You multiply this vector of yours and this vector of yours element wise and by doing this the quantity you get is nothing. If you are multiplying then this word becomes 48. We have retained it at 80.  of this is pot 12 this is actually going to be 07 sorry and this 09 So here's guys our model past memory based on my current input Model created reseteer, this was your power, in power we had a little reseteer in\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='your power, in power we had a little reseteer in conflict, in this we reset a lot based on the current input, next was tragedy, in this also we reset a lot and last was our revenge, in this we reset very less so basically we have  There is a model past memory, now what will we do, we will take it with my current input, basically I am saying that we will take this product forward and merge it with XT, okay, we have merged it with XT and taken it forward. Will go and put it in a neural network', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"forward. Will go and put it in a neural network layer this time this layer is a layer and the output I will get from this is my candidate hidden state so basically mathematically speaking the operation we are doing here is this this is a of this  Let's consider the whole weight. Doublets call it H for rar p for candidate's weight and you are concatenating two things here, one you concatenate this and you concolor with this. Okay if you talk about this neural  There are also four nodes in the\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"this neural  There are also four nodes in the network layer, okay here you have these four inputs which are basically representing this and then here you have three more inputs and this is a fully connected layer. This is a fully connected layer and this is  I am calling it DB, okay and how much will this be? 7 cross will be 4 and the bases here I am calling BC okay. Again you can verify the shape by finding the dot product of the dimensions of these weights. But or that's how you're going to\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"these weights. But or that's how you're going to get your candidate hidden state or candidate memory okay so once you have this you did the dot product with ext again and got the nach out so now you got this okay now maybe based on According to our data so far, it is possible that in the current input we had talked about Raja Naya Raja, then it is possible that it may have gone from Pot S, it may have come to point two, it may have come to point one.  And this has gone to point two, Something\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='one.  And this has gone to point two, Something like that could happen, okay, so this is our proposal, we are kind of proposing that this should be the HT but we do not consider it to be directly HT, okay, I told you, you  You cannot consider it as HT directly. If you consider it as HT then in this way you are relying too much on the current input which is not necessarily very important from the point of view of the overall story. So now we have to work in just two steps.  Well, we have to take', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='to work in just two steps.  Well, we have to take this quantity and we have to take this quantity and we have to play a balancing game between these two using quantity which we will calculate next so guys now we will move ahead and we have reached here now we will collect Will do our update gate f is this thing and to calculate the update gate you again need two things, your past memory till this moment and your current input and by mixing these two you will get Jati. If you talk about', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='these two you will get Jati. If you talk about architecture point of From the view, you go ahead from this place, in this place you have both the things, there is XT, there is also HT, there is also MIV, you just take it further and put it in a neural network layer which will be SIGMOD and from its output you will get JTI.  Mathematically speaking JT is nothing but sigmod of dd plus bj OK again this will be the neural network layer it will have four nodes OK and from here you will get AT MIV', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='four nodes OK and from here you will get AT MIV and from here you will get EXT here which will be your fully connected layer.  That will again be a cross four and here you will have four bases. Okay, you can again do the shape matching, dimension matching, you can try. The bigger question is whether the shape is finally a vector exactly the same size as the shape. AT is my or HT is the candidate one and at the same time this is also a get get means the four numbers inside it will be between', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"means the four numbers inside it will be between zero to one. Now let me tell you what is the purpose of Jati so we have this candidate.  There is hidden state but we cannot make it directly HT because it is heavily skewed. On top of the current input, we have to give value to our past memory as well as past context but we do not know how much value to give. Now this can happen, isn't it yours?  Which is the current input, it is very important like really important, meaning this is such a line\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"really important, meaning this is such a line that after reading it, the story that was in your reader's mind till now gets completely reversed. Imagine that you were reading a murder mystery, till now someone else was reading it.  You were feeling that suddenly after reading the last line of the murder you came to know that no, someone else had committed the murder and because of that the entire contact story in your mind got completely changed. They say, times change.  c Emotions have changed\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='They say, times change.  c Emotions have changed If there was that kind of input then in that case you will obviously have to give more input to the candidate who is heavily relying on the current one and you will give less value to the old one but it may happen that the current one is singing in your story.  This is happening in Bollywood movies, then it is not so important, then the candidate hidden state created by it is also not that important, so in that case, you have to value it more,', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='so in that case, you have to value it more, then basically the work of Jati is that  When you train it on the data, it automatically extracts and gives you a set of four numbers. The athlete in this case already knows how to draw the balance between HT MIVE and HT Tilde and  This happens in the training process after applying back propagation so basically Jati if you have then now you are ready for the final step now you know how to balance the previous memory which is AT -1 and the candidate', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"previous memory which is AT -1 and the candidate hidden state which is AT. Tilde and then by combining these two, you will finally make HT. So guys, we have the candidate cell state, we have the previous memory. Let's assume for a moment that we have a Jathi also calculated and the Jathi is let's say p v 782.  Suppose this is okay after training, then what you have to do now is to calculate HT by mixing these three quantities and the formula of HT is a very simple mathematical operation, you\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='HT is a very simple mathematical operation, you will understand it here also. K so basically what you do is just one second so basically what you do is you calculate n minus jti and multiply it element wise from at my and you take jti and multiply it element wise candidate one With and between these two, you do element wise addition. Okay, this is the operation to calculate HT, meaning here if you see this was your Jati, then you are running an operation on this Jati here on minus. And by', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='an operation on this Jati here on minus. And by taking it and mixing it with AT-1, here you are fine, I am talking about this operation, then you are also taking out Jati from here and taking HT Tilt from here and here  But between these two, you are doing this operation and then this one coming from here and this one coming here, by combining these two, you are doing a plus operation i.e. this one and that is.  HOW YOU ARE GETTING HD Okay now the intuition is very simple, if the JT is high, if', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"intuition is very simple, if the JT is high, if the JT is very high then you are overvaluing the candidate. If the JT is low then you are overvaluing the past memory, so think about it.  Look, if Jati is lower than let's point w then 1 min Jati how much will it be 9 So I am overvaluing h -1 and here z is t1 so I am undervaluing this so in a way here  Since I have started 1 mine and here I have done z, then I have created the balance on the basis of past memory and current memory, it is ok, I\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"of past memory and current memory, it is ok, I hope you understand this and that is how you get your AT like I  Can do a calculation in front of you so if we apply this then 1 - z will come out to be 97 I'm sorry 32 8 and I have to multiply this element wise with 6671 so this will come to you 54 188 point 1408  This is what came out, this is your quantity, okay, and then what I will do is multiply this and this, this will give me 07 14408 PO 04 if I am correct, okay, and now I will add these\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"if I am correct, okay, and now I will add these two, add this and this. So here I will get 6132 p 22 end pv Here is my final HT Now think for yourself, see on which stage we were, where we were giving the introduction of Vikram's son, and after giving the introduction, here is yours.  The power which increased a little bit was PS0, then it became PS1 because we talked about an additional king, after that there was CONFLET CONFLET, the first point was S, it came down to point 32, it got reduced\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='was S, it came down to point 32, it got reduced because there is no fight going on right now.  There was a lot of tragedy, that too has reduced because it has been a long time since Vikram died and this was point one and it has become 122. This is revenge. The angle of revenge is increasing a bit because we feel that what happened to Vikram Son, he will take revenge, so this is the way we reached here from this HT MIVE, so once just to surprise, I am making the flow chart again. By holding the', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='I am making the flow chart again. By holding the AT MIVE XT, you are calculating the reset gate, whose job is to find out. Which dimension has to be reset which is not useful according to the current input then by combining HT MIVE and RESET GATE gate you are creating reset past memory or model past memory using this operation and in this you just You are adding that you are getting your candidate hidden state. Now at the same time, what are you doing. You cannot directly make the candidate', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content=\"you doing. You cannot directly make the candidate hidden state as the final state, so you are again calculating the Jati using h t -1 and acti. Chch is your  Update gate which finds out what should be the balance between h-1 and AT tilde and finally you are calculating AT by combining all three of AT and AT so that's the whole thing we discussed and this is  The Antius Structure Now if you look at this architecture with love then you should understand everything so guys before ending the video\", metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='everything so guys before ending the video one more time we will discuss what are the main differences between LSTM and GR usage so here I have noted down some differences. The first difference is that you already know that the number of gates is more in LSTM, there is input gate, FOR gate and output gate. In comparison too. That GR uses only two gates, reset and update. Okay, we had discussed this well. If you talk further about memory units, then in LSTM you have cell state and hidden state', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='then in LSTM you have cell state and hidden state also. In GR, only hidden.  State is ok this is the second difference parameter count is also a big point of difference in LSTM the parameter count is more and this is the formula by the way if you have an input whose size is D and the hidden size So this is the formula to calculate the total number of parameters that will be present in the LSTM cell where as in comparison to that look here that same formula contains three instead of four so the', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='formula contains three instead of four so the number of parameters is less this you copy once  You can check by taking pen. On the fourth point, if we move, the computational complexity gets more parameters. In comparison to GR use, where they are faster to compute especially on smaller data sets and when computational resources are limited. If you use more  If we talk about performance then LSTM in many tasks is especially more complex than GR. Okay, so if we have a slightly complex set, then', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='Okay, so if we have a slightly complex set, then LSTM has experimentally proven to give slightly better results. Can it outperform STM on certain tasks? We are talking about GR specifically.  When data is limited and tasks are similar they can also train faster due to fewer parameters So basically if you have a smaller data set a little less complexity The choice between LSTM and GRU ofen comes down to Empir Kal testing It is said that by running both Look depending on the data set and the task', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='both Look depending on the data set and the task one might out perform the other how ever GR is used due to their simplicity of being the first choice when starting out try on GR if you want to improve the results try applying LSTM GR is not improving. Leave the use. Okay, so with that, we covered in great detail how gated recurrent units work. I really hope you liked this video. If you liked it, please like it. Very hard work.  It takes great effort to make such videos and if you have not', metadata={'source': 'QQfZAoNGQmE'}),\n",
       " Document(page_content='effort to make such videos and if you have not subscribed to the channel please do subscribe, see you in the next video.', metadata={'source': 'QQfZAoNGQmE'})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckURL(BaseModel):\n",
    "    url: str\n",
    "class CheckSearchInput(BaseModel):\n",
    "    textInput: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_store = FAISS.from_documents(documents=texts,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "# Answer the following question based only on the provided context. \n",
    "# Think step by step before providing a detailed answer. \n",
    "# If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "# <context>\n",
    "# {context}\n",
    "# </context>\n",
    "# Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following  text transcript to answer the user's question in detail.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\n",
    "    Only return the  answer below and nothing else.\n",
    "    Detailed answer:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Stuff Docment Chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                            temperature=0.2,convert_system_message_to_human=True,google_api_key=api)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "llm=model,\n",
    "chain_type=\"stuff\",\n",
    "retriever=v_store.as_retriever(),\n",
    "return_source_documents = True,\n",
    "chain_type_kwargs= chain_type_kwargs,\n",
    "verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\envs\\geminienv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Vikram was a powerful king who had an enemy named Kali. They fought, and Vikram was killed. Vikram's son sought revenge but was also killed. Finally, Vikram's grandson grew up and killed Kali, avenging his father and grandfather.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"explain story of vikram\")[\"result\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
